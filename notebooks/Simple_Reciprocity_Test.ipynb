{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Test of Reciprocity of 3 Sources and 3 Recievers in Groningen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all packages\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sys import argv\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from pyaspect.model.gridmod3d import gridmod3d as gm\n",
    "from pyaspect.model.bbox import bbox as bb\n",
    "from pyaspect.model.gm3d_utils import *\n",
    "from pyaspect.moment_tensor import MomentTensor\n",
    "from pyaspect.specfemio.headers import *\n",
    "#from pyaspect.specfemio.write import *\n",
    "from pyaspect.specfemio.read import *\n",
    "from pyaspect.specfemio.utils import *\n",
    "\n",
    "import pyaspect.events.gevents as gevents\n",
    "import pyaspect.events.gstations as gstations\n",
    "from pyaspect.events.munge.knmi import correct_station_depths as csd_f\n",
    "import pyaspect.events.mtensors as mtensors\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy import UTCDateTime\n",
    "import shapefile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "Extract the ndarray of the subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_dir  = 'data/output/'\n",
    "data_out_dir = data_in_dir\n",
    "!ls {data_in_dir}\n",
    "!ls data/groningen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 \n",
    "\n",
    "Decompress the ndarray of the sliced, subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename then used it to decompress model\n",
    "ifqn = f'{data_out_dir}/vsliced_subsmp_smth_nam_2017_vp_vs_rho_Q_model_dx100_dy100_dz100_maxdepth5850_sig250.npz'\n",
    "vslice_gm3d, other_pars = decompress_gm3d_from_file(ifqn)\n",
    "\n",
    "print()\n",
    "print('decompressed gridded model\\n:',vslice_gm3d) \n",
    "print()\n",
    "print('other parameters:\\n',other_pars)\n",
    "print()\n",
    "\n",
    "# WARNING: this will unpack all other_pars, if you overwrite a variable of the samename as val(key), then you \n",
    "#          may not notice, and this may cause large headaches.  I use it because I am aware of it.\n",
    "'''\n",
    "for key in other_pars:\n",
    "    locals()[key] = other_pars[key]  #this is more advanced python than I think is reasonable for most \n",
    "sig_meters = sig\n",
    "''';\n",
    "\n",
    "# another way to get these varibles is just use the accessor functions for the gridmod3d.  We need them later.\n",
    "xmin = other_pars['xmin']\n",
    "dx   = other_pars['dx']\n",
    "nx   = other_pars['nx']\n",
    "ymin = other_pars['ymin']\n",
    "dy   = other_pars['dy']\n",
    "ny   = other_pars['ny']\n",
    "zmin = other_pars['zmin']\n",
    "dz   = other_pars['dz']\n",
    "nz   = other_pars['nz']\n",
    "sig_meters = other_pars['sig']  # this variable is used later\n",
    "print('sig_meters:',sig_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "nam_dims = list(vslice_gm3d.get_npoints())\n",
    "nam_origin = [0,0,-vslice_gm3d.get_gorigin()[2]]\n",
    "#nam_origin = list(vslice_gm3d.get_gorigin())\n",
    "#nam_origin[2] *= -1\n",
    "nam_origin = tuple(nam_origin)\n",
    "nam_spacing = list(vslice_gm3d.get_deltas())\n",
    "nam_spacing[2] *=-1\n",
    "nam_spacing = tuple(nam_spacing)\n",
    "print('nam_dims:',nam_dims)\n",
    "print('nam_origin:',nam_origin)\n",
    "print('nam_spacing:',nam_spacing)\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.dimensions = np.array(nam_dims) + 1\n",
    "grid.origin = nam_origin  # The bottom left corner of the data set\n",
    "grid.spacing = nam_spacing  # These are the cell sizes along each axis\n",
    "nam_pvalues = vslice_gm3d.getNPArray()[0]\n",
    "print('pvalues.shape:',nam_pvalues.shape)\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_arrays[\"values\"] = nam_pvalues.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "# Now plot the grid!\n",
    "cmap = plt.cm.jet\n",
    "#grid.plot(show_edges=True,cmap=cmap)\n",
    "grid.plot(cmap=cmap,opacity=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = grid.slice_orthogonal()\n",
    "\n",
    "#slices.plot(show_edges=True,cmap=cmap)\n",
    "slices.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create random virtual source (to specfem stations, but using reciprocity -- sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "#n_rand_p = 1000\n",
    "\n",
    "n_rand_p = 3\n",
    "np.random.seed(n_rand_p) #nothing special about using n_rand_p just want reproducible random\n",
    "\n",
    "#stay away from the edges of the model for derivatives \n",
    "# and to avoid boundary effects\n",
    "xy_pad = 500 \n",
    "\n",
    "lrx = np.min(xc) + xy_pad\n",
    "lry = np.min(yc) + xy_pad\n",
    "lrz = -3400.0\n",
    "\n",
    "hrx = np.max(xc) - xy_pad\n",
    "hry = np.max(yc) - xy_pad\n",
    "hrz = -2600.0\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "srz = hrz - lrz\n",
    "\n",
    "r_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    rz = lrz + srz*np.random.rand()\n",
    "    r_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "r_xyz = np.array(r_xyz_list)\n",
    "    \n",
    "\n",
    "#r_xyz = np.vstack(np.meshgrid(rx,ry,rz)).reshape(3,-1).T\n",
    "print('r_xyz:\\n',r_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "slices = grid.slice_orthogonal()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(slices,cmap=cmap,opacity=1)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=1.0)\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Moment Tensors and CMTSolutionHeaders for each tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is the path to the project dir on the cluster\n",
    "my_proj_dir = '/scratch/seismology/tcullison/test_mesh/FWD_Batch_Src_Test'\n",
    "\n",
    "magnitude = np.pi\n",
    "strike = [30,45,90] # just making three to test\n",
    "dip = [30,30,60]\n",
    "rake = [330,190,20]\n",
    "\n",
    "l_mt = []\n",
    "for i in range(len(strike)):\n",
    "    l_mt.append(MomentTensor(mw=magnitude,strike=strike[i],dip=dip[i],rake=rake[i]))\n",
    "\n",
    "assert len(l_mt) == len(r_xyz)\n",
    "\n",
    "for mt in l_mt:\n",
    "    print(mt)\n",
    "    \n",
    "l_cmt_src = []\n",
    "for i in range(len(r_xyz)):\n",
    "    cmt_h = CMTSolutionHeader(date=datetime.datetime.now(),\n",
    "                              ename=f'Event-{str(i).zfill(4)}',\n",
    "                              tshift=0.0,\n",
    "                              hdur=0.0,\n",
    "                              lat_yc=r_xyz[i,1],\n",
    "                              lon_xc=r_xyz[i,0],\n",
    "                              depth=-r_xyz[i,2],\n",
    "                              mt=l_mt[i],\n",
    "                              eid=i,\n",
    "                              sid=0)\n",
    "    l_cmt_src.append(cmt_h)\n",
    "    \n",
    "print()\n",
    "for cmt in l_cmt_src:\n",
    "    print(cmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Corresponding \"Virtual\" Recievers (including cross membors for derivatives) for the CMT's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_delta = 50.0 # distance between cross stations for derivatives\n",
    "assert m_delta < xy_pad #see cells above this is padding\n",
    "#l_grp_vrecs = make_grouped_half_cross_reciprocal_station_headers_from_cmt_list(l_cmt_src,m_delta)\n",
    "l_grp_vrecs = make_grouped_cross_reciprocal_station_headers_from_cmt_list(l_cmt_src,m_delta)\n",
    "\n",
    "ig = 0\n",
    "for grp in l_grp_vrecs:\n",
    "    print(f'***** Group: {ig} *****\\n')\n",
    "    ir = 0\n",
    "    for gvrec in grp:\n",
    "        print(f'*** vrec: {ir} ***\\n{gvrec}')\n",
    "        ir += 1\n",
    "    ig += 1\n",
    "\n",
    "print(len(flatten_grouped_headers(l_grp_vrecs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Virtual Receiver Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(flatten_grouped_headers(l_grp_vrecs))\n",
    "all_g_xyz[:,2] *= -1 #pyview z-up positive and oposize sign of standard geophysics \n",
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.5)\n",
    "p.add_mesh(slices,cmap=cmap,opacity=1.0)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=1.0)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get receiver/station coordinates created from a different notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickle the Bounding box (from a different notebook)\n",
    "\n",
    "#ifqn  = data_out_dir + 'bbox_nvl' + str(int(nvl)) + '_nvb' + str(int(nvb))\n",
    "#ifqn += '_xsft' + str(xshift) + '_ysft' + str(yshift) + '.pickle'\n",
    "ifqn = data_out_dir + 'bbox_nvl152_nvb197_xsft4400_ysft19100.pickle'\n",
    "f = open(ifqn, 'rb')\n",
    "sgf_bbox = pickle.load(f)\n",
    "f.close()\n",
    "print()\n",
    "print('Unpickled Bounding:\\n',sgf_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickle the events if needed (again from a different notebook)\n",
    "ifqn = data_out_dir + 'bbox_groning_events.pickle'\n",
    "f = open(ifqn, 'rb')\n",
    "bbox_events = pickle.load(f)\n",
    "f.close()\n",
    "print()\n",
    "print('Unpickled Events:\\n',bbox_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read moment tensors\n",
    "mt_in_file  = 'data/groningen/events/event_moments.csv' \n",
    "!ls {mt_in_file}\n",
    "bbox_gf_mts = mtensors(mt_in_file)\n",
    "\n",
    "# get event catalog of the events (ObsPy catalog)\n",
    "bbox_event_cat = copy.deepcopy(bbox_events.getIncCatalog())\n",
    "\n",
    "# This is a bit hokey, but it works. Here we update the\n",
    "# event time from the moment tensor CSV file with thouse\n",
    "# from the event catalog\n",
    "bbox_gf_mts.update_utcdatetime(bbox_event_cat)\n",
    "\n",
    "'''\n",
    "#for imt in range(len(bbox_gf_mts)):\n",
    "#    print(\"Moment-Tensor %d:/n\" %(imt),bbox_gf_mts[imt])\n",
    "'''\n",
    "\n",
    "# Create a dictionary that maps moment tensors to events\n",
    "bbox_emap,bbox_mt_cat,bbox_mts = bbox_gf_mts.get_intersect_map_events_mts(bbox_event_cat)\n",
    "bbox_e2mt_keys = bbox_emap.keys()\n",
    "\n",
    "# Print a comparison of events to moment tensors\n",
    "for key in bbox_e2mt_keys:\n",
    "    print('UTC: event[%d][Date] = %s' %(key,bbox_mt_cat[key].origins[0].time))\n",
    "    print('UTC:    MT[%d][Date] = %s' %(key,bbox_emap[key]['Date']))\n",
    "    print('Mag: event[%d][Date] = %s' %(key,bbox_mt_cat[key].magnitudes[0].mag))\n",
    "    print('Mag:    MT[%d][Date] = %s' %(key,bbox_emap[key]['ML']))\n",
    "    print()\n",
    "\n",
    "#replace moment-tensors with only those that intersect with the events in the BoundingBox\n",
    "bbox_gf_mts.replace_moment_tensors_from_map(bbox_emap)\n",
    "    \n",
    "# add mt_catalog to bbox_events\n",
    "bbox_events.mergeMomentTensorsCatalog(bbox_mt_cat)\n",
    "merged_bbox_event_cat = bbox_events.getIncCatalog()\n",
    "print('bbox_event_cat:\\n', bbox_event_cat)\n",
    "print()\n",
    "print('merged_bbox_event_cat:\\n', merged_bbox_event_cat)\n",
    "print()\n",
    "print('bbox_mt_cat:\\n', bbox_mt_cat)\n",
    "print()\n",
    "print('bbox_mt_df:\\n', bbox_gf_mts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifqn = data_out_dir + 'bbox_groning_stations.pickle'\n",
    "\n",
    "print('Unpickling Station Traces')\n",
    "f = open(ifqn, 'rb')\n",
    "bbox_straces = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print('Stations:\\n',type(bbox_straces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read shapefiles\n",
    "shape_in_files  = 'data/groningen/shapefile/Groningen_field' \n",
    "\n",
    "gf_shape = sf.Reader(shape_in_files)\n",
    "print('Groningen Field shape:',gf_shape)\n",
    "\n",
    "#get coordinates for the Shape-File\n",
    "s = gf_shape.shape(0)\n",
    "shape_xy = np.asarray(s.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is kind of hokey, but it works for now.\n",
    "# Some of the stations depths do not follow the \n",
    "# 50, 100, 150, 200 meter depths -- possibly because\n",
    "# the boreholes are slanted. To correct for this,\n",
    "# a hard coded \"patch/update\" is applied. See the\n",
    "# code for details and update values.\n",
    "#from gnam.events.munge.knmi import correct_station_depths as csd_f\n",
    "bbox_straces.correct_stations(csd_f)\n",
    "\n",
    "bbox_bb_diam = 1500  #size of the beachball for plotting. I had to play with this parameter\n",
    "bbox_cmt_bballs = bbox_gf_mts.get_cmt_beachballs(diam=bbox_bb_diam,fc='black')\n",
    "\n",
    "bbox_mt_coords = bbox_events.getIncCoords()\n",
    "\n",
    "#get event and borhole keys used for indexing\n",
    "ekeys = bbox_straces.getEventKeys()\n",
    "bkeys = bbox_straces.getBoreholeKeys()\n",
    "\n",
    "#Plot seuence of events with stations \n",
    "#for ie in ekeys:\n",
    "for i in range(1):\n",
    "    ie = ekeys[i]\n",
    "    # coordinates for stations that are in the bounding box\n",
    "    xy3 = bbox_straces.getIncStationCoords(ie,bkeys[0]) #station code G##3\n",
    "    xy4 = bbox_straces.getIncStationCoords(ie,bkeys[1]) #station code G##4\n",
    "    \n",
    "    # coordinates for stations that are G-stations but outside the bounding box\n",
    "    ex_xy3 = bbox_straces.getExcStationCoords(ie,bkeys[0]) #station code G##3\n",
    "    ex_xy4 = bbox_straces.getExcStationCoords(ie,bkeys[1]) #station code G##4\n",
    "    \n",
    "    # coordinates for stations that are inside the bounding box but there is no data\n",
    "    er_xy3 = bbox_straces.getErrStationCoords(ie,bkeys[0]) #station code G##3\n",
    "    er_xy4 = bbox_straces.getErrStationCoords(ie,bkeys[1]) #station code G##4\n",
    "\n",
    "    fig, ax = plt.subplots(1,figsize=(8,8))\n",
    "    fig.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    #Groningen Field Shape-File\n",
    "    ax.scatter(shape_xy[:,0],shape_xy[:,1],s=1,c='black',zorder=0)\n",
    "    \n",
    "    #Bounding Box\n",
    "    ax.plot(sgf_bbox.getCLoop()[:,0],sgf_bbox.getCLoop()[:,1],c='green',zorder=1)\n",
    "    \n",
    "    #Events (reuse event coordinates from cell further above)\n",
    "    ax.scatter(bbox_mt_coords[ie,0],bbox_mt_coords[ie,1],s=90,c='red',marker='*',zorder=5)\n",
    "    beach = bbox_cmt_bballs[ie]  #this creates a plot collection for the beachball points\n",
    "    beach.set_zorder(3)\n",
    "    ax.add_collection(beach)\n",
    "    \n",
    "    #Included stations\n",
    "    ax.scatter(xy3[:,0],xy3[:,1],s=50,c='blue',marker='v',zorder=3)\n",
    "    ax.scatter(xy4[:,0],xy4[:,1],s=100,c='gray',marker='o',zorder=2)\n",
    "    \n",
    "    #Excluded stations\n",
    "    ax.scatter(ex_xy3[:,0],ex_xy3[:,1],s=80,c='lightgray',marker='1',zorder=4)\n",
    "    ax.scatter(ex_xy4[:,0],ex_xy4[:,1],s=100,c='lightgray',marker='2',zorder=3)\n",
    "    \n",
    "    #Stations without data\n",
    "    ax.scatter(er_xy3[:,0],er_xy3[:,1],s=50,c='yellow',marker='v',zorder=4)\n",
    "    ax.scatter(er_xy4[:,0],er_xy4[:,1],s=100,c='gray',marker='o',zorder=3)\n",
    "    \n",
    "    origin_time = bbox_events[ie].origins[0].time\n",
    "    mag = bbox_events[ie].magnitudes[0].mag\n",
    "    title_str = 'Event-%d, Origin Time: %s, Magnitude: %1.2f' %(ie,str(origin_time),mag)\n",
    "    ax.set_title(title_str)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(1,figsize=(8,8))\n",
    "fig1.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "#Groningen Field Shape-File\n",
    "ax1.scatter(shape_xy[:,0],shape_xy[:,1],s=1,c='black',zorder=0)\n",
    "\n",
    "#Bounding Box\n",
    "ax1.plot(sgf_bbox.getCLoop()[:,0],sgf_bbox.getCLoop()[:,1],c='green',zorder=1)\n",
    "\n",
    "#Events (reuse event coordinates from cell further above)\n",
    "ax1.scatter(bbox_mt_coords[ie,0],bbox_mt_coords[ie,1],s=90,c='red',marker='*',zorder=5)\n",
    "    \n",
    "all_xy4 = np.concatenate((xy4,er_xy4),axis=0)\n",
    "#all_xy4 = xy4\n",
    "\n",
    "ax1.scatter(all_xy4[:,0],all_xy4[:,1],s=100,c='gray',marker='o',zorder=2)\n",
    "ax1.scatter(er_xy4[:,0],er_xy4[:,1],s=100,c='yellow',marker='x',zorder=2)\n",
    "\n",
    "title_str = 'Event-%d, Origin Time: %s, Magnitude: %1.2f' %(ie,str(origin_time),mag)\n",
    "ax1.set_title(title_str)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make random virtual sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = vslice_gm3d.getLocalCoordsPointsXY()\n",
    "\n",
    "x_orig = vslice_gm3d.get_gorigin()[0]\n",
    "y_orig = vslice_gm3d.get_gorigin()[1]\n",
    "\n",
    "clip_xy = all_xy4[9:13]\n",
    "print(clip_xy)\n",
    "\n",
    "s_xyz = np.zeros((len(clip_xy),3))\n",
    "s_xyz[:,0] = clip_xy[:,0] - x_orig\n",
    "s_xyz[:,1] = clip_xy[:,1] - y_orig\n",
    "s_xyz[:,2] = -200\n",
    "\n",
    "print(s_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot virtual sources (red) with virtual receivers (white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_spoints = pv.wrap(s_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=8,opacity=1,color='red')\n",
    "#p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.add_mesh(all_g_xyz, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make StationHeaders (real recievers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_real_recs = []\n",
    "for i in range(len(s_xyz)):\n",
    "    \n",
    "    tr_bname = 'tr'\n",
    "    new_r = StationHeader(name=tr_bname,\n",
    "                          network='NL', #FIXME\n",
    "                          lon_xc=s_xyz[i,0],\n",
    "                          lat_yc=s_xyz[i,1],\n",
    "                          depth=-s_xyz[i,2], #specfem z-down is positive\n",
    "                          elevation=0.0,\n",
    "                          trid=i)\n",
    "    l_real_recs.append(new_r)\n",
    "    \n",
    "for rec in l_real_recs:\n",
    "    print(rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make ForceSolutionHeaders for the above virtual sources (including force-triplets for calculation derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_grp_vsrcs = make_grouped_reciprocal_force_solution_triplet_headers_from_rec_list(l_real_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make replicates of each virtual receiver list: one for each force-triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_grp_vrecs_by_vsrcs = make_replicated_reciprocal_station_headers_from_src_triplet_list(l_grp_vsrcs,\n",
    "                                                                                          l_grp_vrecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot virtual sources (red) and virtual receivers (white) FROM headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp_s_xyz = get_unique_xyz_coords_from_solution_list(flatten_grouped_headers(l_grp_vsrcs))\n",
    "grp_s_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "flat_recs = flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs))\n",
    "grp_r_xyz = get_unique_xyz_coords_from_station_list(flat_recs)\n",
    "grp_r_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "print(len(grp_s_xyz))\n",
    "print(len(grp_r_xyz))\n",
    "\n",
    "pv_spoints = pv.wrap(grp_s_xyz)\n",
    "pv_rpoints = pv.wrap(grp_r_xyz)\n",
    "\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=8,opacity=1,color='red')\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make replicates of each \"real\" receiver list: for each CMT source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_grp_recs_by_srcs = make_replicated_station_headers_from_src_list(l_cmt_src,l_real_recs)\n",
    "\n",
    "\n",
    "for i in range(len(l_cmt_src)):\n",
    "    print(f'***** SRC Records for Source: {i} *****\\n')\n",
    "    for j in range(len(l_real_recs)):\n",
    "        print(f'*** REC Header for Receiver: {j} ***\\n{l_grp_recs_by_srcs[i][j]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot \"real\" sources (red) and virtual receivers (white) FROM headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_s_xyz = get_unique_xyz_coords_from_solution_list(l_cmt_src)\n",
    "grp_s_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "flat_recs = flatten_grouped_headers(l_grp_recs_by_srcs) #real!\n",
    "grp_r_xyz = get_unique_xyz_coords_from_station_list(flat_recs)\n",
    "grp_r_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "print(len(grp_s_xyz))\n",
    "print(len(grp_r_xyz))\n",
    "\n",
    "pv_spoints = pv.wrap(grp_s_xyz)\n",
    "pv_rpoints = pv.wrap(grp_r_xyz)\n",
    "\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=12,opacity=1,color='red')\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=8,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reciprical RecordHeader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(flatten_grouped_headers(l_grp_vsrcs.copy())))\n",
    "print(len(flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs.copy()))))\n",
    "print('nrec_per_src*nsrc:',21*12)\n",
    "\n",
    "l_flat_vsrcs = flatten_grouped_headers(l_grp_vsrcs)\n",
    "l_flat_vrecs = flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs))\n",
    "\n",
    "vrecord_h = RecordHeader(name='Reciprocal-Record',solutions_h=l_flat_vsrcs,stations_h=l_flat_vrecs)\n",
    "\n",
    "'''\n",
    "#print(vrecord_h.get_event_nsolutions(1))\n",
    "#print(vrecord_h)\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "slice_rec_h = vrecord_h.copy()\n",
    "slice_rec_h.solutions_df.reset_index(inplace=True)\n",
    "#slice_rec_h.stations_df.reset_index(inplace=True)\n",
    "slice_rec_h.stations_df\n",
    "rec_df = slice_rec_h.stations_df\n",
    "#print(f'rec_df = {rec_df}')\n",
    "for index, src in slice_rec_h.solutions_df.iterrows():\n",
    "    print(f'**** src.sid = {src.sid} ****************\\n')\n",
    "    print(f'**** src.eid = {src.eid} ****************\\n')\n",
    "    #print(rec_df[rec_df['proj_id'] == src.proj_id])\n",
    "    #print(f'index = {index}')\n",
    "    #new_rec_df = rec_df[(rec_df['proj_id'] == src.proj_id) & (rec_df['eid'] == src.eid) & (rec_df['sid'] == src.sid)]\n",
    "    for index,rec in rec_df.loc[idx[src.proj_id,src.eid,src.sid],:].reset_index().iterrows():\n",
    "        print(rec)\n",
    "#slice_rec_h.stations_df.loc[idx[0,0,:,:],'data_fqdn'] = '/somewhere/over/the/rainbow'\n",
    "#print(slice_rec_h.stations_df.loc[idx[0,0,:,:],'data_fqdn'])\n",
    "''';\n",
    "\n",
    "print()\n",
    "svr = vrecord_h[0,:,:,:]\n",
    "print('svr:',svr)\n",
    "svr.solutions_df['proj_id'] = 1\n",
    "svr.stations_df['proj_id'] = 1\n",
    "print()\n",
    "print('proj svr:',svr)\n",
    "\n",
    "#print('index:',svr.index)\n",
    "#print('shape:',svr.index.shape)\n",
    "\n",
    "'''\n",
    "svr_idx_names = svr.index.names\n",
    "print('names:\\n',svr_idx_names)\n",
    "print()\n",
    "print('orig:\\n',svr)\n",
    "svr.reset_index(inplace=True)\n",
    "print()\n",
    "print('reset:\\n',svr)\n",
    "svr['proj_id'] = 1\n",
    "print()\n",
    "print('new proj_id:\\n',svr)\n",
    "svr.set_index(svr_idx_names)\n",
    "print()\n",
    "print('new idx:\\n',svr)\n",
    "''';\n",
    "\n",
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo Make Project Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.write import write_solution\n",
    "from pyaspect.specfemio.write import write_stations\n",
    "from pyaspect.specfemio.write import _write_header\n",
    "\n",
    "def write_record(rdir_fqp,\n",
    "                 record_h,\n",
    "                 fname='event_record',\n",
    "                 write_spec_files=True,\n",
    "                 write_record_h=True,\n",
    "                 write_h=False,\n",
    "                 auto_name=False,\n",
    "                 auto_network=False):\n",
    "\n",
    "    data_fqp   = _join_path_fname(rdir_fqp,'DATA')\n",
    "    record_fqp = _get_header_path(data_fqp,fname)\n",
    "\n",
    "\n",
    "    # write SPECFEM solution files and headers\n",
    "    if write_spec_files:\n",
    "        srcs_df = record_h.solutions_df.copy()\n",
    "        srcs_df.reset_index(inplace=True)\n",
    "\n",
    "        recs_df = record_h.stations_df.copy()\n",
    "        recs_df.reset_index(inplace=True)\n",
    "\n",
    "        i = 0\n",
    "        for sindex,src in srcs_df.iterrows():\n",
    "\n",
    "            mk_sym_link = False\n",
    "            if i == 0: #write SOLUTION symlink\n",
    "                mk_sym_link = True\n",
    "            SolHeaderCls = record_h._get_header_class(is_stations=False)\n",
    "            solution_h = SolHeaderCls.from_series(src)\n",
    "            write_solution(data_fqp,solution_h,postfix=f'sid{src.sid}',write_h=write_h,mk_sym_link=mk_sym_link)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "            #get related stations\n",
    "            slice_recs_df = recs_df[(rec_df['proj_id'] == src.proj_id)]\n",
    "            print(f'slice_recs_df:\\n{slice_recs_df}')\n",
    "            '''\n",
    "            slice_recs_df = recs_df[(rec_df['proj_id'] == src.proj_id) &\n",
    "                                    (rec_df['eid'] == src.eid) &\n",
    "                                    (rec_df['sid'] == src.sid)]\n",
    "\n",
    "            l_stations = []\n",
    "            for rindex,rec in slice_recs_df.iterrows():\n",
    "\n",
    "                #TODO: could make '/SYN' dir variable dynamic\n",
    "                #      also with out knowing specfem DT, can't\n",
    "                #      finish the name of the station data (trace)\n",
    "                data_fname = station_auto_data_fname_id(rec)\n",
    "                rec.data_fqdn = _join_path_fname(edir_fqp,f'/SYN/{data_fname}')\n",
    "\n",
    "                StatHeaderCls = record_h._get_header_class(is_stations=True)\n",
    "                l_stations.append(SolHeaderCls.from_series(rec))\n",
    "\n",
    "\n",
    "            r_fname = f'STATIONS.sid{src.sid}'\n",
    "            write_stations(data_fqp,\n",
    "                           l_stations,\n",
    "                           fname=r_fname,\n",
    "                           write_h=write_h,\n",
    "                           auto_name=auto_name,\n",
    "                           auto_network=auto_network,\n",
    "                           mk_sym_link=mk_sym_link)\n",
    "            ''';\n",
    "\n",
    "    '''\n",
    "    # write record header\n",
    "    if write_record_h:\n",
    "        _write_header(record_fqp,record_h)\n",
    "    ''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.utils import make_record_headers\n",
    "from pyaspect.specfemio.utils import _mk_symlink\n",
    "from pyaspect.specfemio.utils import _copy_recursive_dir\n",
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "from pyaspect.specfemio.utils import _get_header_path\n",
    "from pyaspect.parfile import change_multiple_parameters_in_lines\n",
    "from pyaspect.parfile import readlines\n",
    "from pyaspect.parfile import writelines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MAX_SPEC_SRC = int(9999) # see SPECFEM3D_Cartesian manual\n",
    "\n",
    "# list of directories need for every event\n",
    "common_dir_struct = {'DATA': {},\n",
    "                     'OUTPUT_FILES' : {'DATABASES_MPI':{}},\n",
    "                     'SYN': {},\n",
    "                     'FILT_SYN': {} }\n",
    "\n",
    "# extra common dirs for fwi\n",
    "common_fwi_dir_struct = {'SEM': {},\n",
    "                         'OBS': {},\n",
    "                         'FILT_OBS': {} }\n",
    "\n",
    "# list of directories only needed for the primary run0001 dir\n",
    "primary_dir_struct= {'INPUT_GRADIENT': {},\n",
    "                    'INPUT_KERNELS': {},\n",
    "                    'INPUT_MODEL': {},\n",
    "                    'OUTPUT_MODEL': {},\n",
    "                    'OUTPUT_SUM': {},\n",
    "                    'SMOOTH': {},\n",
    "                    'COMBINE': {},\n",
    "                    'topo': {} }\n",
    "\n",
    "\n",
    "def set_proj_id(record_h, proj_ival):\n",
    "    rec_df = record_h.stations_df\n",
    "    src_df = record_h.solutions_df\n",
    "    rec_names = rec_df.index.names\n",
    "    src_names = src_df.index.names\n",
    "    rec_df.reset_index(inplace=True)\n",
    "    src_df.reset_index(inplace=True)\n",
    "    rec_df['proj_id'] = 1\n",
    "    src_df['proj_id'] = 1\n",
    "    rec_df.set_index(rec_names)\n",
    "    src_df.set_index(src_names)\n",
    "    \n",
    "    \n",
    "\n",
    "def _make_dirs(fqdn,access_rights=0o755):\n",
    "    if os.path.isdir(fqdn):\n",
    "        raise OSError(f'The directory {fqdn} has already been created')\n",
    "    try:\n",
    "        os.makedirs(fqdn, access_rights)\n",
    "    except OSError:\n",
    "        print(f'Creation of the directory {fqdn} failed')\n",
    "        return OSError\n",
    "    \n",
    "    \n",
    "def _recursive_proj_dirs(dl,pdir,access_rights=0o755):\n",
    "\n",
    "    if len(dl.keys()) == 0:\n",
    "        return\n",
    "    else:\n",
    "        for dl_key in dl.keys():\n",
    "            new_dir = os.path.join(pdir, dl_key)\n",
    "            _make_dirs(new_dir,access_rights=0o755)\n",
    "            _recursive_proj_dirs(dl[dl_key],new_dir)\n",
    "\n",
    "\n",
    "def _make_proj_dir(proj_root_fqp,\n",
    "                   proj_base_name,\n",
    "                   pyutils_fqp=None,\n",
    "                   script_fqp=None):\n",
    "\n",
    "        projdir_fqp = os.path.join(proj_root_fqp, proj_base_name)\n",
    "        print(f'projdir_fqp: {projdir_fqp}')\n",
    "        _make_dirs(projdir_fqp)\n",
    "        \n",
    "        # create project level symlinks \n",
    "        if pyutils_fqp != None:\n",
    "            lname = 'pyutils'\n",
    "            src = pyutils_fqp\n",
    "            dst = os.path.join(projdir_fqp, lname)\n",
    "            _mk_symlink(src,dst)\n",
    "\n",
    "        if script_fqp != None:\n",
    "            lname = 'scriptutils'\n",
    "            src = script_fqp\n",
    "            dst = os.path.join(projdir_fqp, lname)\n",
    "            _mk_symlink(src,dst)\n",
    "        \n",
    "        return projdir_fqp\n",
    "            \n",
    "def _make_run_dir(irdir,\n",
    "                  projdir_fqp,\n",
    "                  spec_bin_fqp,\n",
    "                  spec_utils_fqp,\n",
    "                  par_lines,\n",
    "                  dir_struct,\n",
    "                  record_h):\n",
    "\n",
    "    rdir_name = 'run' + str(irdir+1).zfill(4)\n",
    "    rundir_fqp = os.path.join(projdir_fqp, rdir_name)\n",
    "    _make_dirs(rundir_fqp)\n",
    "\n",
    "    # make sim links for each event dir \n",
    "    # (related to the computational node(s) filesytem\n",
    "    lname = 'bin'\n",
    "    src = spec_bin_fqp\n",
    "    dst = os.path.join(rundir_fqp, lname)\n",
    "    _mk_symlink(src,dst)\n",
    "\n",
    "    lname = 'utils'\n",
    "    src = spec_utils_fqp\n",
    "    dst = os.path.join(rundir_fqp, lname)\n",
    "    _mk_symlink(src,dst)\n",
    "\n",
    "    # make subdirectorieds for each event\n",
    "    _recursive_proj_dirs(common_dir_struct,rundir_fqp)\n",
    "\n",
    "    #write Par_files in DATA dirs\n",
    "    ddir_fqp = os.path.join(rundir_fqp, 'DATA')\n",
    "    out_par_fqp  = os.path.join(ddir_fqp, 'Par_file')\n",
    "    writelines(out_par_fqp,par_lines)\n",
    "    \n",
    "    #write Headers and Record\n",
    "    write_record(rundir_fqp,\n",
    "                 record_h,\n",
    "                 fname='rdir_record',\n",
    "                 write_spec_files=True,\n",
    "                 write_record_h=True,\n",
    "                 write_h=False,\n",
    "                 auto_name=True,\n",
    "                 auto_network=True)\n",
    "\n",
    "    \n",
    "\n",
    "def make_fwd_project_dir(proj_base_name,\n",
    "                         proj_root_fqp,\n",
    "                         parfile_fqp,\n",
    "                         mesh_fqp,\n",
    "                         spec_fqp,\n",
    "                         pyutils_fqp,\n",
    "                         script_fqp,\n",
    "                         proj_record_h,\n",
    "                         sub_proj_name=None,\n",
    "                         batch_srcs=False,\n",
    "                         copy_mesh=False,\n",
    "                         max_event_rdirs=MAX_SPEC_SRC,\n",
    "                         verbose=False):\n",
    "    \n",
    "    \n",
    "    if not isinstance(proj_record_h,RecordHeader):\n",
    "        raise TypeError('arg: \\'record_h\\' must be a RecordHeader type')\n",
    "\n",
    "    if not isinstance(proj_base_name,str):\n",
    "        raise TypeError('proj_base_name must be a str type')\n",
    "\n",
    "    if not isinstance(proj_root_fqp,str):\n",
    "        raise TypeError('proj_root_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(parfile_fqp,str):\n",
    "        raise TypeError('parfile_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(mesh_fqp,str):\n",
    "        raise TypeError('mesh_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(spec_fqp,str):\n",
    "        raise TypeError('spec_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(pyutils_fqp,str):\n",
    "        raise TypeError('pyutils_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(script_fqp,str):\n",
    "        raise TypeError('script_fqp must be a str type')\n",
    "        \n",
    "    \n",
    "    ########################################################################\n",
    "    #\n",
    "    # setup project structure parameters\n",
    "    #\n",
    "    ########################################################################\n",
    "            \n",
    "    nevents = 0        \n",
    "    # get unique event id's and make sure the are the same\n",
    "    s_nu_eid = list(src_df.index.get_level_values('eid').unique())\n",
    "    r_nu_eid = list(rec_df.index.get_level_values('eid').unique())\n",
    "    \n",
    "    # checkt that stations and solutions agree on number of events\n",
    "    if s_nu_eid != r_nu_eid:\n",
    "        estr  = 'Number of events differ between Solutions and Stations'\n",
    "        raise Exception(estr)\n",
    "    else:\n",
    "        nevents = len(s_nu_eid)\n",
    "        \n",
    "    \n",
    "    nbatch = 0\n",
    "    # get unique source id's and make sure the are the same\n",
    "    s_nu_sid = list(src_df.index.get_level_values('sid').unique())\n",
    "    r_nu_sid = list(rec_df.index.get_level_values('sid').unique())\n",
    "\n",
    "    # checkt that stations and solutions agree on nbatch\n",
    "    if s_nu_sid != r_nu_sid:\n",
    "        estr  = 'batch_srcs was specified, but unique source id\\'s'\n",
    "        estr += ' differ between Solutions and Stations'\n",
    "        raise Exception(estr)\n",
    "    else:\n",
    "        nbatch = 1\n",
    "        if batch_srcs:\n",
    "            nbatch = len(s_nu_sid)\n",
    "            \n",
    "       \n",
    "    # calculate number of rundirs (events) per subproject/project\n",
    "    l_nrundirs = [nevents]\n",
    "    if not batch_srcs:\n",
    "        l_nrundirs[0] = nevents*len(s_nu_sid)\n",
    "        \n",
    "    # calculate if project or subprojects and addjust rundirs\n",
    "    if max_event_rdirs < l_nrundirs[0]:\n",
    "        old_nrundirs = l_nrundirs[0]\n",
    "        ndiv = l_nrundirs[0]//max_event_rdirs\n",
    "        l_nrundirs[0] = max_event_rdirs\n",
    "        for i in range(1,ndiv):\n",
    "            l_nrundirs.append(max_event_rdirs)\n",
    "        rem_rundirs = old_nrundirs%max_event_rdirs \n",
    "        if rem_rundirs != 0:\n",
    "            l_nrundirs.append(rem_rundirs)\n",
    "            \n",
    "    # actual number of subproject dirs\n",
    "    nprojdirs = len(l_nrundirs)\n",
    "    \n",
    "    # info\n",
    "    if verbose:\n",
    "        print(f'nevents: {nevents}')\n",
    "        print(f'nbatch: {nbatch}')\n",
    "        print(f'l_nrundirs: {l_nrundirs}')\n",
    "        print(f'total_rundirs: {sum(l_nrundirs)}')\n",
    "        print(f'nprojdirs: {nprojdirs}')\n",
    "        \n",
    "        \n",
    "    # setup paths to specfem binarys and utils/tools\n",
    "    spec_bin_fqp   = os.path.join(spec_fqp, 'bin')\n",
    "    spec_utils_fqp   = os.path.join(spec_fqp, 'utils')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Read input Par_file stub \n",
    "    par_lines = readlines(parfile_fqp)\n",
    "    \n",
    "    # Setup output Par_files based on user input Par_file stub\n",
    "    par_keys = ['SIMULATION_TYPE','SAVE_FORWARD','MODEL','SAVE_MESH_FILES','USE_BINARY_FOR_SEISMOGRAMS']\n",
    "    keys_vals_dict = dict(zip(par_keys,[1,False,'gll',False,True]))\n",
    "    par_lines = change_multiple_parameters_in_lines(par_lines,keys_vals_dict)\n",
    "    \n",
    "    \n",
    "    ########################################################################\n",
    "    #\n",
    "    # If more than one proj_dir is needed then make a main proj_dir\n",
    "    # for the sub_proj_dirs.  \n",
    "    #\n",
    "    ########################################################################\n",
    "    \n",
    "    #Setup sub_dir pars here\n",
    "    if sub_proj_name == None:\n",
    "        sub_proj_name = 'Sub_' + proj_base_name\n",
    "        \n",
    "    sub_proj_root_fqp = proj_root_fqp\n",
    "    sub_pdir_name = proj_base_name\n",
    "    \n",
    "    \n",
    "    # make the main_dir if needed\n",
    "    if 1 < nprojdirs:\n",
    "        sub_proj_root_fqp = _make_proj_dir(proj_root_fqp,\n",
    "                                           proj_base_name)\n",
    "        \n",
    "        \n",
    "    if verbose: print(f'sub_proj_root_fqp: {sub_proj_root_fqp}')\n",
    "        \n",
    "    \n",
    "    ie = 0\n",
    "    for ipdir in range(nprojdirs):\n",
    "        \n",
    "        # Make subdir or main project dir depending on number of proj_dirs\n",
    "        sub_projdir_fqp = sub_proj_root_fqp\n",
    "        \n",
    "        if nprojdirs != 1:\n",
    "            sub_pdir_name = sub_proj_name + '_' + str(ipdir+1).zfill(4)\n",
    "            \n",
    "        sub_projdir_fqp = _make_proj_dir(sub_projdir_fqp,\n",
    "                                         sub_pdir_name,\n",
    "                                         pyutils_fqp=pyutils_fqp,\n",
    "                                         script_fqp=script_fqp,)\n",
    "        \n",
    "        \n",
    "        ####################################################################\n",
    "        #\n",
    "        # Make all the run dirs per proj/sub_proj dirs\n",
    "        #\n",
    "        ####################################################################\n",
    "        isrc = 0\n",
    "        for irdir in range(l_nrundirs[ipdir]):\n",
    "            \n",
    "            rdir_record_h = proj_record_h[ie,isrc:isrc+nbatch-1,:,:]\n",
    "            \n",
    "            rdir_record_h.solutions_df['proj_id'] = ipdir\n",
    "            rdir_record_h.stations_df['proj_id']  = ipdir\n",
    " \n",
    "            _make_run_dir(irdir,\n",
    "                          sub_projdir_fqp,\n",
    "                          spec_bin_fqp,\n",
    "                          spec_utils_fqp,\n",
    "                          par_lines,\n",
    "                          common_dir_struct,\n",
    "                          rdir_record_h)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proj_name = 'TestNewMKProject'\n",
    "test_proj_root_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects/NewMKProj')\n",
    "test_parfile_fqp =  os.path.join(data_out_dir, 'Par_file')\n",
    "test_mesh_fqp = '/scratch/seismology/tcullison/test_mesh/MESH-default_batch_force_src'\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/python/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "\n",
    "#print(len(flatten_grouped_headers(l_grp_vsrcs.copy())))\n",
    "#print(len(flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs.copy()))))\n",
    "#print('nrec_per_src*nsrc:',21*12)\n",
    "\n",
    "l_flat_vsrcs = flatten_grouped_headers(l_grp_vsrcs)\n",
    "l_flat_vrecs = flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs))\n",
    "\n",
    "vrecord_h = RecordHeader(name='Reciprocal-Record',solutions_h=l_flat_vsrcs,stations_h=l_flat_vrecs)\n",
    "\n",
    "#print(vrecord_h)\n",
    "src_df = vrecord_h.solutions_df\n",
    "rec_df = vrecord_h.stations_df\n",
    "\n",
    "s_nsid = list(src_df.index.get_level_values('sid').unique())\n",
    "r_nsid = list(rec_df.index.get_level_values('sid').unique())\n",
    "\n",
    "#print(f'Are Same: {s_nsid == r_nsid}')\n",
    "\n",
    "d = {'one':1, 'two':2, 'three': 3}\n",
    "k = list(d.keys())\n",
    "\n",
    "isin = 'three' in k\n",
    "#print(f'is in: {isin}')\n",
    "\n",
    "test_proj_record_h = vrecord_h.copy()\n",
    "\n",
    "make_fwd_project_dir(test_proj_name,\n",
    "                     test_proj_root_fqp,\n",
    "                     test_parfile_fqp,\n",
    "                     test_mesh_fqp,\n",
    "                     test_spec_fqp,\n",
    "                     test_pyutils_fqp,\n",
    "                     test_script_fqp,\n",
    "                     test_proj_record_h,\n",
    "                     batch_srcs=False,\n",
    "                     verbose=True,\n",
    "                     max_event_rdirs=5)\n",
    "                     #max_event_rdirs=MAX_SPEC_SRC)\n",
    "        \n",
    "\n",
    "print()\n",
    "print('ls:')\n",
    "!ls {test_proj_root_fqp}\n",
    "print('ls:')\n",
    "!ls {test_proj_root_fqp}/*/*\n",
    "'''\n",
    "print('rm:')\n",
    "!rm -rf {test_proj_root_fqp}/*\n",
    "print('ls:')\n",
    "!ls {test_proj_root_fqp}\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reciprocal project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False\n",
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "test_proj_name = 'ReciprocalTestProject'\n",
    "test_proj_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects')\n",
    "test_parfile_fqp =  os.path.join(data_out_dir, 'Par_file')\n",
    "test_mesh_fqp = '/scratch/seismology/tcullison/test_mesh/MESH-default_batch_force_src'\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/python/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "\n",
    "from pyaspect.project import make_project\n",
    "#make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "make_project(test_proj_name,\n",
    "             test_proj_fqp,\n",
    "             test_parfile_fqp,\n",
    "             test_mesh_fqp,\n",
    "             test_spec_fqp,\n",
    "             test_pyutils_fqp,\n",
    "             test_script_fqp,\n",
    "             l_grp_vsrcs,\n",
    "             l_grp_vrecs_by_vsrcs,\n",
    "             copy_mesh=False)\n",
    "\n",
    "!ls -ltrh {os.path.join(test_proj_fqp,test_proj_name)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make \"real\" project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "test_proj_name = 'ForwardTestProject'\n",
    "test_proj_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects')\n",
    "test_parfile_fqp =  os.path.join(data_out_dir, 'Par_file')\n",
    "test_mesh_fqp = '/scratch/seismology/tcullison/test_mesh/MESH-default_batch_force_src'\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/python/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "\n",
    "from pyaspect.project import make_project\n",
    "#make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "make_project(test_proj_name,\n",
    "             test_proj_fqp,\n",
    "             test_parfile_fqp,\n",
    "             test_mesh_fqp,\n",
    "             test_spec_fqp,\n",
    "             test_pyutils_fqp,\n",
    "             test_script_fqp,\n",
    "             l_cmt_src,\n",
    "             l_grp_recs_by_srcs,\n",
    "             copy_mesh=False)\n",
    "\n",
    "!ls -ltrh {os.path.join(test_proj_fqp,test_proj_name)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyaspect.parfile import change_parameter\n",
    "from pyaspect.parfile import change_multiple_parameters\n",
    "\n",
    "parfile_fqp = os.path.join(data_out_dir,'Par_file')\n",
    "out_parfile_fqp = os.path.join(data_out_dir,'New_Par_file')\n",
    "\n",
    "with open(parfile_fqp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "il = 0\n",
    "for line in lines:\n",
    "    if il == 313:\n",
    "        break\n",
    "    print(line)\n",
    "    il += 1\n",
    "'''\n",
    "''';\n",
    "    \n",
    "#change_parameter(parfile_fqp,lines,'SAVE_FORWARD',True,out_parfile_fqp=out_parfile_fqp)\n",
    "par_keys = ['SIMULATION_TYPE','SAVE_FORWARD','NSTEP','DT','MODEL','MIN_ATTENUATION_PERIOD','LOCAL_PATH','SAVE_MESH_FILES']\n",
    "par_vals = [4,True,8192,0.002,'default',666666666,'./BOOGER/in_my_nose',True]\n",
    "s_keys_vals = set(zip(par_keys,par_vals))\n",
    "print(f's_keys_vals:\\n{s_keys_vals}')\n",
    "print(f'd_keys_vals:\\n{[(k,v) for k,v in dict(zip(par_keys,par_vals)).items()]}')\n",
    "\n",
    "change_multiple_parameters(parfile_fqp,s_keys_vals,out_parfile_fqp=out_parfile_fqp)\n",
    "    \n",
    "\n",
    "print('\\n\\n----------------------- changed -------------------------')\n",
    "\n",
    "with open(out_parfile_fqp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "il = 0\n",
    "for line in lines:\n",
    "    if il == 313:\n",
    "        break\n",
    "    print(line)\n",
    "    il += 1\n",
    "'''\n",
    "''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!diff {parfile_fqp} {out_parfile_fqp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "out_parfile_json = os.path.join(data_out_dir,'Par_file.json')\n",
    "with open(out_parfile_json, 'w') as f:\n",
    "    json.dump(lines,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parfile_fqp = os.path.join(data_out_dir,'Par_file')\n",
    "with open(parfile_fqp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "parfile_pickle = pickle.dumps(lines, 0)\n",
    "parfile_pickle_str = parfile_pickle.decode('ascii')\n",
    "#parfile_pickle_str = str(parfile_pickle)\n",
    "\n",
    "#print(f'Parfile Type:\\n{type(parfile_pickle)}')\n",
    "print(f'Parfile String:\\n{type(parfile_pickle_str)}')\n",
    "print(f'Parfile String:\\n{parfile_pickle_str.__repr__()}')\n",
    "\n",
    "new_par_bytes = parfile_pickle_str.encode('ascii')\n",
    "new_lines = pickle.loads(new_par_bytes)\n",
    "\n",
    "'''\n",
    "print('---------- The Winding Road -----------------')\n",
    "for line in new_lines:\n",
    "    print(line)\n",
    "''';\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parfile_str = \"\"\"(lp0\\nV#-----------------------------------------------------------\\\\u000a\\np1\\naV#\\\\u000a\\np2\\naV# Simulation input parameters\\\\u000a\\np3\\naV#\\\\u000a\\np4\\naV#-----------------------------------------------------------\\\\u000a\\np5\\naV\\\\u000a\\np6\\naV# forward or adjoint simulation\\\\u000a\\np7\\naV# 1 = forward, 2 = adjoint, 3 = both simultaneously\\\\u000a\\np8\\naVSIMULATION_TYPE                 = 1\\\\u000a\\np9\\naV# 0 = earthquake simulation,  1/2/3 = three steps in noise simulation\\\\u000a\\np10\\naVNOISE_TOMOGRAPHY                = 0\\\\u000a\\np11\\naVSAVE_FORWARD                    = .false.\\\\u000a\\np12\\nag6\\naV# solve a full FWI inverse problem from a single calling program with no I/Os, storing everything in memory,\\\\u000a\\np13\\naV# or run a classical forward or adjoint problem only and save the seismograms and/or sensitivity kernels to disk (with costlier I/Os)\\\\u000a\\np14\\naVINVERSE_FWI_FULL_PROBLEM        = .false.\\\\u000a\\np15\\nag6\\naV# UTM projection parameters\\\\u000a\\np16\\naV# Use a negative zone number for the Southern hemisphere:\\\\u000a\\np17\\naV# The Northern hemisphere corresponds to zones +1 to +60,\\\\u000a\\np18\\naV# The Southern hemisphere corresponds to zones -1 to -60.\\\\u000a\\np19\\naVUTM_PROJECTION_ZONE             = 11\\\\u000a\\np20\\naVSUPPRESS_UTM_PROJECTION         = .true.\\\\u000a\\np21\\nag6\\naV# number of MPI processors\\\\u000a\\np22\\naVNPROC                           = 4 \\\\u000a\\np23\\nag6\\naV# time step parameters\\\\u000a\\np24\\naVNSTEP                           = 4096\\\\u000a\\np25\\naVDT                              = 0.001\\\\u000a\\np26\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np27\\naV#\\\\u000a\\np28\\naV# LDDRK time scheme\\\\u000a\\np29\\naV#\\\\u000a\\np30\\naV#-----------------------------------------------------------\\\\u000a\\np31\\naVUSE_LDDRK                       = .false.\\\\u000a\\np32\\naVINCREASE_CFL_FOR_LDDRK          = .false.\\\\u000a\\np33\\naVRATIO_BY_WHICH_TO_INCREASE_IT   = 1.4\\\\u000a\\np34\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np35\\naV#\\\\u000a\\np36\\naV# Mesh\\\\u000a\\np37\\naV#\\\\u000a\\np38\\naV#-----------------------------------------------------------\\\\u000a\\np39\\nag6\\naV# Number of nodes for 2D and 3D shape functions for hexahedra.\\\\u000a\\np40\\naV# We use either 8-node mesh elements (bricks) or 27-node elements.\\\\u000a\\np41\\naV# If you use our internal mesher, the only option is 8-node bricks (27-node elements are not supported).\\\\u000a\\np42\\naVNGNOD                           = 8\\\\u000a\\np43\\nag6\\naV# models:\\\\u000a\\np44\\naV# available options are:\\\\u000a\\np45\\naV#   default (model parameters described by mesh properties)\\\\u000a\\np46\\naV# 1D models available are:\\\\u000a\\np47\\naV#   1d_prem,1d_socal,1d_cascadia\\\\u000a\\np48\\naV# 3D models available are:\\\\u000a\\np49\\naV#   aniso,external,gll,salton_trough,tomo,SEP,coupled,...\\\\u000a\\np50\\naVMODEL                           = gll\\\\u000a\\np51\\nag6\\naV# path for external tomographic models files\\\\u000a\\np52\\naVTOMOGRAPHY_PATH                 = ./DATA/tomo_files/\\\\u000a\\np53\\naV# if you are using a SEP model (oil-industry format)\\\\u000a\\np54\\naVSEP_MODEL_DIRECTORY             = ./DATA/my_SEP_model/\\\\u000a\\np55\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np56\\nag6\\naV# parameters describing the model\\\\u000a\\np57\\naVAPPROXIMATE_OCEAN_LOAD          = .false.\\\\u000a\\np58\\naVTOPOGRAPHY                      = .false.\\\\u000a\\np59\\naVATTENUATION                     = .false.\\\\u000a\\np60\\naVANISOTROPY                      = .false.\\\\u000a\\np61\\naVGRAVITY                         = .false.\\\\u000a\\np62\\nag6\\naV# in case of attenuation, reference frequency in Hz at which the velocity values in the velocity model are given (unused otherwise)\\\\u000a\\np63\\naVATTENUATION_f0_REFERENCE        = 18.d0\\\\u000a\\np64\\nag6\\naV# attenuation period range over which we try to mimic a constant Q factor\\\\u000a\\np65\\naVMIN_ATTENUATION_PERIOD          = 999999998.d0\\\\u000a\\np66\\naVMAX_ATTENUATION_PERIOD          = 999999999.d0\\\\u000a\\np67\\naV# ignore this range and ask the code to compute it automatically instead based on the estimated resolution of the mesh (use this unless you know what you are doing)\\\\u000a\\np68\\naVCOMPUTE_FREQ_BAND_AUTOMATIC     = .true.\\\\u000a\\np69\\nag6\\naV# Olsen\\'s constant for Q_mu = constant * V_s attenuation rule\\\\u000a\\np70\\naVUSE_OLSEN_ATTENUATION           = .false.\\\\u000a\\np71\\naVOLSEN_ATTENUATION_RATIO         = 0.05\\\\u000a\\np72\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np73\\naV#\\\\u000a\\np74\\naV# Absorbing boundary conditions\\\\u000a\\np75\\naV#\\\\u000a\\np76\\naV#-----------------------------------------------------------\\\\u000a\\np77\\nag6\\naV# C-PML boundary conditions for a regional simulation\\\\u000a\\np78\\naV# (if set to .false., and STACEY_ABSORBING_CONDITIONS is also set to .false., you get a free surface instead\\\\u000a\\np79\\naV# in the case of elastic or viscoelastic mesh elements, and a rigid surface in the case of acoustic (fluid) elements\\\\u000a\\np80\\naVPML_CONDITIONS                  = .false.\\\\u000a\\np81\\nag6\\naV# C-PML top surface\\\\u000a\\np82\\naVPML_INSTEAD_OF_FREE_SURFACE     = .false.\\\\u000a\\np83\\nag6\\naV# C-PML dominant frequency\\\\u000a\\np84\\naVf0_FOR_PML                      = 0.05555\\\\u000a\\np85\\nag6\\naV# parameters used to rotate C-PML boundary conditions by a given angle (not completed yet)\\\\u000a\\np86\\naV# ROTATE_PML_ACTIVATE           = .false.\\\\u000a\\np87\\naV# ROTATE_PML_ANGLE              = 0.\\\\u000a\\np88\\nag6\\naV# absorbing boundary conditions for a regional simulation\\\\u000a\\np89\\naV# (if set to .false., and PML_CONDITIONS is also set to .false., you get a free surface instead\\\\u000a\\np90\\naV# in the case of elastic or viscoelastic mesh elements, and a rigid surface in the case of acoustic (fluid) elements\\\\u000a\\np91\\naVSTACEY_ABSORBING_CONDITIONS     = .true.\\\\u000a\\np92\\nag6\\naV# absorbing top surface (defined in mesh as \\'free_surface_file\\')\\\\u000a\\np93\\naVSTACEY_INSTEAD_OF_FREE_SURFACE  = .false.\\\\u000a\\np94\\nag6\\naV# When STACEY_ABSORBING_CONDITIONS is set to .true. :\\\\u000a\\np95\\naV# absorbing conditions are defined in xmin, xmax, ymin, ymax and zmin\\\\u000a\\np96\\naV# this option BOTTOM_FREE_SURFACE can be set to .true. to\\\\u000a\\np97\\naV# make zmin free surface instead of absorbing condition\\\\u000a\\np98\\naVBOTTOM_FREE_SURFACE             = .false.\\\\u000a\\np99\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np100\\naV#\\\\u000a\\np101\\naV# undoing attenuation and/or PMLs for sensitivity kernel calculations\\\\u000a\\np102\\naV#\\\\u000a\\np103\\naV#-----------------------------------------------------------\\\\u000a\\np104\\nag6\\naV# to undo attenuation and/or PMLs for sensitivity kernel calculations or forward runs with SAVE_FORWARD\\\\u000a\\np105\\naV# use the flag below. It performs undoing of attenuation and/or of PMLs in an exact way for sensitivity kernel calculations\\\\u000a\\np106\\naV# but requires disk space for temporary storage, and uses a significant amount of memory used as buffers for temporary storage.\\\\u000a\\np107\\naV# When that option is on the second parameter indicates how often the code dumps restart files to disk (if in doubt, use something between 100 and 1000).\\\\u000a\\np108\\naVUNDO_ATTENUATION_AND_OR_PML     = .false.\\\\u000a\\np109\\naVNT_DUMP_ATTENUATION             = 500\\\\u000a\\np110\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np111\\naV#\\\\u000a\\np112\\naV# Visualization\\\\u000a\\np113\\naV#\\\\u000a\\np114\\naV#-----------------------------------------------------------\\\\u000a\\np115\\nag6\\naV# save AVS or OpenDX movies\\\\u000a\\np116\\naV# MOVIE_TYPE = 1 to show the top surface\\\\u000a\\np117\\naV# MOVIE_TYPE = 2 to show all the external faces of the mesh\\\\u000a\\np118\\naVCREATE_SHAKEMAP                 = .false.\\\\u000a\\np119\\naVMOVIE_SURFACE                   = .false.\\\\u000a\\np120\\naVMOVIE_TYPE                      = 2\\\\u000a\\np121\\naVMOVIE_VOLUME                    = .false.\\\\u000a\\np122\\naVSAVE_DISPLACEMENT               = .true.\\\\u000a\\np123\\naVUSE_HIGHRES_FOR_MOVIES          = .false.\\\\u000a\\np124\\naVNTSTEP_BETWEEN_FRAMES           = 500\\\\u000a\\np125\\naVHDUR_MOVIE                      = 0.0\\\\u000a\\np126\\nag6\\naV# save AVS or OpenDX mesh files to check the mesh\\\\u000a\\np127\\naVSAVE_MESH_FILES                 = .false.\\\\u000a\\np128\\nag6\\naV# path to store the local database file on each node\\\\u000a\\np129\\naVLOCAL_PATH                      = ./OUTPUT_FILES/DATABASES_MPI\\\\u000a\\np130\\naV#LOCAL_PATH                      = ./COMMON/DATABASES_MPI\\\\u000a\\np131\\nag6\\naV# interval at which we output time step info and max of norm of displacement\\\\u000a\\np132\\naVNTSTEP_BETWEEN_OUTPUT_INFO      = 1000\\\\u000a\\np133\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np134\\naV#\\\\u000a\\np135\\naV# Sources\\\\u000a\\np136\\naV#\\\\u000a\\np137\\naV#-----------------------------------------------------------\\\\u000a\\np138\\nag6\\naV# sources and receivers Z coordinates given directly (i.e. as their true position) instead of as their depth\\\\u000a\\np139\\naVUSE_SOURCES_RECEIVERS_Z         = .false.\\\\u000a\\np140\\nag6\\naV# use a (tilted) FORCESOLUTION force point source (or several) instead of a CMTSOLUTION moment-tensor source.\\\\u000a\\np141\\naV# This can be useful e.g. for oil industry foothills simulations or asteroid simulations\\\\u000a\\np142\\naV# in which the source is a vertical force, normal force, tilted force, impact etc.\\\\u000a\\np143\\naV# If this flag is turned on, the FORCESOLUTION file must be edited by giving:\\\\u000a\\np144\\naV# - the corresponding time-shift parameter,\\\\u000a\\np145\\naV# - the half duration parameter of the source,\\\\u000a\\np146\\naV# - the coordinates of the source,\\\\u000a\\np147\\naV# - the magnitude of the force source,\\\\u000a\\np148\\naV# - the components of a (non necessarily unitary) direction vector for the force source in the E/N/Z_UP basis.\\\\u000a\\np149\\naV# The direction vector is made unitary internally in the code and thus only its direction matters here;\\\\u000a\\np150\\naV# its norm is ignored and the norm of the force used is the factor force source times the source time function.\\\\u000a\\np151\\naVUSE_FORCE_POINT_SOURCE          = .true.\\\\u000a\\np152\\nag6\\naV# set to true to use a Ricker source time function instead of the source time functions set by default\\\\u000a\\np153\\naV# to represent a (tilted) FORCESOLUTION force point source or a CMTSOLUTION moment-tensor source.\\\\u000a\\np154\\naVUSE_RICKER_TIME_FUNCTION        = .false.\\\\u000a\\np155\\nag6\\naV# Use an external source time function.\\\\u000a\\np156\\naV# if .true. you must add a file with your source time function and the file name\\\\u000a\\np157\\naV# path relative to lauching directory at the end of CMTSOLUTION or FORCESOURCE file\\\\u000a\\np158\\naV# (with multiple sources, one file per source is required).\\\\u000a\\np159\\naV# This file must have a single column containing the amplitude of the source at that time step,\\\\u000a\\np160\\naV# and on its first line it must contain the time step used, which must be equal to DT as defined at the beginning of this Par_file (the code will check that).\\\\u000a\\np161\\naV# Be sure when this option is .false. to remove the name of stf file in CMTSOLUTION or FORCESOURCE\\\\u000a\\np162\\naVUSE_EXTERNAL_SOURCE_FILE        = .false.\\\\u000a\\np163\\nag6\\naV# print source time function\\\\u000a\\np164\\naVPRINT_SOURCE_TIME_FUNCTION      = .true.\\\\u000a\\np165\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np166\\naV#\\\\u000a\\np167\\naV# Seismograms\\\\u000a\\np168\\naV#\\\\u000a\\np169\\naV#-----------------------------------------------------------\\\\u000a\\np170\\nag6\\naV# interval in time steps for writing of seismograms\\\\u000a\\np171\\naVNTSTEP_BETWEEN_OUTPUT_SEISMOS   = 4096\\\\u000a\\np172\\nag6\\naV# decide if we save displacement, velocity, acceleration and/or pressure in forward runs (they can be set to true simultaneously)\\\\u000a\\np173\\naV# currently pressure seismograms are implemented in acoustic (i.e. fluid) elements only\\\\u000a\\np174\\naVSAVE_SEISMOGRAMS_DISPLACEMENT   = .true.\\\\u000a\\np175\\naVSAVE_SEISMOGRAMS_VELOCITY       = .false.\\\\u000a\\np176\\naVSAVE_SEISMOGRAMS_ACCELERATION   = .false.\\\\u000a\\np177\\naVSAVE_SEISMOGRAMS_PRESSURE       = .false.   # currently implemented in acoustic (i.e. fluid) elements only\\\\u000a\\np178\\nag6\\naV# save seismograms also when running the adjoint runs for an inverse problem\\\\u000a\\np179\\naV# (usually they are unused and not very meaningful, leave this off in almost all cases)\\\\u000a\\np180\\naVSAVE_SEISMOGRAMS_IN_ADJOINT_RUN = .false.\\\\u000a\\np181\\nag6\\naV# save seismograms in binary or ASCII format (binary is smaller but may not be portable between machines)\\\\u000a\\np182\\naVUSE_BINARY_FOR_SEISMOGRAMS      = .false.\\\\u000a\\np183\\nag6\\naV# output seismograms in Seismic Unix format (binary with 240-byte-headers)\\\\u000a\\np184\\naVSU_FORMAT                       = .false.\\\\u000a\\np185\\nag6\\naV# output seismograms in ASDF (requires asdf-library)\\\\u000a\\np186\\naVASDF_FORMAT                     = .false.\\\\u000a\\np187\\nag6\\naV# decide if master process writes all the seismograms or if all processes do it in parallel\\\\u000a\\np188\\naVWRITE_SEISMOGRAMS_BY_MASTER     = .false.\\\\u000a\\np189\\nag6\\naV# save all seismograms in one large combined file instead of one file per seismogram\\\\u000a\\np190\\naV# to avoid overloading shared non-local file systems such as LUSTRE or GPFS for instance\\\\u000a\\np191\\naVSAVE_ALL_SEISMOS_IN_ONE_FILE    = .false.\\\\u000a\\np192\\nag6\\naV# use a trick to increase accuracy of pressure seismograms in fluid (acoustic) elements:\\\\u000a\\np193\\naV# use the second derivative of the source for the source time function instead of the source itself,\\\\u000a\\np194\\naV# and then record -potential_acoustic() as pressure seismograms instead of -potential_dot_dot_acoustic();\\\\u000a\\np195\\naV# this is mathematically equivalent, but numerically significantly more accurate because in the explicit\\\\u000a\\np196\\naV# Newmark time scheme acceleration is accurate at zeroth order while displacement is accurate at second order,\\\\u000a\\np197\\naV# thus in fluid elements potential_dot_dot_acoustic() is accurate at zeroth order while potential_acoustic()\\\\u000a\\np198\\naV# is accurate at second order and thus contains significantly less numerical noise.\\\\u000a\\np199\\naVUSE_TRICK_FOR_BETTER_PRESSURE   = .false.\\\\u000a\\np200\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np201\\naV#\\\\u000a\\np202\\naV# Source encoding\\\\u000a\\np203\\naV#\\\\u000a\\np204\\naV#-----------------------------------------------------------\\\\u000a\\np205\\nag6\\naV# (for acoustic simulations only for now) determines source encoding factor +/-1 depending on sign of moment tensor\\\\u000a\\np206\\naV# (see e.g. Krebs et al., 2009. Fast full-wavefield seismic inversion using encoded sources, Geophysics, 74 (6), WCC177-WCC188.)\\\\u000a\\np207\\naVUSE_SOURCE_ENCODING             = .false.\\\\u000a\\np208\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np209\\naV#\\\\u000a\\np210\\naV# Energy calculation\\\\u000a\\np211\\naV#\\\\u000a\\np212\\naV#-----------------------------------------------------------\\\\u000a\\np213\\nag6\\naV# to plot energy curves, for instance to monitor how CPML absorbing layers behave;\\\\u000a\\np214\\naV# should be turned OFF in most cases because a bit expensive\\\\u000a\\np215\\naVOUTPUT_ENERGY                   = .false.\\\\u000a\\np216\\naV# every how many time steps we compute energy (which is a bit expensive to compute)\\\\u000a\\np217\\naVNTSTEP_BETWEEN_OUTPUT_ENERGY    = 10\\\\u000a\\np218\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np219\\naV#\\\\u000a\\np220\\naV# Adjoint kernel outputs\\\\u000a\\np221\\naV#\\\\u000a\\np222\\naV#-----------------------------------------------------------\\\\u000a\\np223\\nag6\\naV# interval in time steps for reading adjoint traces\\\\u000a\\np224\\naV# 0 = read the whole adjoint sources at the same time\\\\u000a\\np225\\naVNTSTEP_BETWEEN_READ_ADJSRC      = 0\\\\u000a\\np226\\nag6\\naV# read adjoint sources using ASDF (requires asdf-library)\\\\u000a\\np227\\naVREAD_ADJSRC_ASDF                = .false.\\\\u000a\\np228\\nag6\\naV# this parameter must be set to .true. to compute anisotropic kernels\\\\u000a\\np229\\naV# in crust and mantle (related to the 21 Cij in geographical coordinates)\\\\u000a\\np230\\naV# default is .false. to compute isotropic kernels (related to alpha and beta)\\\\u000a\\np231\\naVANISOTROPIC_KL                  = .false.\\\\u000a\\np232\\nag6\\naV# compute transverse isotropic kernels (alpha_v,alpha_h,beta_v,beta_h,eta,rho)\\\\u000a\\np233\\naV# rather than fully anisotropic kernels in case ANISOTROPIC_KL is set to .true.\\\\u000a\\np234\\naVSAVE_TRANSVERSE_KL              = .false.\\\\u000a\\np235\\nag6\\naV# this parameter must be set to .true. to compute anisotropic kernels for\\\\u000a\\np236\\naV# cost function using velocity observable rather than displacement\\\\u000a\\np237\\naVANISOTROPIC_VELOCITY_KL         = .false.\\\\u000a\\np238\\nag6\\naV# outputs approximate Hessian for preconditioning\\\\u000a\\np239\\naV#APPROXIMATE_HESS_KL             = .false. ##per Ebru do use for\\\\u000a\\np240\\naV#preconditioning\\\\u000a\\np241\\naVAPPROXIMATE_HESS_KL             = .true.\\\\u000a\\np242\\nag6\\naV# save Moho mesh and compute Moho boundary kernels\\\\u000a\\np243\\naVSAVE_MOHO_MESH                  = .false.\\\\u000a\\np244\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np245\\naV#\\\\u000a\\np246\\naV# Coupling with an injection technique (DSM, AxiSEM, or FK)\\\\u000a\\np247\\naV#\\\\u000a\\np248\\naV#-----------------------------------------------------------\\\\u000a\\np249\\nag6\\naVCOUPLE_WITH_INJECTION_TECHNIQUE = .false.\\\\u000a\\np250\\naVINJECTION_TECHNIQUE_TYPE        = 3   # 1 = DSM, 2 = AxiSEM, 3 = FK\\\\u000a\\np251\\naVMESH_A_CHUNK_OF_THE_EARTH       = .false.\\\\u000a\\np252\\naVTRACTION_PATH                   = ./DATA/AxiSEM_tractions/3/\\\\u000a\\np253\\naVFKMODEL_FILE                    = FKmodel\\\\u000a\\np254\\naVRECIPROCITY_AND_KH_INTEGRAL     = .false.   # does not work yet\\\\u000a\\np255\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np256\\nag6\\naV# Dimitri Komatitsch, July 2014, CNRS Marseille, France:\\\\u000a\\np257\\naV# added the ability to run several calculations (several earthquakes)\\\\u000a\\np258\\naV# in an embarrassingly-parallel fashion from within the same run;\\\\u000a\\np259\\naV# this can be useful when using a very large supercomputer to compute\\\\u000a\\np260\\naV# many earthquakes in a catalog, in which case it can be better from\\\\u000a\\np261\\naV# a batch job submission point of view to start fewer and much larger jobs,\\\\u000a\\np262\\naV# each of them computing several earthquakes in parallel.\\\\u000a\\np263\\naV# To turn that option on, set parameter NUMBER_OF_SIMULTANEOUS_RUNS to a value greater than 1.\\\\u000a\\np264\\naV# To implement that, we create NUMBER_OF_SIMULTANEOUS_RUNS MPI sub-communicators,\\\\u000a\\np265\\naV# each of them being labeled \"my_local_mpi_comm_world\", and we use them\\\\u000a\\np266\\naV# in all the routines in \"src/shared/parallel.f90\", except in MPI_ABORT() because in that case\\\\u000a\\np267\\naV# we need to kill the entire run.\\\\u000a\\np268\\naV# When that option is on, of course the number of processor cores used to start\\\\u000a\\np269\\naV# the code in the batch system must be a multiple of NUMBER_OF_SIMULTANEOUS_RUNS,\\\\u000a\\np270\\naV# all the individual runs must use the same number of processor cores,\\\\u000a\\np271\\naV# which as usual is NPROC in the Par_file,\\\\u000a\\np272\\naV# and thus the total number of processor cores to request from the batch system\\\\u000a\\np273\\naV# should be NUMBER_OF_SIMULTANEOUS_RUNS * NPROC.\\\\u000a\\np274\\naV# All the runs to perform must be placed in directories called run0001, run0002, run0003 and so on\\\\u000a\\np275\\naV# (with exactly four digits).\\\\u000a\\np276\\naV#\\\\u000a\\np277\\naV# Imagine you have 10 independent calculations to do, each of them on 100 cores; you have three options:\\\\u000a\\np278\\naV#\\\\u000a\\np279\\naV# 1/ submit 10 jobs to the batch system\\\\u000a\\np280\\naV#\\\\u000a\\np281\\naV# 2/ submit a single job on 1000 cores to the batch, and in that script create a sub-array of jobs to start 10 jobs,\\\\u000a\\np282\\naV# each running on 100 cores (see e.g. http://www.schedmd.com/slurmdocs/job_array.html )\\\\u000a\\np283\\naV#\\\\u000a\\np284\\naV# 3/ submit a single job on 1000 cores to the batch, start SPECFEM3D on 1000 cores, create 10 sub-communicators,\\\\u000a\\np285\\naV# cd into one of 10 subdirectories (called e.g. run0001, run0002,... run0010) depending on the sub-communicator\\\\u000a\\np286\\naV# your MPI rank belongs to, and run normally on 100 cores using that sub-communicator.\\\\u000a\\np287\\naV#\\\\u000a\\np288\\naV# The option below implements 3/.\\\\u000a\\np289\\naV#\\\\u000a\\np290\\naVNUMBER_OF_SIMULTANEOUS_RUNS     = 1\\\\u000a\\np291\\nag6\\naV# if we perform simultaneous runs in parallel, if only the source and receivers vary between these runs\\\\u000a\\np292\\naV# but not the mesh nor the model (velocity and density) then we can also read the mesh and model files\\\\u000a\\np293\\naV# from a single run in the beginning and broadcast them to all the others; for a large number of simultaneous\\\\u000a\\np294\\naV# runs for instance when solving inverse problems iteratively this can DRASTICALLY reduce I/Os to disk in the solver\\\\u000a\\np295\\naV# (by a factor equal to NUMBER_OF_SIMULTANEOUS_RUNS), and reducing I/Os is crucial in the case of huge runs.\\\\u000a\\np296\\naV# Thus, always set this option to .true. if the mesh and the model are the same for all simultaneous runs.\\\\u000a\\np297\\naV# In that case there is no need to duplicate the mesh and model file database (the content of the DATABASES_MPI\\\\u000a\\np298\\naV# directories) in each of the run0001, run0002,... directories, it is sufficient to have one in run0001\\\\u000a\\np299\\naV# and the code will broadcast it to the others)\\\\u000a\\np300\\naVBROADCAST_SAME_MESH_AND_MODEL   = .true.\\\\u000a\\np301\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np302\\nag6\\naV# set to true to use GPUs\\\\u000a\\np303\\naVGPU_MODE                        = .true.\\\\u000a\\np304\\nag6\\naV# ADIOS Options for I/Os\\\\u000a\\np305\\naVADIOS_ENABLED                   = .false.\\\\u000a\\np306\\naVADIOS_FOR_DATABASES             = .false.\\\\u000a\\np307\\naVADIOS_FOR_MESH                  = .false.\\\\u000a\\np308\\naVADIOS_FOR_FORWARD_ARRAYS        = .false.\\\\u000a\\np309\\naVADIOS_FOR_KERNELS               = .false.\\\\u000a\\np310\\nag6\\naV#**********************\\\\u000a\\np311\\naV#Serial mesh decomposer\\\\u000a\\np312\\naV#**********************\\\\u000a\\np313\\nag6\\naVLTS_MODE                        = .false.\\\\u000a\\np314\\nag6\\naVPARTITIONING_TYPE               = 1\\\\u000a\\np315\\na.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_parfile_str == parfile_pickle_str)\n",
    "#print(set(test_parfile_str.split()).difference(set(parfile_pickle_str.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = goobleydoobleydoo('42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sillyfunc(index=None):\n",
    "    if index == None:\n",
    "        print('none')\n",
    "    else:\n",
    "        print(f'index={index}')\n",
    "        \n",
    "sillyfunc(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = []\n",
    "for i in range(6):\n",
    "    df_rec.append({'sid':i%3,'eid':0})\n",
    "df = pd.DataFrame.from_records(df_rec)\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(df['sid'])\n",
    "print()\n",
    "s = set(df['sid'])\n",
    "ss = set(df['sid'])\n",
    "print(s)\n",
    "print(f's == ss: {s == ss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fqp = os.path.join(data_out_dir,'tmp')\n",
    "!ls {tmp_fqp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "for src_path in glob(data_out_dir):\n",
    "    print(src_path)\n",
    "    print(os.path.relpath(src_path, 'tmp'))\n",
    "    '''\n",
    "    os.symlink(\n",
    "        os.path.relpath(\n",
    "            src_path,\n",
    "            'dst/'\n",
    "        ),\n",
    "        os.path.join('dst', os.path.basename(src_path))\n",
    "    )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = os.path.relpath('tmp', start=os.curdir)\n",
    "p = os.path.relpath(tmp_fqp, start='data/')\n",
    "fqpname = os.path.join(p, 'tmp')\n",
    "\n",
    "print(p)\n",
    "print(fqpname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpath = 'data/output/tmp/TestProjects/FirstTestProject/run0001/DATA'\n",
    "os.symlink('STAIONS.sid0','data/output/tmp/TestProjects/FirstTestProject/run0001/DATA/LINKSTATIONS')\n",
    "!ls -ltrh {tpath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mesh_root = '/Users/seismac/Documents/Work/Bench/ForkGnam/pyaspect/notebooks/data/output/tmp/TestProjects/'\n",
    "test_mesh_name = 'MESH-default_norot_xsft4400_ysft19100_quart_100m_min_vs_sig250m'\n",
    "test_src = os.path.join(test_mesh_root,test_mesh_name)\n",
    "test_dst = os.path.join(test_mesh_root,'copy_MESH-default')\n",
    "print(test_src)\n",
    "print(test_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.copytree(test_src, test_dst)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -dltrh {test_src}\n",
    "!ls -dltrh {test_dst}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls {data_out_dir}\n",
    "ztr = np.fromfile(data_out_dir + 's00.t000960g03.FXZ.semd',dtype=np.float32)\n",
    "\n",
    "plt.plot(ztr[1243:1260])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(data_out_dir + 's00.t000960g03.FXZ.npz',ztr=ztr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dir = os.path.join(data_out_dir,'SYN')\n",
    "'''\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        print os.path.join(subdir, file)\n",
    "''';\n",
    "        \n",
    "all_traces = []\n",
    "i = 0\n",
    "for entry in os.scandir(start_dir):\n",
    "    #if entry.path.endswith('FXZ.semd'):\n",
    "    if entry.path.endswith('.semd'):\n",
    "        all_traces.append(np.fromfile(entry.path,dtype=np.float32))\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "stk_traces = np.stack(all_traces,axis=0)\n",
    "print(stk_traces.shape)\n",
    "print(stk_traces.dtype)\n",
    "print(stk_traces.nbytes/(1024**2))\n",
    "\n",
    "out_comp_file = os.path.join(start_dir , 'stk_traces_compressed.npz')\n",
    "np.savez_compressed(out_comp_file,stk_traces=stk_traces)\n",
    "!ls -ltrh {out_comp_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /Users/seismac/Seismic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xran = np.random.randn(3, 4)\n",
    "print('xran:\\n',xran.shape)\n",
    "arrays = [np.random.randn(3, 4) for _ in range(10)]\n",
    "print('arrays:\\n',arrays)\n",
    "\n",
    "stk = np.stack(arrays, axis=0)\n",
    "print('stk:\\n',stk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
