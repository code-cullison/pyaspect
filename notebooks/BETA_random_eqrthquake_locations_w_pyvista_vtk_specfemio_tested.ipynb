{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA_random_eqrthquake_locations_w_pyvista_vtk\n",
    "\n",
    "Testing the random picking of subsurface eqrthquake locations (moment-tensor locations) and 3D plotting of the picks and model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all packages\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sys import argv\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from pyaspect.model.gridmod3d import gridmod3d as gm\n",
    "from pyaspect.model.bbox import bbox as bb\n",
    "from pyaspect.model.gm3d_utils import compress_gm3d_to_file\n",
    "from pyaspect.model.gm3d_utils import decompress_gm3d_from_file\n",
    "from pyaspect.moment_tensor import MomentTensor\n",
    "from pyaspect.specfemio.headers import StationHeader\n",
    "from pyaspect.specfemio.headers import SolutionHeader\n",
    "from pyaspect.specfemio.headers import CMTSolutionHeader\n",
    "from pyaspect.specfemio.headers import ForceSolutionHeader\n",
    "from pyaspect.specfemio.write import write_cmtsolution\n",
    "from pyaspect.specfemio.write import write_forcesolution\n",
    "from pyaspect.specfemio.write import write_stations\n",
    "from pyaspect.specfemio.read import read_stations\n",
    "from pyaspect.specfemio.read import read_solution\n",
    "from pyaspect.specfemio.read import read_cmtsolution\n",
    "from pyaspect.specfemio.read import read_forcesolution\n",
    "from pyaspect.specfemio.utils import get_xyz_coords_from_station_list\n",
    "from pyaspect.specfemio.utils import get_xyz_coords_from_station_list_except\n",
    "from pyaspect.specfemio.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "Extract the ndarray of the subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_dir  = 'data/output/'\n",
    "data_out_dir = data_in_dir\n",
    "!ls {data_in_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 \n",
    "\n",
    "Decompress the ndarray of the sliced, subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename then used it to decompress model\n",
    "ifqn = f'{data_out_dir}/vsliced_subsmp_smth_nam_2017_vp_vs_rho_Q_model_dx100_dy100_dz100_maxdepth5850_sig250.npz'\n",
    "vslice_gm3d, other_pars = decompress_gm3d_from_file(ifqn)\n",
    "\n",
    "print()\n",
    "print('decompressed gridded model\\n:',vslice_gm3d) \n",
    "print()\n",
    "print('other parameters:\\n',other_pars)\n",
    "print()\n",
    "\n",
    "# WARNING: this will unpack all other_pars, if you overwrite a variable of the samename as val(key), then you \n",
    "#          may not notice, and this may cause large headaches.  I use it because I am aware of it.\n",
    "'''\n",
    "for key in other_pars:\n",
    "    locals()[key] = other_pars[key]  #this is more advanced python than I think is reasonable for most \n",
    "sig_meters = sig\n",
    "''';\n",
    "\n",
    "# another way to get these varibles is just use the accessor functions for the gridmod3d.  We need them later.\n",
    "xmin = other_pars['xmin']\n",
    "dx   = other_pars['dx']\n",
    "nx   = other_pars['nx']\n",
    "ymin = other_pars['ymin']\n",
    "dy   = other_pars['dy']\n",
    "ny   = other_pars['ny']\n",
    "zmin = other_pars['zmin']\n",
    "dz   = other_pars['dz']\n",
    "nz   = other_pars['nz']\n",
    "sig_meters = other_pars['sig']  # this variable is used later\n",
    "print('sig_meters:',sig_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "nam_dims = list(vslice_gm3d.get_npoints())\n",
    "nam_origin = list(vslice_gm3d.get_gorigin())\n",
    "nam_origin[2] *= -1\n",
    "nam_origin = tuple(nam_origin)\n",
    "nam_spacing = list(vslice_gm3d.get_deltas())\n",
    "nam_spacing[2] *=-1\n",
    "nam_spacing = tuple(nam_spacing)\n",
    "print('nam_dims:',nam_dims)\n",
    "print('nam_origin:',nam_origin)\n",
    "print('nam_spacing:',nam_spacing)\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.dimensions = np.array(nam_dims) + 1\n",
    "grid.origin = nam_origin  # The bottom left corner of the data set\n",
    "grid.spacing = nam_spacing  # These are the cell sizes along each axis\n",
    "nam_pvalues = vslice_gm3d.getNPArray()[0]\n",
    "print('pvalues.shape:',nam_pvalues.shape)\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_arrays[\"values\"] = nam_pvalues.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "# Now plot the grid!\n",
    "cmap = plt.cm.jet\n",
    "#grid.plot(show_edges=True,cmap=cmap)\n",
    "grid.plot(cmap=cmap,opacity=1.0)\n",
    "print('dir:\\n',dir(grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = grid.slice_orthogonal()\n",
    "\n",
    "#slices.plot(show_edges=True,cmap=cmap)\n",
    "slices.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vslice_gm32:\\n',vslice_gm3d)\n",
    "coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "print(coords.shape)\n",
    "print('norm:\\n',coords)\n",
    "\n",
    "xc = vslice_gm3d.getLocalCoordsPointsX()\n",
    "yc = vslice_gm3d.getLocalCoordsPointsY()\n",
    "zc = vslice_gm3d.getLocalCoordsPointsZ()\n",
    "\n",
    "thing = np.meshgrid(xc,yc,zc)\n",
    "print('thing0.shape:',thing[0].shape)\n",
    "print('thing1.shape:',thing[1].shape)\n",
    "print('thing2.shape:',thing[2].shape)\n",
    "print('thing0:\\n',thing[0])\n",
    "print('thing1:\\n',thing[1])\n",
    "print('thing2:\\n',thing[2])\n",
    "\n",
    "'''\n",
    "print(coords.shape)\n",
    "coords = coords.transpose()\n",
    "#coords[0,:] += np.arange(4)*100000\n",
    "#coords[1,:] -= np.arange(4)*100000\n",
    "print('transp:\\n',coords)\n",
    "sort_coords = coords [ :, coords[2].argsort()]\n",
    "coords = sort_coords\n",
    "print('sort:\\n',coords)\n",
    "coords = coords.transpose()\n",
    "print('norm:\\n',coords)\n",
    "coords = coords[::4,:]\n",
    "''';\n",
    "def skip_by_index(arr,i,skip):\n",
    "    #print('arr norm-shape:\\n',arr.shape)\n",
    "    arr = arr.transpose()\n",
    "    print('arr trans:\\n',arr)\n",
    "    sort_arr = arr[ :, arr[i].argsort()]\n",
    "    arr = sort_arr\n",
    "    arr = arr.transpose()\n",
    "    arr = arr[::skip,:]\n",
    "    return arr\n",
    "\n",
    "def get_unique_xyz_skip(arr,skip):\n",
    "    ux = np.unique(arr.T[0,:])\n",
    "    print('nx:',len(np.unique(arr.T[0,:])))\n",
    "    uy = np.unique(arr.T[1,:])\n",
    "    print('ny:',len(np.unique(arr.T[1,:])))\n",
    "    uz = np.unique(arr.T[2,:])\n",
    "    print('nz:',len(np.unique(arr.T[2,:])))\n",
    "    \n",
    "    ux = ux[::skip]\n",
    "    uy = uy[::skip]\n",
    "    uz = uz[::skip]\n",
    "    \n",
    "    return np.vstack(np.meshgrid(ux,uy,uz)).reshape(3,-1).T\n",
    "   \n",
    "coords = get_unique_xyz_skip(coords,8)\n",
    "\n",
    "\n",
    "\n",
    "#coords = skip_by_index(coords,2,8)\n",
    "#coords = skip_by_index(coords,1,8)\n",
    "#coords = skip_by_index(coords,0,8)\n",
    "\n",
    "print('coords after sort:\\n',coords)\n",
    "print('coords after shape:\\n',coords.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "coords = coords[::4,:]\n",
    "print()\n",
    "print(coords.shape)\n",
    "print(coords)\n",
    "coords[:,2] = -coords[:,2]\n",
    "print(coords)\n",
    "coords = coords.transpose()\n",
    "print('transp:\\n',coords)\n",
    "sort_coords = coords [ :, coords[2].argsort()]\n",
    "coords = sort_coords\n",
    "coords = coords[:,::4].transpose()\n",
    "sort_coords = coords [ :, coords[1].argsort()]\n",
    "coords = sort_coords\n",
    "coords = coords[:,::4].transpose()\n",
    "print('sort:\\n',coords)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_points = pv.wrap(coords)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.85)\n",
    "p.add_mesh(pv_points, render_points_as_spheres=True, point_size=10,opacity=0.85)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "#print('nx:\\n',len(xc))\n",
    "#print('x:\\n',xc)\n",
    "\n",
    "\n",
    "n_rx = 5\n",
    "n_ry = 5\n",
    "n_rz = 5\n",
    "n_rand_p = 1000\n",
    "\n",
    "lrx = np.min(xc)\n",
    "lry = np.min(yc)\n",
    "lrz = -4300.0\n",
    "\n",
    "hrx = np.max(xc)\n",
    "hry = np.max(yc)\n",
    "hrz = -3000.0\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "srz = hrz - lrz\n",
    "\n",
    "r_xyz_list = []\n",
    "#for i in range(n_rx*n_ry*n_rz):\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    rz = lrz + srz*np.random.rand()\n",
    "    r_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "r_xyz = np.array(r_xyz_list)\n",
    "    \n",
    "\n",
    "#rx = lrx + srx*np.random.rand(n_rx)\n",
    "#ry = lry + sry*np.random.rand(n_ry)\n",
    "#rz = lrz + srz*np.random.rand(n_rz)\n",
    "\n",
    "#print('rx:\\n',rx)\n",
    "#print('ry:\\n',ry)\n",
    "#print('rz:\\n',rz)\n",
    "\n",
    "#r_xyz = np.vstack(np.meshgrid(rx,ry,rz)).reshape(3,-1).T\n",
    "print('r_xyz:\\n',r_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(r_xyz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir {data_out_dir}/tmp\n",
    "!ls {data_out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'l_grp_stations' in locals() or 'l_grp_stations' in globals():\n",
    "    print('deleting')\n",
    "    del l_grp_stations\n",
    "    \n",
    "l_stations = []\n",
    "\n",
    "for i in range(len(r_xyz)):\n",
    "    \n",
    "    name = 't' + str(i).zfill(len(str(len(r_xyz))))\n",
    "    new_s = StationHeader(name=name,\n",
    "                          network='NL',\n",
    "                          lat_yc=r_xyz[i,1],\n",
    "                          lon_xc=r_xyz[i,0],\n",
    "                          elevation=0.0,\n",
    "                          burial=r_xyz[i,2],\n",
    "                          trid=i)\n",
    "    l_stations.append(new_s)\n",
    "print('len(l_stats):',len(l_stations))\n",
    "                                           \n",
    "l_grp_stations = make_grouped_cross_station_headers(stations=l_stations,delta=250.0)\n",
    "#l_grp_stations += [l_grp_stations[0]]\n",
    "t_grp_stations = sorted(copy.deepcopy(flatten_grouped_headers(l_grp_stations)))\n",
    "print('len(t_grp):',len(t_grp_stations))\n",
    "print('len(t_grp[0]):',len([t_grp_stations[0]]))\n",
    "t_grp_stations += [t_grp_stations[0]]\n",
    "print('len(t_grp):',len(t_grp_stations))\n",
    "s_grp_stations = sorted(copy.deepcopy(flatten_grouped_headers_unique(l_grp_stations)))\n",
    "print('len(s_grp):',len(s_grp_stations))\n",
    "print('type(stat):',type(s_grp_stations))\n",
    "ts = copy.deepcopy(s_grp_stations[-1])\n",
    "ts.trid = -2\n",
    "ts.name = 'dummy'\n",
    "ts['goobid'] = 13\n",
    "print('ts:\\n',ts)\n",
    "s_grp_stations += [ts]\n",
    "for i in range(10):\n",
    "    s_grp_stations += [ts]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#print(l_grp_stations)\n",
    "print('len:',len(s_grp_stations))\n",
    "l_same = []\n",
    "for i in range(len(s_grp_stations)-1):\n",
    "    si = s_grp_stations[i]\n",
    "    ih = si.__hash__()\n",
    "    for j in range(i+1,len(s_grp_stations)):\n",
    "        sj = s_grp_stations[j]\n",
    "        jh = sj.__hash__()\n",
    "        if ih == jh:\n",
    "            l_same.append(si)\n",
    "            l_same.append(sj)\n",
    "            print(f'{i},{j} have same hash: {ih}')\n",
    "            print(f'si.x,si,y,si.z = {si.lon_xc},{si.lat_yc},{si.burial}')\n",
    "            print(f'sj.x,sj,y,sj.z = {sj.lon_xc},{sj.lat_yc},{sj.burial}')\n",
    "'''\n",
    "''';\n",
    "\n",
    "print()\n",
    "print('SAME')\n",
    "for ls in l_same:\n",
    "    print(ls)\n",
    "print()\n",
    "print('set(SAME)')\n",
    "for ls in set(l_same):\n",
    "    print(ls)\n",
    "print()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grp_stations = list(sorted(set(s_grp_stations)))\n",
    "print('len:',len(s_grp_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_grp_stations[0])\n",
    "print(s_grp_stations[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grp_stations = sorted(s_grp_stations)\n",
    "print('dummy.hash:31880328642150367')\n",
    "#for s in sorted(l_grp_stations):\n",
    "    #print(s.name)\n",
    "#write_stations(data_out_dir + '/tmp',l_stations)\n",
    "#write_stations(data_out_dir + '/tmp',l_grp_stations)\n",
    "#write_stations(data_out_dir + '/tmp',s_grp_stations)\n",
    "write_stations(data_out_dir + '/tmp',s_grp_stations)\n",
    "write_stations(data_out_dir + '/tmp',l_grp_stations,fname='STATIONS_GROUPED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh {data_out_dir + '/tmp'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_out_dir}/tmp/STATIONS_GROUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!tail {data_out_dir}/tmp/STATIONS_GROUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!diff {data_out_dir}/tmp/STATIONS {data_out_dir}/tmp/STATIONS_GROUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqp = data_out_dir + '/tmp'\n",
    "rw_stations = sorted(read_stations(fqp))\n",
    "\n",
    "for s in sorted(rw_stations):\n",
    "    print(f'{s.name}_{s.trid}_{s.gid}_{s.sid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.path.relpath(data_out_dir + '///', start=os.curdir)\n",
    "newp = os.path.join(path, 'tmp', 'STATIONS')\n",
    "\n",
    "print(newp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /Users/seismac/Documents/Work/Bench/ForkGnam/pyaspect/notebooks/Full_Workflow/FWI_Workflow_for_Groningen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "                 \n",
    "print()\n",
    "print('--------------------Test CMTSolution--------------------------------------------------')\n",
    "print()\n",
    "\n",
    "test_cmt_h = CMTSolutionHeader(date=datetime.datetime.now(),\n",
    "                               ename='test_event',\n",
    "                               tshift=0.0,\n",
    "                               hdur=0.0,\n",
    "                               lat_yc=2.71,\n",
    "                               lon_xc=3.14,\n",
    "                               depth=-5432,\n",
    "                               mt=MomentTensor(mw=3.5,strike=120,dip=40,rake=15),\n",
    "                               eid=7,\n",
    "                               sid=77)\n",
    "\n",
    "print(test_cmt_h)\n",
    "print()\n",
    "test_cmt_h.mrr = -1 \n",
    "print(test_cmt_h)\n",
    "print()\n",
    "#tmt = test_cmt_h.mt\n",
    "#print('mt:',tmt)\n",
    "print(test_cmt_h)\n",
    "print()\n",
    "\n",
    "other_cmt_h = copy.deepcopy(test_cmt_h)\n",
    "other_cmt_h['goobid'] = -12\n",
    "#other_cmt_h.depth = 12\n",
    "\n",
    "test_set = set([other_cmt_h,test_cmt_h,test_cmt_h])\n",
    "print('Set test:', test_set)\n",
    "print()\n",
    "print('let(set):', len(test_set))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_force_h = ForceSolutionHeader(date=datetime.datetime.now(),\n",
    "                                   tshift=0.0,\n",
    "                                   f0=0.0,\n",
    "                                   lat_yc=2.71,\n",
    "                                   lon_xc=3.17,\n",
    "                                   depth=13,\n",
    "                                   factor_fs=1,\n",
    "                                   comp_src_EX=1,\n",
    "                                   comp_src_NY=0,\n",
    "                                   comp_src_Zup=0,\n",
    "                                   eid=7,\n",
    "                                   sid=77)\n",
    "\n",
    "print()\n",
    "print('--------------------Test FroceSolution------------------------------------------------')\n",
    "print()\n",
    "print(test_force_h)\n",
    "print()\n",
    "other_force_h = copy.deepcopy(test_force_h)\n",
    "other_force_h['goobid'] = -12\n",
    "other_force_h.comp_src_EX = 0\n",
    "other_force_h.comp_src_Zup = 1\n",
    "print(other_force_h)\n",
    "\n",
    "test_set = set([other_force_h,test_force_h,test_force_h])\n",
    "print('Set test:', test_set)\n",
    "print()\n",
    "print('len(set):', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cmtsolution(data_out_dir + '/tmp',test_cmt_h)\n",
    "write_forcesolution(data_out_dir + '/tmp',test_force_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh {data_out_dir + '/tmp'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat {data_out_dir}/tmp/CMTSOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat {data_out_dir}/tmp/FORCESOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqp = f'{data_out_dir}/tmp'\n",
    "read_gen_cmt = read_solution(fqp,'CMTSOLUTION')\n",
    "read_gen_fs  = read_solution(fqp,'FORCESOLUTION')\n",
    "read_cmt = read_cmtsolution(fqp,fname='CMTSOLUTION')\n",
    "read_fs  = read_forcesolution(fqp,fname='FORCESOLUTION')\n",
    "    \n",
    "print('gen cmt:\\n',read_gen_cmt)\n",
    "print()\n",
    "print('gen fs:\\n',read_gen_fs)\n",
    "print()\n",
    "print('cmt:\\n',read_cmt)\n",
    "print()\n",
    "print('fs:\\n',read_fs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(rw_stations)\n",
    "exc_g_xyz = get_xyz_coords_from_station_list_except(rw_stations,key='gid',val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_exc_points = pv.wrap(exc_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_exc_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class StationTrace(object):\n",
    "    \n",
    "    def __init__(self, header=None, data=None, start_t=0, dt=1):\n",
    "        \n",
    "        if not isinstance(header,StationHeader):\n",
    "            raise Exception('arg: \\'header\\' must be of type StationHeader')\n",
    "            \n",
    "        self.header  = copy.deepcopy(header)\n",
    "        self.data    = np.array(data)\n",
    "        self.start_t = start_t\n",
    "        self.dt      = dt\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, islice):\n",
    "        print(dir(islice))\n",
    "        return self.data[islice]\n",
    "    \n",
    "    def __setitem__(self, islice, value):\n",
    "        self.data[islice] = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        out_str  = f'Station Header:\\n{self.header}\\n'\n",
    "        out_str += f'Data:\\n{self.data}'\n",
    "        return out_str\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out_str  = f'Station Header:\\n{self.header.__repr__()}\\n'\n",
    "        out_str += f'Data:\\n{self.data.__repr__()}'\n",
    "        return out_str\n",
    "    \n",
    "    def time_slice(self,*args):\n",
    "        tslice = slice(*args)\n",
    "        \n",
    "        if tslice.start == None and tslice.step == None:\n",
    "            print('if')\n",
    "            return self.start_t + tslice.stop*self.dt\n",
    "        \n",
    "        elif  tslice.step == None:\n",
    "            print('elif')\n",
    "            if tslice.stop <= tslice.start:\n",
    "                raise Exception('stop index must be greater than start index')\n",
    "                \n",
    "            return self.start_t + np.arange(tslice.start,tslice.stop)*self.dt\n",
    "            \n",
    "        else:\n",
    "            print('else')\n",
    "            if tslice.stop <= tslice.start:\n",
    "                raise Exception('stop index must be greater than start index')\n",
    "                \n",
    "            return self.start_t + np.arange(tslice.start,tslice.stop,tslice.step)*self.dt\n",
    "        \n",
    "        \n",
    "    def data_and_time(self,*args):\n",
    "        islice = slice(*args)\n",
    "        return self[islice], self.time_slice(islice)\n",
    "    '''\n",
    "    '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "class Record(object):\n",
    "    \n",
    "    def __init__(self, solution_h=None, stations_h=None, rid=0, iter_id=0):\n",
    "        \n",
    "        if not isinstance(solution_h,SolutionHeader):\n",
    "            raise Exception('arg: \\'solution_h\\' must be of type SolutionHeader')\n",
    "            \n",
    "        check_all = all(isinstance(s,StationHeader) for s in stations_h)\n",
    "        if not check_all:\n",
    "            raise Exception('elements in arg: \\'stations_h\\' must be of type StationHeader')\n",
    "            \n",
    "        check_all = all(s.sid == solution_h.sid for s in stations_h)\n",
    "        if not check_all:\n",
    "            raise Exception('sid of each element in arg: \\'stations_h\\' must match solution_h.sid')\n",
    "            \n",
    "        self.solution_header = copy.deepcopy(solution_h)\n",
    "        self.station_headers = copy.deepcopy(stations_h)\n",
    "        \n",
    "        self.rid     = rid\n",
    "        self.iter_id = iter_id\n",
    "        \n",
    "        self.df = pd.DataFrame.from_records(self.station_headers, index=['sid','trid','gid'])\n",
    "        \n",
    "        self.traces = []\n",
    "        \n",
    "        self.added_header_words = []\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        out_str  = f'Solution Header:\\n{self.solution_header}\\n'\n",
    "        out_str += f'Station Headers:\\n {self.df}'\n",
    "        return out_str\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out_str  = f'Solution Header:\\n{self.solution_header.__repr__()}\\n'\n",
    "        out_str += f'Station Headers:\\n {self.df.__repr__()}'\n",
    "        return out_str\n",
    "    \n",
    "    \n",
    "    def _update_df(self):\n",
    "        df_index = copy.deepcopy(self.df.index)\n",
    "        del self.df\n",
    "        self.df = pd.DataFrame.from_records(self.station_headers, index=df_index)\n",
    "    \n",
    "    def add_header_word_to_stations(self, func=None):\n",
    "        \n",
    "        if not callable(func):\n",
    "            raise Exception('arg: \\'func\\' must be a function')\n",
    "            \n",
    "        key = func(self)\n",
    "        self.added_header_words.append(key)\n",
    "            \n",
    "        self._update_df()\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "#df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\n",
    "#df = pd.DataFrame.from_records(l_grp_stations)\n",
    "#df = pd.DataFrame.from_records(flatten_grouped_headers(l_grp_stations),index=['sid','trid','gid'])\n",
    "#headers = [{'A': 'apple', 'B': 0, 'C': 3, 'D': 3}, {'A': 'banana', 'B': 9, 'C': 3, 'D': 5}, {'A': 'orange', 'B': 4, 'C': 7, 'D': 6}]\n",
    "#df = pd.DataFrame(headers)\n",
    "#print('df:\\n',df[0:18])\n",
    "\n",
    "df_force_h = test_force_h\n",
    "df_force_h.sid = 0\n",
    "print('df_force_h.sid:',df_force_h.sid)\n",
    "print()\n",
    "t_record = Record(solution_h=df_force_h,stations_h=flatten_grouped_headers(l_grp_stations))\n",
    "\n",
    "print(t_record)\n",
    "print()\n",
    "\n",
    "def offset_func(self):\n",
    "    for s in self.station_headers:\n",
    "        x_sqrd = (self.solution_header.lon_xc - s.lon_xc)**2\n",
    "        y_sqrd = (self.solution_header.lat_yc - s.lat_yc)**2\n",
    "        offset = np.sqrt(x_sqrd + y_sqrd)\n",
    "        s['offset'] = offset\n",
    "        return 'offset'\n",
    "        \n",
    "    \n",
    "t_record.add_header_word_to_stations(offset_func)\n",
    "\n",
    "print(t_record)\n",
    "print()\n",
    "print(t_record.added_header_words)\n",
    "print()\n",
    "\n",
    "t_st = StationTrace(s_grp_stations[0],np.arange(2000), start_t=0, dt=0.25)\n",
    "\n",
    "print(t_st)\n",
    "print()\n",
    "\n",
    "t_st[:3] = -1*np.ones((3))\n",
    "print(t_st)\n",
    "print()\n",
    "#t_st.time_slice(0,10,2)\n",
    "myslice = t_st.time_slice(2,10,2)\n",
    "print(f'myslice:\\n{myslice}')\n",
    "\n",
    "t_st[4:20:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silly_func(x):\n",
    "    print('Hahwoe!')\n",
    "\n",
    "silly_func = 'Hahwoe!'\n",
    "    \n",
    "print(callable(silly_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mys = slice(0,10,2)\n",
    "print('dir:',dir(mys))\n",
    "print()\n",
    "print('dir(index):',dir(mys.indices))\n",
    "print()\n",
    "print('start:',mys.start)\n",
    "print('stop :',mys.stop)\n",
    "print('step :',mys.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
