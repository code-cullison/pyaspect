{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA_random_eqrthquake_locations_w_pyvista_vtk\n",
    "\n",
    "Testing the random picking of subsurface eqrthquake locations (moment-tensor locations) and 3D plotting of the picks and model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all packages\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sys import argv\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from pyaspect.model.gridmod3d import gridmod3d as gm\n",
    "from pyaspect.model.bbox import bbox as bb\n",
    "from pyaspect.model.gm3d_utils import compress_gm3d_to_file\n",
    "from pyaspect.model.gm3d_utils import decompress_gm3d_from_file\n",
    "from pyaspect.moment_tensor import MomentTensor\n",
    "from pyaspect.specfemio.headers import StationHeader\n",
    "from pyaspect.specfemio.headers import SolutionHeader\n",
    "from pyaspect.specfemio.headers import CMTSolutionHeader\n",
    "from pyaspect.specfemio.headers import ForceSolutionHeader\n",
    "from pyaspect.specfemio.headers import RecordHeader\n",
    "from pyaspect.specfemio.write import write_cmtsolution\n",
    "from pyaspect.specfemio.write import write_forcesolution\n",
    "from pyaspect.specfemio.write import write_grouped_forcesolutions\n",
    "from pyaspect.specfemio.write import write_stations\n",
    "from pyaspect.specfemio.write import write_record\n",
    "from pyaspect.specfemio.write import write_records\n",
    "from pyaspect.specfemio.read import read_stations\n",
    "from pyaspect.specfemio.read import read_solution\n",
    "from pyaspect.specfemio.read import read_cmtsolution\n",
    "from pyaspect.specfemio.read import read_forcesolution\n",
    "from pyaspect.specfemio.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "Extract the ndarray of the subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_dir  = 'data/output/'\n",
    "data_out_dir = data_in_dir\n",
    "!ls {data_in_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 \n",
    "\n",
    "Decompress the ndarray of the sliced, subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename then used it to decompress model\n",
    "ifqn = f'{data_out_dir}/vsliced_subsmp_smth_nam_2017_vp_vs_rho_Q_model_dx100_dy100_dz100_maxdepth5850_sig250.npz'\n",
    "vslice_gm3d, other_pars = decompress_gm3d_from_file(ifqn)\n",
    "\n",
    "print()\n",
    "print('decompressed gridded model\\n:',vslice_gm3d) \n",
    "print()\n",
    "print('other parameters:\\n',other_pars)\n",
    "print()\n",
    "\n",
    "# WARNING: this will unpack all other_pars, if you overwrite a variable of the samename as val(key), then you \n",
    "#          may not notice, and this may cause large headaches.  I use it because I am aware of it.\n",
    "'''\n",
    "for key in other_pars:\n",
    "    locals()[key] = other_pars[key]  #this is more advanced python than I think is reasonable for most \n",
    "sig_meters = sig\n",
    "''';\n",
    "\n",
    "# another way to get these varibles is just use the accessor functions for the gridmod3d.  We need them later.\n",
    "xmin = other_pars['xmin']\n",
    "dx   = other_pars['dx']\n",
    "nx   = other_pars['nx']\n",
    "ymin = other_pars['ymin']\n",
    "dy   = other_pars['dy']\n",
    "ny   = other_pars['ny']\n",
    "zmin = other_pars['zmin']\n",
    "dz   = other_pars['dz']\n",
    "nz   = other_pars['nz']\n",
    "sig_meters = other_pars['sig']  # this variable is used later\n",
    "print('sig_meters:',sig_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "nam_dims = list(vslice_gm3d.get_npoints())\n",
    "nam_origin = [0,0,-vslice_gm3d.get_gorigin()[2]]\n",
    "#nam_origin = list(vslice_gm3d.get_gorigin())\n",
    "#nam_origin[2] *= -1\n",
    "nam_origin = tuple(nam_origin)\n",
    "nam_spacing = list(vslice_gm3d.get_deltas())\n",
    "nam_spacing[2] *=-1\n",
    "nam_spacing = tuple(nam_spacing)\n",
    "print('nam_dims:',nam_dims)\n",
    "print('nam_origin:',nam_origin)\n",
    "print('nam_spacing:',nam_spacing)\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.dimensions = np.array(nam_dims) + 1\n",
    "grid.origin = nam_origin  # The bottom left corner of the data set\n",
    "grid.spacing = nam_spacing  # These are the cell sizes along each axis\n",
    "nam_pvalues = vslice_gm3d.getNPArray()[0]\n",
    "print('pvalues.shape:',nam_pvalues.shape)\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_arrays[\"values\"] = nam_pvalues.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "# Now plot the grid!\n",
    "cmap = plt.cm.jet\n",
    "#grid.plot(show_edges=True,cmap=cmap)\n",
    "grid.plot(cmap=cmap,opacity=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = grid.slice_orthogonal()\n",
    "\n",
    "#slices.plot(show_edges=True,cmap=cmap)\n",
    "slices.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create random virtual source (to specfem stations, but using reciprocity -- sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 1000\n",
    "\n",
    "#stay away from the edges of the model for derivatives \n",
    "# and to avoid boundary effects\n",
    "xy_pad = 1000 \n",
    "\n",
    "lrx = np.min(xc) + xy_pad\n",
    "lry = np.min(yc) + xy_pad\n",
    "lrz = -4300.0\n",
    "\n",
    "hrx = np.max(xc) - xy_pad\n",
    "hry = np.max(yc) - xy_pad\n",
    "hrz = -3000.0\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "srz = hrz - lrz\n",
    "\n",
    "r_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    rz = lrz + srz*np.random.rand()\n",
    "    r_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "r_xyz = np.array(r_xyz_list)\n",
    "    \n",
    "\n",
    "#r_xyz = np.vstack(np.meshgrid(rx,ry,rz)).reshape(3,-1).T\n",
    "print('r_xyz:\\n',r_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir {data_out_dir}/tmp\n",
    "!ls {data_out_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make cross (half or full) group for calculating spacial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'l_grp_stations' in locals() or 'l_grp_stations' in globals():\n",
    "    print('deleting')\n",
    "    del l_grp_stations\n",
    "    \n",
    "    \n",
    "# this is the path to the project dir on the cluster\n",
    "my_proj_dir = '/scratch/seismology/tcullison/test_mesh/Batch_Src_Test'\n",
    "\n",
    "    \n",
    "# make a list of station headers for each random location\n",
    "l_stations = []\n",
    "for i in range(len(r_xyz)):\n",
    "    \n",
    "    #name = 't' + str(i).zfill(len(str(len(r_xyz))))\n",
    "    tr_bname = 'tr'\n",
    "    new_s = StationHeader(name=tr_bname,\n",
    "                          network='NL',\n",
    "                          lat_yc=r_xyz[i,1],\n",
    "                          lon_xc=r_xyz[i,0],\n",
    "                          elevation=0.0,\n",
    "                          depth=-r_xyz[i,2],\n",
    "                          trid=i)\n",
    "    l_stations.append(new_s)\n",
    "#print('len(l_stats):',len(l_stations))\n",
    "                                           \n",
    "# make the group membors for each station above\n",
    "# function below returns a list[list[]] like structure\n",
    "m_delta = 250.0 # distance between cross stations for derivatives\n",
    "assert m_delta < xy_pad #see cells above this is padding\n",
    "l_grp_stations = make_grouped_half_cross_station_headers(stations=l_stations,delta=m_delta)\n",
    "name_list = []\n",
    "for s in flatten_grouped_headers(l_grp_stations):\n",
    "    name_list.append(f's{str(s.sid).zfill(2)}g{str(s.gid).zfill(2)}t{str(s.trid).zfill(6)}')\n",
    "    \n",
    "print('len(name_list):',len(name_list))\n",
    "print('len(name_set): ',len(set(name_list)))\n",
    "    \n",
    "\n",
    "# this is a list[] structure with unique stations (small chance a group member has same coordinate)\n",
    "s_grp_stations = sorted(copy.deepcopy(flatten_grouped_headers_unique(l_grp_stations)))\n",
    "\n",
    "# if lengths are the same then all group members are unique\n",
    "print(f'len(l_grp): {len(flatten_grouped_headers(l_grp_stations))}')\n",
    "print(f'len(s_grp): {len(s_grp_stations)}')\n",
    "\n",
    "#flatten the l_grp_stations (NOT guarantied unique!)\n",
    "l_grp_stations = sorted(flatten_grouped_headers(l_grp_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stations(data_out_dir + '/tmp',l_grp_stations,auto_name=True,auto_network=True)\n",
    "\n",
    "# For records only!\n",
    "#write_stations(data_out_dir + '/tmp',l_grp_stations,write_h=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh {data_out_dir + '/tmp'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqp = data_out_dir + '/tmp'\n",
    "rw_stations = sorted(read_stations(fqp))\n",
    "\n",
    "print('stations equal?:', rw_stations == l_grp_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(rw_stations)\n",
    "all_g_xyz[:,2] *= -1 #pyview z-up positive and oposize sign of standard geophysics \n",
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make random virtual recievers locations (solutions/sources in specfem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 5\n",
    "rz = -500\n",
    "\n",
    "lrx = np.min(xc)\n",
    "lry = np.min(yc)\n",
    "\n",
    "hrx = np.max(xc)\n",
    "hry = np.max(yc)\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "\n",
    "s_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    s_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "s_xyz = np.array(s_xyz_list)\n",
    "    \n",
    "\n",
    "print('s_xyz:\\n',s_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_spoints = pv.wrap(s_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=8,opacity=1,color='red')\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make force solution headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of station headers for each random location\n",
    "l_solutions = []\n",
    "for i in range(len(s_xyz)):\n",
    "    \n",
    "    #NOTE!!!! the depth is set to the NEGATIVE (makes it positive) due to sign convention\n",
    "    new_s = ForceSolutionHeader(ename=f'Event-{str(i).zfill(4)}',\n",
    "                                lat_yc=s_xyz[i,1],\n",
    "                                lon_xc=s_xyz[i,0],\n",
    "                                depth=-s_xyz[i,2],\n",
    "                                tshift=0.0,\n",
    "                                date=datetime.datetime.now(),\n",
    "                                f0=0.0,\n",
    "                                factor_fs=1,\n",
    "                                comp_src_EX=1,\n",
    "                                comp_src_NY=0,\n",
    "                                comp_src_Zup=0,\n",
    "                                proj_id=0,\n",
    "                                eid=i,\n",
    "                                sid=0)\n",
    "    l_solutions.append(new_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make solution group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_grp_solutions_h = make_grouped_triplet_force_solution_headers(solutions=l_solutions)\n",
    "grp_s_xyz = get_xyz_coords_from_solution_list(flatten_grouped_headers(l_grp_solutions_h))\n",
    "\n",
    "for s in flatten_grouped_headers(l_grp_solutions_h):\n",
    "    print(f'solution:\\n{s}')\n",
    "    print()\n",
    "print(f'coords:\\n{grp_s_xyz}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    p = os.path.join(data_out_dir, 'tmp')\n",
    "    p = os.path.join(p, f'run{str(grp_s[0].eid+1).zfill(4)}')\n",
    "    p = os.path.join(p, 'DATA')\n",
    "    #p = f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}'\n",
    "    #print(p)\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_out_dir}/tmp\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s)\n",
    "    #write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s,write_h=False)\n",
    "    #print(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}')\n",
    "    #for s in grp_s:\n",
    "        #print(f's:\\n{s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -l {data_out_dir}tmp/run*/DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make record from solutions and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make stations list per solution_group \n",
    "poor_l_records = []\n",
    "\n",
    "for i in range(len(l_grp_solutions_h)):\n",
    "    grp_s = l_grp_solutions_h[i]\n",
    "    l_force_stations = []\n",
    "    for sol in grp_s:\n",
    "        l_stat = copy.deepcopy(l_grp_stations)\n",
    "        for s in l_stat:\n",
    "            s.eid = sol.eid\n",
    "            s.sid = sol.sid\n",
    "        l_force_stations += l_stat\n",
    "\n",
    "    record = RecordHeader(solutions_h=grp_s,stations_h=l_force_stations,rid=i)\n",
    "    poor_l_records.append(record)\n",
    "\n",
    "src_grouped_stations = []\n",
    "for sgrp in l_grp_solutions_h:\n",
    "    l_rgrp = []\n",
    "    for s in sgrp:\n",
    "        l_rgrp.append(copy.deepcopy(l_grp_stations))\n",
    "    src_grouped_stations.append(l_rgrp)\n",
    "\n",
    "print(f'len(l_grp_solutions_h):{len(l_grp_solutions_h)}')\n",
    "print(f'len(l_grp_solutions_h[0]):{len(l_grp_solutions_h[0])}')\n",
    "print(f'len(l_grp_solutions_h[0][0]):{len(l_grp_solutions_h[0][0])}')\n",
    "print(f'len(src_grouped_stations):{len(src_grouped_stations)}')\n",
    "print(f'len(src_grouped_stations[0]):{len(src_grouped_stations[0])}')\n",
    "l_records = []\n",
    "l_records = make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "    \n",
    "print(f'len(l_records): {len(l_records)}')\n",
    "print()\n",
    "\n",
    "for i in range(len(l_records)):\n",
    "    print(f'rec[{i}] == prec[{i}]: {l_records[i] == poor_l_records[i]}')\n",
    "\n",
    "print('Print Out of ALL Records\\n')\n",
    "for i in range(len(l_records)):\n",
    "    r = l_records[i]\n",
    "    p = poor_l_records[i]\n",
    "    print(f'Record-{r.rid}:\\n{r}')\n",
    "    print()\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickle record list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{data_out_dir}/tmp/project_record_list','wb')\n",
    "pickle.dump(l_records,f)\n",
    "f.close()\n",
    "\n",
    "!ls -ltrh {data_out_dir}/tmp/project_record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{data_out_dir}/tmp/project_record_list','rb')\n",
    "dill_l_records = pickle.load(f)\n",
    "f.close\n",
    "\n",
    "print('Check all records:')\n",
    "print()\n",
    "for i in range(len(dill_l_records)):\n",
    "    print(f'Dill_Rec == Orig_Rec?: {dill_l_records == l_records}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write records and headers in records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proj_fqp = f'{data_out_dir}tmp/' \n",
    "#write_record(proj_fqp,dill_l_records[0],fname='test_project.record')\n",
    "write_records(proj_fqp,dill_l_records,fname='test_project_record',auto_name=True,auto_network=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -ltrh data/output/tmp/\n",
    "print()\n",
    "!ls -ltrh {data_out_dir}tmp/run*/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in dill_l_records:\n",
    "    print(f'Record:\\n{r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = goobleydoobleydoo('42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sillyfunc(index=None):\n",
    "    if index == None:\n",
    "        print('none')\n",
    "    else:\n",
    "        print(f'index={index}')\n",
    "        \n",
    "sillyfunc(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = []\n",
    "for i in range(6):\n",
    "    df_rec.append({'sid':i%3,'eid':0})\n",
    "df = pd.DataFrame.from_records(df_rec)\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(df['sid'])\n",
    "print()\n",
    "s = set(df['sid'])\n",
    "ss = set(df['sid'])\n",
    "print(s)\n",
    "print(f's == ss: {s == ss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
