{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA_random_eqrthquake_locations_w_pyvista_vtk\n",
    "\n",
    "Testing the random picking of subsurface eqrthquake locations (moment-tensor locations) and 3D plotting of the picks and model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all packages\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sys import argv\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from pyaspect.model.gridmod3d import gridmod3d as gm\n",
    "from pyaspect.model.bbox import bbox as bb\n",
    "from pyaspect.model.gm3d_utils import compress_gm3d_to_file\n",
    "from pyaspect.model.gm3d_utils import decompress_gm3d_from_file\n",
    "from pyaspect.moment_tensor import MomentTensor\n",
    "from pyaspect.specfemio.headers import StationHeader\n",
    "from pyaspect.specfemio.headers import SolutionHeader\n",
    "from pyaspect.specfemio.headers import CMTSolutionHeader\n",
    "from pyaspect.specfemio.headers import ForceSolutionHeader\n",
    "from pyaspect.specfemio.headers import RecordHeader\n",
    "from pyaspect.specfemio.write import write_cmtsolution\n",
    "from pyaspect.specfemio.write import write_forcesolution\n",
    "from pyaspect.specfemio.write import write_grouped_forcesolutions\n",
    "from pyaspect.specfemio.write import write_stations\n",
    "from pyaspect.specfemio.write import write_record\n",
    "from pyaspect.specfemio.write import write_records\n",
    "from pyaspect.specfemio.read import read_stations\n",
    "from pyaspect.specfemio.read import read_solution\n",
    "from pyaspect.specfemio.read import read_cmtsolution\n",
    "from pyaspect.specfemio.read import read_forcesolution\n",
    "from pyaspect.specfemio.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "Extract the ndarray of the subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_dir  = 'data/output/'\n",
    "data_out_dir = data_in_dir\n",
    "!ls {data_in_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 \n",
    "\n",
    "Decompress the ndarray of the sliced, subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename then used it to decompress model\n",
    "ifqn = f'{data_out_dir}/vsliced_subsmp_smth_nam_2017_vp_vs_rho_Q_model_dx100_dy100_dz100_maxdepth5850_sig250.npz'\n",
    "vslice_gm3d, other_pars = decompress_gm3d_from_file(ifqn)\n",
    "\n",
    "print()\n",
    "print('decompressed gridded model\\n:',vslice_gm3d) \n",
    "print()\n",
    "print('other parameters:\\n',other_pars)\n",
    "print()\n",
    "\n",
    "# WARNING: this will unpack all other_pars, if you overwrite a variable of the samename as val(key), then you \n",
    "#          may not notice, and this may cause large headaches.  I use it because I am aware of it.\n",
    "'''\n",
    "for key in other_pars:\n",
    "    locals()[key] = other_pars[key]  #this is more advanced python than I think is reasonable for most \n",
    "sig_meters = sig\n",
    "''';\n",
    "\n",
    "# another way to get these varibles is just use the accessor functions for the gridmod3d.  We need them later.\n",
    "xmin = other_pars['xmin']\n",
    "dx   = other_pars['dx']\n",
    "nx   = other_pars['nx']\n",
    "ymin = other_pars['ymin']\n",
    "dy   = other_pars['dy']\n",
    "ny   = other_pars['ny']\n",
    "zmin = other_pars['zmin']\n",
    "dz   = other_pars['dz']\n",
    "nz   = other_pars['nz']\n",
    "sig_meters = other_pars['sig']  # this variable is used later\n",
    "print('sig_meters:',sig_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "nam_dims = list(vslice_gm3d.get_npoints())\n",
    "nam_origin = [0,0,-vslice_gm3d.get_gorigin()[2]]\n",
    "#nam_origin = list(vslice_gm3d.get_gorigin())\n",
    "#nam_origin[2] *= -1\n",
    "nam_origin = tuple(nam_origin)\n",
    "nam_spacing = list(vslice_gm3d.get_deltas())\n",
    "nam_spacing[2] *=-1\n",
    "nam_spacing = tuple(nam_spacing)\n",
    "print('nam_dims:',nam_dims)\n",
    "print('nam_origin:',nam_origin)\n",
    "print('nam_spacing:',nam_spacing)\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.dimensions = np.array(nam_dims) + 1\n",
    "grid.origin = nam_origin  # The bottom left corner of the data set\n",
    "grid.spacing = nam_spacing  # These are the cell sizes along each axis\n",
    "nam_pvalues = vslice_gm3d.getNPArray()[0]\n",
    "print('pvalues.shape:',nam_pvalues.shape)\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_arrays[\"values\"] = nam_pvalues.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "# Now plot the grid!\n",
    "cmap = plt.cm.jet\n",
    "#grid.plot(show_edges=True,cmap=cmap)\n",
    "grid.plot(cmap=cmap,opacity=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = grid.slice_orthogonal()\n",
    "\n",
    "#slices.plot(show_edges=True,cmap=cmap)\n",
    "slices.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create random virtual source (to specfem stations, but using reciprocity -- sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 1000\n",
    "\n",
    "#stay away from the edges of the model for derivatives \n",
    "# and to avoid boundary effects\n",
    "xy_pad = 1000 \n",
    "\n",
    "lrx = np.min(xc) + xy_pad\n",
    "lry = np.min(yc) + xy_pad\n",
    "lrz = -4300.0\n",
    "\n",
    "hrx = np.max(xc) - xy_pad\n",
    "hry = np.max(yc) - xy_pad\n",
    "hrz = -3000.0\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "srz = hrz - lrz\n",
    "\n",
    "r_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    rz = lrz + srz*np.random.rand()\n",
    "    r_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "r_xyz = np.array(r_xyz_list)\n",
    "    \n",
    "\n",
    "#r_xyz = np.vstack(np.meshgrid(rx,ry,rz)).reshape(3,-1).T\n",
    "print('r_xyz:\\n',r_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir {data_out_dir}/tmp\n",
    "!ls {data_out_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make cross (half or full) group for calculating spacial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'l_grp_stations' in locals() or 'l_grp_stations' in globals():\n",
    "    print('deleting')\n",
    "    del l_grp_stations\n",
    "    \n",
    "    \n",
    "# this is the path to the project dir on the cluster\n",
    "my_proj_dir = '/scratch/seismology/tcullison/test_mesh/Batch_Src_Test'\n",
    "\n",
    "    \n",
    "# make a list of station headers for each random location\n",
    "l_stations = []\n",
    "for i in range(len(r_xyz)):\n",
    "    \n",
    "    #name = 't' + str(i).zfill(len(str(len(r_xyz))))\n",
    "    tr_bname = 'tr'\n",
    "    new_s = StationHeader(name=tr_bname,\n",
    "                          network='NL',\n",
    "                          lat_yc=r_xyz[i,1],\n",
    "                          lon_xc=r_xyz[i,0],\n",
    "                          elevation=0.0,\n",
    "                          depth=-r_xyz[i,2],\n",
    "                          trid=i)\n",
    "    l_stations.append(new_s)\n",
    "#print('len(l_stats):',len(l_stations))\n",
    "                                           \n",
    "# make the group membors for each station above\n",
    "# function below returns a list[list[]] like structure\n",
    "m_delta = 250.0 # distance between cross stations for derivatives\n",
    "assert m_delta < xy_pad #see cells above this is padding\n",
    "l_grp_stations = make_grouped_half_cross_station_headers(stations=l_stations,delta=m_delta)\n",
    "name_list = []\n",
    "for s in flatten_grouped_headers(l_grp_stations):\n",
    "    name_list.append(f's{str(s.sid).zfill(2)}g{str(s.gid).zfill(2)}t{str(s.trid).zfill(6)}')\n",
    "    \n",
    "print('len(name_list):',len(name_list))\n",
    "print('len(name_set): ',len(set(name_list)))\n",
    "    \n",
    "\n",
    "# this is a list[] structure with unique stations (small chance a group member has same coordinate)\n",
    "s_grp_stations = sorted(copy.deepcopy(flatten_grouped_headers_unique(l_grp_stations)))\n",
    "\n",
    "# if lengths are the same then all group members are unique\n",
    "print(f'len(l_grp): {len(flatten_grouped_headers(l_grp_stations))}')\n",
    "print(f'len(s_grp): {len(s_grp_stations)}')\n",
    "\n",
    "#flatten the l_grp_stations (NOT guarantied unique!)\n",
    "l_grp_stations = sorted(flatten_grouped_headers(l_grp_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stations(data_out_dir + '/tmp',l_grp_stations,auto_name=True,auto_network=True)\n",
    "\n",
    "# For records only!\n",
    "#write_stations(data_out_dir + '/tmp',l_grp_stations,write_h=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh {data_out_dir + '/tmp'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqp = data_out_dir + '/tmp'\n",
    "rw_stations = sorted(read_stations(fqp))\n",
    "\n",
    "print('stations equal?:', rw_stations == l_grp_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(rw_stations)\n",
    "all_g_xyz[:,2] *= -1 #pyview z-up positive and oposize sign of standard geophysics \n",
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make random virtual recievers locations (solutions/sources in specfem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 5\n",
    "rz = -500\n",
    "\n",
    "lrx = np.min(xc)\n",
    "lry = np.min(yc)\n",
    "\n",
    "hrx = np.max(xc)\n",
    "hry = np.max(yc)\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "\n",
    "s_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    s_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "s_xyz = np.array(s_xyz_list)\n",
    "    \n",
    "\n",
    "print('s_xyz:\\n',s_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_spoints = pv.wrap(s_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=8,opacity=1,color='red')\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make force solution headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of station headers for each random location\n",
    "l_solutions = []\n",
    "for i in range(len(s_xyz)):\n",
    "    \n",
    "    #NOTE!!!! the depth is set to the NEGATIVE (makes it positive) due to sign convention\n",
    "    new_s = ForceSolutionHeader(ename=f'Event-{str(i).zfill(4)}',\n",
    "                                lat_yc=s_xyz[i,1],\n",
    "                                lon_xc=s_xyz[i,0],\n",
    "                                depth=-s_xyz[i,2],\n",
    "                                tshift=0.0,\n",
    "                                date=datetime.datetime.now(),\n",
    "                                f0=0.0,\n",
    "                                factor_fs=1,\n",
    "                                comp_src_EX=1,\n",
    "                                comp_src_NY=0,\n",
    "                                comp_src_Zup=0,\n",
    "                                proj_id=0,\n",
    "                                eid=i,\n",
    "                                sid=0)\n",
    "    l_solutions.append(new_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make solution group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_grp_solutions_h = make_grouped_triplet_force_solution_headers(solutions=l_solutions)\n",
    "grp_s_xyz = get_xyz_coords_from_solution_list(flatten_grouped_headers(l_grp_solutions_h))\n",
    "\n",
    "for s in flatten_grouped_headers(l_grp_solutions_h):\n",
    "    print(f'solution:\\n{s}')\n",
    "    print()\n",
    "print(f'coords:\\n{grp_s_xyz}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    p = os.path.join(data_out_dir, 'tmp')\n",
    "    print(p)\n",
    "    p = os.path.join(p, f'run{str(grp_s[0].eid+1).zfill(4)}')\n",
    "    print(p)\n",
    "    p = os.path.join(p, 'DATA')\n",
    "    print(p)\n",
    "    #p = f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}'\n",
    "    #print(p)\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_out_dir}/tmp\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s)\n",
    "    #write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s,write_h=False)\n",
    "    #print(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}')\n",
    "    #for s in grp_s:\n",
    "        #print(f's:\\n{s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -l {data_out_dir}tmp/run*/DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make record from solutions and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make stations list per solution_group \n",
    "poor_l_records = []\n",
    "\n",
    "for i in range(len(l_grp_solutions_h)):\n",
    "    grp_s = l_grp_solutions_h[i]\n",
    "    l_force_stations = []\n",
    "    for sol in grp_s:\n",
    "        l_stat = copy.deepcopy(l_grp_stations)\n",
    "        for s in l_stat:\n",
    "            s.eid = sol.eid\n",
    "            s.sid = sol.sid\n",
    "        l_force_stations += l_stat\n",
    "\n",
    "    record = RecordHeader(solutions_h=grp_s,stations_h=l_force_stations,rid=i)\n",
    "    poor_l_records.append(record)\n",
    "\n",
    "src_grouped_stations = []\n",
    "for sgrp in l_grp_solutions_h:\n",
    "    l_rgrp = []\n",
    "    for s in sgrp:\n",
    "        l_rgrp.append(copy.deepcopy(l_grp_stations))\n",
    "    src_grouped_stations.append(l_rgrp)\n",
    "\n",
    "print(f'len(l_grp_solutions_h):{len(l_grp_solutions_h)}')\n",
    "print(f'len(l_grp_solutions_h[0]):{len(l_grp_solutions_h[0])}')\n",
    "print(f'len(l_grp_solutions_h[0][0]):{len(l_grp_solutions_h[0][0])}')\n",
    "print(f'len(src_grouped_stations):{len(src_grouped_stations)}')\n",
    "print(f'len(src_grouped_stations[0]):{len(src_grouped_stations[0])}')\n",
    "l_records = []\n",
    "l_records = make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "    \n",
    "print(f'len(l_records): {len(l_records)}')\n",
    "print()\n",
    "\n",
    "for i in range(len(l_records)):\n",
    "    print(f'rec[{i}] == prec[{i}]: {l_records[i] == poor_l_records[i]}')\n",
    "\n",
    "print('Print Out of ALL Records\\n')\n",
    "for i in range(len(l_records)):\n",
    "    r = l_records[i]\n",
    "    p = poor_l_records[i]\n",
    "    print(f'Record-{r.rid}:\\n{r}')\n",
    "    print()\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickle record list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{data_out_dir}/tmp/project_record_list','wb')\n",
    "pickle.dump(l_records,f)\n",
    "f.close()\n",
    "\n",
    "!ls -ltrh {data_out_dir}/tmp/project_record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{data_out_dir}/tmp/project_record_list','rb')\n",
    "dill_l_records = pickle.load(f)\n",
    "f.close\n",
    "\n",
    "print('Check all records:')\n",
    "print()\n",
    "for i in range(len(dill_l_records)):\n",
    "    print(f'Dill_Rec == Orig_Rec?: {dill_l_records == l_records}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write records and headers in records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proj_fqp = f'{data_out_dir}tmp/' \n",
    "#write_record(proj_fqp,dill_l_records[0],fname='test_project.record')\n",
    "write_records(proj_fqp,dill_l_records,fname='test_project_record',auto_name=True,auto_network=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -ltrh data/output/tmp/\n",
    "print()\n",
    "!ls -ltrh {data_out_dir}tmp/run*/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in dill_l_records:\n",
    "    print(f'Record:\\n{r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyaspect.specfemio.utils import make_records\n",
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "from pyaspect.specfemio.utils import _mk_symlink\n",
    "from pyaspect.specfemio.headers import *\n",
    "\n",
    "\n",
    "MAX_SPEC_SRC = int(9999) # see SPECFEM3D_Cartesian manual\n",
    "\n",
    "# list of directories need for every event\n",
    "common_dir_struct = {'DATA': {},\n",
    "                     'OUTPUT_FILES' : {'DATABASES_MPI':{}},\n",
    "                     'SEM': {},\n",
    "                     'OBS': {},\n",
    "                     'SYN': {},\n",
    "                     'FILT_OBS': {},\n",
    "                     'FILT_SYN': {} }\n",
    "\n",
    "# list of directories only needed for the primary run0001 dir\n",
    "primary_dir_struct= {'INPUT_GRADIENT': {},\n",
    "                    'INPUT_KERNELS': {},\n",
    "                    'INPUT_MODEL': {},\n",
    "                    'OUTPUT_MODEL': {},\n",
    "                    'OUTPUT_SUM': {},\n",
    "                    'SMOOTH': {},\n",
    "                    'COMBINE': {},\n",
    "                    'topo': {} }\n",
    "\n",
    "\n",
    "def _make_proj_dirs(fqdn,access_rights=0o755):\n",
    "    if os.path.isdir(fqdn):\n",
    "        raise OSError(f'The directory {fqdn} has already been created')\n",
    "    try:\n",
    "        os.makedirs(fqdn, access_rights)\n",
    "    except OSError:\n",
    "        print(f'Creation of the directory {fqdn} failed')\n",
    "\n",
    "\n",
    "def _recursive_proj_dirs(dl,pdir,access_rights=0o755):\n",
    "\n",
    "    if len(dl.keys()) == 0:\n",
    "        return \n",
    "    else:\n",
    "        for dl_key in dl.keys():\n",
    "            new_dir = os.path.join(pdir, dl_key)\n",
    "            _make_proj_dirs(new_dir,access_rights=0o755)\n",
    "            _recursive_proj_dirs(dl[dl_key],new_dir)\n",
    "\n",
    "\n",
    "#def _mk_symlink(src,dst,lname):\n",
    "    #if not os.path.islink(dst):\n",
    "        #os.symlink(src, dst)\n",
    "\n",
    "\n",
    "def make_project(proj_name,\n",
    "                 proj_root_path,\n",
    "                 spec_fqp,\n",
    "                 pyutils_fqp,\n",
    "                 script_fqp,\n",
    "                 src_list,\n",
    "                 rec_list,\n",
    "                 obs_rec_list=None,\n",
    "                 ignore_spec_max=False):\n",
    "\n",
    "        \n",
    "        # Check src_list and rec_list (most likely to have a mistake)\n",
    "        if not isinstance(src_list,list):\n",
    "            raise TypeError('src_list must be a list type')\n",
    "\n",
    "        if not isinstance(rec_list,list):\n",
    "            raise TypeError('rec_list must be a list type')\n",
    "        \n",
    "        if obs_rec_list != None:\n",
    "            if not isinstance(obs_rec_list,list):\n",
    "                raise TypeError('obs_rec_list must be a list type')\n",
    "\n",
    "        batch_size = None\n",
    "        nevents    = None\n",
    "        \n",
    "        if isinstance(src_list[0],list):\n",
    "            if not isinstance(src_list[0][0],SolutionHeader):\n",
    "                raise TypeError(f'elemts of src_list[:][:] must be of type={type(SolutionHeader)}')\n",
    "            batch_size = len(src_list[0])\n",
    "        else:\n",
    "            if not isinstance(src_list[0],SolutionHeader):\n",
    "                raise TypeError(f'elements of src_list[:] must be of type={type(SolutionHeader)}')\n",
    "            batch_size = 1\n",
    "\n",
    "        nevents    = len(src_list)\n",
    "\n",
    "        # NON-Batch src \n",
    "        if batch_size == 1:\n",
    "            if not isinstance(rec_list[0],list):\n",
    "                raise TypeError(f'rec_list[:][:] must be a list type')\n",
    "            if not isinstance(rec_list[0][0],StationHeader):\n",
    "                raise TypeError(f'elemts of rec_list[:][:] must be of type={type(StationHeader)}')\n",
    "\n",
    "            # if doing fwi \n",
    "            # TODO: better to not repeat code\n",
    "            if obs_rec_list != None:\n",
    "                if not isinstance(obs_rec_list[0],list):\n",
    "                    raise TypeError(f'obs_rec_list[:][:] must be a list type')\n",
    "                if len(obs_rec_list) != nevents:\n",
    "                    raise Exception(f'number of observed data traces is inconsitant with number of receiver stations')\n",
    "                #TODO: include Trace when when finished\n",
    "                #if not isinstance(obs_rec_list[0,0],Trace):\n",
    "                    #raise TypeError(f'elemts of obs_rec_list[0,:] must be of type={type(StationHeader)}')\n",
    "\n",
    "        # Batch src \n",
    "        elif 1 < batch_size:\n",
    "            if not isinstance(rec_list[0][0],list):\n",
    "                raise TypeError(f'rec_list[:][:] must be a list type')\n",
    "            if not isinstance(rec_list[0][0][0],StationHeader):\n",
    "                raise TypeError(f'elemts of rec_list[:][:][:] must be of type={type(StationHeader)}')\n",
    "\n",
    "            if obs_rec_list != None:\n",
    "                if len(obs_rec_list) != nevents:\n",
    "                    raise Exception(f'number of observed data traces is inconsitant with number of receiver stations')\n",
    "                if not isinstance(obs_rec_list[0][0],list):\n",
    "                    raise TypeError(f'obs_rec_list[:][:] must be a list type')\n",
    "                #TODO: include Trace when when finished\n",
    "                #if not isinstance(obs_rec_list[0,0,0],Trace):\n",
    "                #    raise TypeError(f'elemts of obs_rec_list[:][:][:] must be of type={type(StationHeader)}')\n",
    "\n",
    "        # not designed/implemented for other types\n",
    "        else:\n",
    "            raise Exception('structure of src_list and rec_list are not complient')\n",
    "\n",
    "\n",
    "        if len(rec_list) != nevents:\n",
    "            raise Exception(f'number of receiver stations is inconsitant with number of sourcs')\n",
    "\n",
    "\n",
    "\n",
    "        if not isinstance(spec_fqp,str):\n",
    "            raise TypeError('spec_fqp must be a str type')\n",
    "\n",
    "        if not isinstance(pyutils_fqp,str):\n",
    "            raise TypeError('pyutils_fqp must be a str type')\n",
    "        \n",
    "        if not isinstance(script_fqp,str):\n",
    "            raise TypeError('script_fqp must be a str type')\n",
    "        \n",
    "        if not isinstance(proj_root_path,str):\n",
    "            raise TypeError('proj_root_path must be a str type')\n",
    "\n",
    "\n",
    "        if MAX_SPEC_SRC < nevents and not ignore_spec_max:\n",
    "            excpt_str  = f'The number events exceeds MAX_SPEC_SRC.\\n'\n",
    "            excpt_str += f'To create more than {MAX_SPEC_SRC} directoies,'\n",
    "            excpt_str += f'please set \\'ignore_spec_max\\' to True.'\n",
    "            raise Exception(excpt_str)\n",
    "\n",
    "\n",
    "\n",
    "        spec_bin_fqp   = os.path.join(spec_fqp, 'bin')\n",
    "        spec_utils_fqp   = os.path.join(spec_fqp, 'utils')\n",
    "\n",
    "        proj_fqdn = os.path.join(proj_root_path, proj_name)\n",
    "\n",
    "        fwd_only = True\n",
    "        if obs_rec_list != None:\n",
    "            fwd_only = False\n",
    "\n",
    "        access_rights = 0o755\n",
    "\n",
    "        l_records = make_records(l_src=src_list,l_rec=rec_list)\n",
    "\n",
    "\n",
    "        #create_project_dirs and write records\n",
    "\n",
    "        # create main project dir\n",
    "        err = _make_proj_dirs(proj_fqdn)\n",
    "        \n",
    "        # create project level symlinks \n",
    "        lname = 'pyutils'\n",
    "        src = pyutils_fqp\n",
    "        dst = os.path.join(proj_fqdn, lname)\n",
    "        _mk_symlink(src,dst,lname)\n",
    "\n",
    "        lname = 'scriptutils'\n",
    "        src = script_fqp\n",
    "        dst = os.path.join(proj_fqdn, lname)\n",
    "        _mk_symlink(src,dst,lname)\n",
    "\n",
    "        #loop over number of events and create (run####) dirs\n",
    "        for e in range(nevents):\n",
    "\n",
    "            # make event dir\n",
    "            \n",
    "            rdir = 'run' + str(e+1).zfill(4)\n",
    "            edir = os.path.join(proj_fqdn, rdir)\n",
    "            err  = _make_proj_dirs(edir)\n",
    "            \n",
    "            ddir = os.path.join(edir, 'DATA')\n",
    "\n",
    "            # make sim links for each event dir (related to the computational node(s) filesytem\n",
    "            lname = 'bin'\n",
    "            src = spec_bin_fqp\n",
    "            print(f'src: {src}')\n",
    "            dst = os.path.join(edir, lname)\n",
    "            _mk_symlink(src,dst,lname)\n",
    "\n",
    "            lname = 'utils'\n",
    "            src = spec_utils_fqp\n",
    "            print(f'src: {src}')\n",
    "            dst = os.path.join(edir, lname)\n",
    "            _mk_symlink(src,dst,lname)\n",
    "\n",
    "            \n",
    "            # make subdirectorieds for each event\n",
    "            _recursive_proj_dirs(common_dir_struct,edir)\n",
    "                \n",
    "            # make sub-dirs needed only in the primary run0001 dir (used for inversion, model-updating, etc.)\n",
    "            if e == 0 and not fwd_only:\n",
    "                _recursive_proj_dirs(primary_dir_struct,edir)\n",
    "                \n",
    "        #end for e in range(nevents)\n",
    "                \n",
    "            \n",
    "        write_records(proj_fqdn,l_records,\n",
    "                      fname=proj_name + '_records',\n",
    "                      write_record_h=True,\n",
    "                      write_h=True,\n",
    "                      auto_name=True,\n",
    "                      auto_network=True)\n",
    "            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "test_proj_name = 'FirstTestProject'\n",
    "test_proj_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects')\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "print(f'test_proj_fqp: {test_proj_fqp}')\n",
    "!ls {test_proj_fqp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.project import make_project\n",
    "#make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "make_project(test_proj_name,\n",
    "             test_proj_fqp,\n",
    "             test_spec_fqp,\n",
    "             test_pyutils_fqp,\n",
    "             test_script_fqp,\n",
    "             l_grp_solutions_h,\n",
    "             src_grouped_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -ltrh data/output/tmp/TestProjects/FirstTestProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat ~/Seismic/myscratch/test_mesh/NN_Batch_Src_Test_E5_Stub/run0001/DATA/Par_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = goobleydoobleydoo('42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sillyfunc(index=None):\n",
    "    if index == None:\n",
    "        print('none')\n",
    "    else:\n",
    "        print(f'index={index}')\n",
    "        \n",
    "sillyfunc(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = []\n",
    "for i in range(6):\n",
    "    df_rec.append({'sid':i%3,'eid':0})\n",
    "df = pd.DataFrame.from_records(df_rec)\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(df['sid'])\n",
    "print()\n",
    "s = set(df['sid'])\n",
    "ss = set(df['sid'])\n",
    "print(s)\n",
    "print(f's == ss: {s == ss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
