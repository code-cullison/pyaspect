{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA_random_eqrthquake_locations_w_pyvista_vtk\n",
    "\n",
    "Testing the random picking of subsurface eqrthquake locations (moment-tensor locations) and 3D plotting of the picks and model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all packages\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sys import argv\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from pyaspect.model.gridmod3d import gridmod3d as gm\n",
    "from pyaspect.model.bbox import bbox as bb\n",
    "from pyaspect.model.gm3d_utils import compress_gm3d_to_file\n",
    "from pyaspect.model.gm3d_utils import decompress_gm3d_from_file\n",
    "from pyaspect.moment_tensor import MomentTensor\n",
    "from pyaspect.specfemio.headers import StationHeader\n",
    "from pyaspect.specfemio.headers import SolutionHeader\n",
    "from pyaspect.specfemio.headers import CMTSolutionHeader\n",
    "from pyaspect.specfemio.headers import ForceSolutionHeader\n",
    "from pyaspect.specfemio.headers import RecordHeader\n",
    "from pyaspect.specfemio.write import write_cmtsolution\n",
    "from pyaspect.specfemio.write import write_forcesolution\n",
    "from pyaspect.specfemio.write import write_grouped_forcesolutions\n",
    "from pyaspect.specfemio.write import write_stations\n",
    "from pyaspect.specfemio.write import write_record\n",
    "from pyaspect.specfemio.write import write_records\n",
    "from pyaspect.specfemio.read import read_stations\n",
    "from pyaspect.specfemio.read import read_solution\n",
    "from pyaspect.specfemio.read import read_cmtsolution\n",
    "from pyaspect.specfemio.read import read_forcesolution\n",
    "from pyaspect.specfemio.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "Extract the ndarray of the subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_dir  = 'data/output/'\n",
    "data_out_dir = data_in_dir\n",
    "!ls {data_in_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 \n",
    "\n",
    "Decompress the ndarray of the sliced, subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename then used it to decompress model\n",
    "ifqn = f'{data_out_dir}/vsliced_subsmp_smth_nam_2017_vp_vs_rho_Q_model_dx100_dy100_dz100_maxdepth5850_sig250.npz'\n",
    "vslice_gm3d, other_pars = decompress_gm3d_from_file(ifqn)\n",
    "\n",
    "print()\n",
    "print('decompressed gridded model\\n:',vslice_gm3d) \n",
    "print()\n",
    "print('other parameters:\\n',other_pars)\n",
    "print()\n",
    "\n",
    "# WARNING: this will unpack all other_pars, if you overwrite a variable of the samename as val(key), then you \n",
    "#          may not notice, and this may cause large headaches.  I use it because I am aware of it.\n",
    "'''\n",
    "for key in other_pars:\n",
    "    locals()[key] = other_pars[key]  #this is more advanced python than I think is reasonable for most \n",
    "sig_meters = sig\n",
    "''';\n",
    "\n",
    "# another way to get these varibles is just use the accessor functions for the gridmod3d.  We need them later.\n",
    "xmin = other_pars['xmin']\n",
    "dx   = other_pars['dx']\n",
    "nx   = other_pars['nx']\n",
    "ymin = other_pars['ymin']\n",
    "dy   = other_pars['dy']\n",
    "ny   = other_pars['ny']\n",
    "zmin = other_pars['zmin']\n",
    "dz   = other_pars['dz']\n",
    "nz   = other_pars['nz']\n",
    "sig_meters = other_pars['sig']  # this variable is used later\n",
    "print('sig_meters:',sig_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "nam_dims = list(vslice_gm3d.get_npoints())\n",
    "nam_origin = [0,0,-vslice_gm3d.get_gorigin()[2]]\n",
    "#nam_origin = list(vslice_gm3d.get_gorigin())\n",
    "#nam_origin[2] *= -1\n",
    "nam_origin = tuple(nam_origin)\n",
    "nam_spacing = list(vslice_gm3d.get_deltas())\n",
    "nam_spacing[2] *=-1\n",
    "nam_spacing = tuple(nam_spacing)\n",
    "print('nam_dims:',nam_dims)\n",
    "print('nam_origin:',nam_origin)\n",
    "print('nam_spacing:',nam_spacing)\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.dimensions = np.array(nam_dims) + 1\n",
    "grid.origin = nam_origin  # The bottom left corner of the data set\n",
    "grid.spacing = nam_spacing  # These are the cell sizes along each axis\n",
    "nam_pvalues = vslice_gm3d.getNPArray()[0]\n",
    "print('pvalues.shape:',nam_pvalues.shape)\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_arrays[\"values\"] = nam_pvalues.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "# Now plot the grid!\n",
    "cmap = plt.cm.jet\n",
    "#grid.plot(show_edges=True,cmap=cmap)\n",
    "grid.plot(cmap=cmap,opacity=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = grid.slice_orthogonal()\n",
    "\n",
    "#slices.plot(show_edges=True,cmap=cmap)\n",
    "slices.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create random virtual source (to specfem stations, but using reciprocity -- sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 1000\n",
    "\n",
    "#stay away from the edges of the model for derivatives \n",
    "# and to avoid boundary effects\n",
    "xy_pad = 1000 \n",
    "\n",
    "lrx = np.min(xc) + xy_pad\n",
    "lry = np.min(yc) + xy_pad\n",
    "lrz = -4300.0\n",
    "\n",
    "hrx = np.max(xc) - xy_pad\n",
    "hry = np.max(yc) - xy_pad\n",
    "hrz = -3000.0\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "srz = hrz - lrz\n",
    "\n",
    "r_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    rz = lrz + srz*np.random.rand()\n",
    "    r_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "r_xyz = np.array(r_xyz_list)\n",
    "    \n",
    "\n",
    "#r_xyz = np.vstack(np.meshgrid(rx,ry,rz)).reshape(3,-1).T\n",
    "print('r_xyz:\\n',r_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir {data_out_dir}/tmp\n",
    "!ls {data_out_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make cross (half or full) group for calculating spacial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'l_grp_stations' in locals() or 'l_grp_stations' in globals():\n",
    "    print('deleting')\n",
    "    del l_grp_stations\n",
    "    \n",
    "    \n",
    "# this is the path to the project dir on the cluster\n",
    "my_proj_dir = '/scratch/seismology/tcullison/test_mesh/Batch_Src_Test'\n",
    "\n",
    "    \n",
    "# make a list of station headers for each random location\n",
    "l_stations = []\n",
    "for i in range(len(r_xyz)):\n",
    "    \n",
    "    #name = 't' + str(i).zfill(len(str(len(r_xyz))))\n",
    "    tr_bname = 'tr'\n",
    "    new_s = StationHeader(name=tr_bname,\n",
    "                          network='NL',\n",
    "                          lat_yc=r_xyz[i,1],\n",
    "                          lon_xc=r_xyz[i,0],\n",
    "                          elevation=0.0,\n",
    "                          depth=-r_xyz[i,2],\n",
    "                          trid=i)\n",
    "    l_stations.append(new_s)\n",
    "#print('len(l_stats):',len(l_stations))\n",
    "                                           \n",
    "# make the group membors for each station above\n",
    "# function below returns a list[list[]] like structure\n",
    "m_delta = 250.0 # distance between cross stations for derivatives\n",
    "assert m_delta < xy_pad #see cells above this is padding\n",
    "l_grp_stations = make_grouped_half_cross_station_headers(stations=l_stations,delta=m_delta)\n",
    "name_list = []\n",
    "for s in flatten_grouped_headers(l_grp_stations):\n",
    "    name_list.append(f's{str(s.sid).zfill(2)}g{str(s.gid).zfill(2)}t{str(s.trid).zfill(6)}')\n",
    "    \n",
    "print('len(name_list):',len(name_list))\n",
    "print('len(name_set): ',len(set(name_list)))\n",
    "    \n",
    "\n",
    "# this is a list[] structure with unique stations (small chance a group member has same coordinate)\n",
    "s_grp_stations = sorted(copy.deepcopy(flatten_grouped_headers_unique(l_grp_stations)))\n",
    "\n",
    "# if lengths are the same then all group members are unique\n",
    "print(f'len(l_grp): {len(flatten_grouped_headers(l_grp_stations))}')\n",
    "print(f'len(s_grp): {len(s_grp_stations)}')\n",
    "\n",
    "#flatten the l_grp_stations (NOT guarantied unique!)\n",
    "l_grp_stations = sorted(flatten_grouped_headers(l_grp_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stations(data_out_dir + '/tmp',l_grp_stations,auto_name=True,auto_network=True)\n",
    "\n",
    "# For records only!\n",
    "#write_stations(data_out_dir + '/tmp',l_grp_stations,write_h=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh {data_out_dir + '/tmp'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqp = data_out_dir + '/tmp'\n",
    "rw_stations = sorted(read_stations(fqp))\n",
    "\n",
    "print('stations equal?:', rw_stations == l_grp_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(rw_stations)\n",
    "all_g_xyz[:,2] *= -1 #pyview z-up positive and oposize sign of standard geophysics \n",
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make random virtual recievers locations (solutions/sources in specfem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 5\n",
    "rz = -500\n",
    "\n",
    "lrx = np.min(xc)\n",
    "lry = np.min(yc)\n",
    "\n",
    "hrx = np.max(xc)\n",
    "hry = np.max(yc)\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "\n",
    "s_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    s_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "s_xyz = np.array(s_xyz_list)\n",
    "    \n",
    "\n",
    "print('s_xyz:\\n',s_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_spoints = pv.wrap(s_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=8,opacity=1,color='red')\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make force solution headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of station headers for each random location\n",
    "l_solutions = []\n",
    "for i in range(len(s_xyz)):\n",
    "    \n",
    "    #NOTE!!!! the depth is set to the NEGATIVE (makes it positive) due to sign convention\n",
    "    new_s = ForceSolutionHeader(ename=f'Event-{str(i).zfill(4)}',\n",
    "                                lat_yc=s_xyz[i,1],\n",
    "                                lon_xc=s_xyz[i,0],\n",
    "                                depth=-s_xyz[i,2],\n",
    "                                tshift=0.0,\n",
    "                                date=datetime.datetime.now(),\n",
    "                                f0=0.0,\n",
    "                                factor_fs=1,\n",
    "                                comp_src_EX=1,\n",
    "                                comp_src_NY=0,\n",
    "                                comp_src_Zup=0,\n",
    "                                proj_id=0,\n",
    "                                eid=i,\n",
    "                                sid=0)\n",
    "    l_solutions.append(new_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make solution group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_grp_solutions_h = make_grouped_triplet_force_solution_headers(solutions=l_solutions)\n",
    "grp_s_xyz = get_xyz_coords_from_solution_list(flatten_grouped_headers(l_grp_solutions_h))\n",
    "\n",
    "for s in flatten_grouped_headers(l_grp_solutions_h):\n",
    "    print(f'solution:\\n{s}')\n",
    "    print()\n",
    "print(f'coords:\\n{grp_s_xyz}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    p = os.path.join(data_out_dir, 'tmp')\n",
    "    print(p)\n",
    "    p = os.path.join(p, f'run{str(grp_s[0].eid+1).zfill(4)}')\n",
    "    print(p)\n",
    "    p = os.path.join(p, 'DATA')\n",
    "    print(p)\n",
    "    #p = f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}'\n",
    "    #print(p)\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_out_dir}/tmp\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s)\n",
    "    #write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s,write_h=False)\n",
    "    #print(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}')\n",
    "    #for s in grp_s:\n",
    "        #print(f's:\\n{s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -l {data_out_dir}tmp/run*/DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make record from solutions and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make stations list per solution_group \n",
    "poor_l_records = []\n",
    "\n",
    "for i in range(len(l_grp_solutions_h)):\n",
    "    grp_s = l_grp_solutions_h[i]\n",
    "    l_force_stations = []\n",
    "    for sol in grp_s:\n",
    "        l_stat = copy.deepcopy(l_grp_stations)\n",
    "        for s in l_stat:\n",
    "            s.eid = sol.eid\n",
    "            s.sid = sol.sid\n",
    "        l_force_stations += l_stat\n",
    "\n",
    "    record = RecordHeader(solutions_h=grp_s,stations_h=l_force_stations,rid=i)\n",
    "    poor_l_records.append(record)\n",
    "\n",
    "src_grouped_stations = []\n",
    "for sgrp in l_grp_solutions_h:\n",
    "    l_rgrp = []\n",
    "    for s in sgrp:\n",
    "        l_rgrp.append(copy.deepcopy(l_grp_stations))\n",
    "    src_grouped_stations.append(l_rgrp)\n",
    "\n",
    "print(f'len(l_grp_solutions_h):{len(l_grp_solutions_h)}')\n",
    "print(f'len(l_grp_solutions_h[0]):{len(l_grp_solutions_h[0])}')\n",
    "print(f'len(l_grp_solutions_h[0][0]):{len(l_grp_solutions_h[0][0])}')\n",
    "print(f'len(src_grouped_stations):{len(src_grouped_stations)}')\n",
    "print(f'len(src_grouped_stations[0]):{len(src_grouped_stations[0])}')\n",
    "l_records = []\n",
    "l_records = make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "    \n",
    "print(f'len(l_records): {len(l_records)}')\n",
    "print()\n",
    "\n",
    "for i in range(len(l_records)):\n",
    "    print(f'rec[{i}] == prec[{i}]: {l_records[i] == poor_l_records[i]}')\n",
    "\n",
    "print('Print Out of ALL Records\\n')\n",
    "for i in range(len(l_records)):\n",
    "    r = l_records[i]\n",
    "    p = poor_l_records[i]\n",
    "    print(f'Record-{r.rid}:\\n{r}')\n",
    "    print()\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickle record list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{data_out_dir}/tmp/project_record_list','wb')\n",
    "pickle.dump(l_records,f)\n",
    "f.close()\n",
    "\n",
    "!ls -ltrh {data_out_dir}/tmp/project_record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{data_out_dir}/tmp/project_record_list','rb')\n",
    "dill_l_records = pickle.load(f)\n",
    "f.close\n",
    "\n",
    "print('Check all records:')\n",
    "print()\n",
    "for i in range(len(dill_l_records)):\n",
    "    print(f'Dill_Rec == Orig_Rec?: {dill_l_records == l_records}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write records and headers in records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proj_fqp = f'{data_out_dir}tmp/' \n",
    "#write_record(proj_fqp,dill_l_records[0],fname='test_project.record')\n",
    "write_records(proj_fqp,dill_l_records,fname='test_project_record',auto_name=True,auto_network=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -ltrh data/output/tmp/\n",
    "print()\n",
    "!ls -ltrh {data_out_dir}tmp/run*/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in dill_l_records:\n",
    "    print(f'Record:\\n{r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyaspect.specfemio.utils import make_records\n",
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "from pyaspect.specfemio.utils import _mk_symlink\n",
    "from pyaspect.specfemio.headers import *\n",
    "\n",
    "\n",
    "MAX_SPEC_SRC = int(9999) # see SPECFEM3D_Cartesian manual\n",
    "\n",
    "# list of directories need for every event\n",
    "common_dir_struct = {'DATA': {},\n",
    "                     'OUTPUT_FILES' : {'DATABASES_MPI':{}},\n",
    "                     'SEM': {},\n",
    "                     'OBS': {},\n",
    "                     'SYN': {},\n",
    "                     'FILT_OBS': {},\n",
    "                     'FILT_SYN': {} }\n",
    "\n",
    "# list of directories only needed for the primary run0001 dir\n",
    "primary_dir_struct= {'INPUT_GRADIENT': {},\n",
    "                    'INPUT_KERNELS': {},\n",
    "                    'INPUT_MODEL': {},\n",
    "                    'OUTPUT_MODEL': {},\n",
    "                    'OUTPUT_SUM': {},\n",
    "                    'SMOOTH': {},\n",
    "                    'COMBINE': {},\n",
    "                    'topo': {} }\n",
    "\n",
    "\n",
    "def _make_proj_dirs(fqdn,access_rights=0o755):\n",
    "    if os.path.isdir(fqdn):\n",
    "        raise OSError(f'The directory {fqdn} has already been created')\n",
    "    try:\n",
    "        os.makedirs(fqdn, access_rights)\n",
    "    except OSError:\n",
    "        print(f'Creation of the directory {fqdn} failed')\n",
    "\n",
    "\n",
    "def _recursive_proj_dirs(dl,pdir,access_rights=0o755):\n",
    "\n",
    "    if len(dl.keys()) == 0:\n",
    "        return \n",
    "    else:\n",
    "        for dl_key in dl.keys():\n",
    "            new_dir = os.path.join(pdir, dl_key)\n",
    "            _make_proj_dirs(new_dir,access_rights=0o755)\n",
    "            _recursive_proj_dirs(dl[dl_key],new_dir)\n",
    "\n",
    "\n",
    "#def _mk_symlink(src,dst,lname):\n",
    "    #if not os.path.islink(dst):\n",
    "        #os.symlink(src, dst)\n",
    "\n",
    "\n",
    "def make_project(proj_name,\n",
    "                 proj_root_path,\n",
    "                 spec_fqp,\n",
    "                 pyutils_fqp,\n",
    "                 script_fqp,\n",
    "                 src_list,\n",
    "                 rec_list,\n",
    "                 obs_rec_list=None,\n",
    "                 ignore_spec_max=False):\n",
    "\n",
    "        \n",
    "        # Check src_list and rec_list (most likely to have a mistake)\n",
    "        if not isinstance(src_list,list):\n",
    "            raise TypeError('src_list must be a list type')\n",
    "\n",
    "        if not isinstance(rec_list,list):\n",
    "            raise TypeError('rec_list must be a list type')\n",
    "        \n",
    "        if obs_rec_list != None:\n",
    "            if not isinstance(obs_rec_list,list):\n",
    "                raise TypeError('obs_rec_list must be a list type')\n",
    "\n",
    "        batch_size = None\n",
    "        nevents    = None\n",
    "        \n",
    "        if isinstance(src_list[0],list):\n",
    "            if not isinstance(src_list[0][0],SolutionHeader):\n",
    "                raise TypeError(f'elemts of src_list[:][:] must be of type={type(SolutionHeader)}')\n",
    "            batch_size = len(src_list[0])\n",
    "        else:\n",
    "            if not isinstance(src_list[0],SolutionHeader):\n",
    "                raise TypeError(f'elements of src_list[:] must be of type={type(SolutionHeader)}')\n",
    "            batch_size = 1\n",
    "\n",
    "        nevents    = len(src_list)\n",
    "\n",
    "        # NON-Batch src \n",
    "        if batch_size == 1:\n",
    "            if not isinstance(rec_list[0],list):\n",
    "                raise TypeError(f'rec_list[:][:] must be a list type')\n",
    "            if not isinstance(rec_list[0][0],StationHeader):\n",
    "                raise TypeError(f'elemts of rec_list[:][:] must be of type={type(StationHeader)}')\n",
    "\n",
    "            # if doing fwi \n",
    "            # TODO: better to not repeat code\n",
    "            if obs_rec_list != None:\n",
    "                if not isinstance(obs_rec_list[0],list):\n",
    "                    raise TypeError(f'obs_rec_list[:][:] must be a list type')\n",
    "                if len(obs_rec_list) != nevents:\n",
    "                    raise Exception(f'number of observed data traces is inconsitant with number of receiver stations')\n",
    "                #TODO: include Trace when when finished\n",
    "                #if not isinstance(obs_rec_list[0,0],Trace):\n",
    "                    #raise TypeError(f'elemts of obs_rec_list[0,:] must be of type={type(StationHeader)}')\n",
    "\n",
    "        # Batch src \n",
    "        elif 1 < batch_size:\n",
    "            if not isinstance(rec_list[0][0],list):\n",
    "                raise TypeError(f'rec_list[:][:] must be a list type')\n",
    "            if not isinstance(rec_list[0][0][0],StationHeader):\n",
    "                raise TypeError(f'elemts of rec_list[:][:][:] must be of type={type(StationHeader)}')\n",
    "\n",
    "            if obs_rec_list != None:\n",
    "                if len(obs_rec_list) != nevents:\n",
    "                    raise Exception(f'number of observed data traces is inconsitant with number of receiver stations')\n",
    "                if not isinstance(obs_rec_list[0][0],list):\n",
    "                    raise TypeError(f'obs_rec_list[:][:] must be a list type')\n",
    "                #TODO: include Trace when when finished\n",
    "                #if not isinstance(obs_rec_list[0,0,0],Trace):\n",
    "                #    raise TypeError(f'elemts of obs_rec_list[:][:][:] must be of type={type(StationHeader)}')\n",
    "\n",
    "        # not designed/implemented for other types\n",
    "        else:\n",
    "            raise Exception('structure of src_list and rec_list are not complient')\n",
    "\n",
    "\n",
    "        if len(rec_list) != nevents:\n",
    "            raise Exception(f'number of receiver stations is inconsitant with number of sourcs')\n",
    "\n",
    "\n",
    "\n",
    "        if not isinstance(spec_fqp,str):\n",
    "            raise TypeError('spec_fqp must be a str type')\n",
    "\n",
    "        if not isinstance(pyutils_fqp,str):\n",
    "            raise TypeError('pyutils_fqp must be a str type')\n",
    "        \n",
    "        if not isinstance(script_fqp,str):\n",
    "            raise TypeError('script_fqp must be a str type')\n",
    "        \n",
    "        if not isinstance(proj_root_path,str):\n",
    "            raise TypeError('proj_root_path must be a str type')\n",
    "\n",
    "\n",
    "        if MAX_SPEC_SRC < nevents and not ignore_spec_max:\n",
    "            excpt_str  = f'The number events exceeds MAX_SPEC_SRC.\\n'\n",
    "            excpt_str += f'To create more than {MAX_SPEC_SRC} directoies,'\n",
    "            excpt_str += f'please set \\'ignore_spec_max\\' to True.'\n",
    "            raise Exception(excpt_str)\n",
    "\n",
    "\n",
    "\n",
    "        spec_bin_fqp   = os.path.join(spec_fqp, 'bin')\n",
    "        spec_utils_fqp   = os.path.join(spec_fqp, 'utils')\n",
    "\n",
    "        proj_fqdn = os.path.join(proj_root_path, proj_name)\n",
    "\n",
    "        fwd_only = True\n",
    "        if obs_rec_list != None:\n",
    "            fwd_only = False\n",
    "\n",
    "        access_rights = 0o755\n",
    "\n",
    "        l_records = make_records(l_src=src_list,l_rec=rec_list)\n",
    "\n",
    "\n",
    "        #create_project_dirs and write records\n",
    "\n",
    "        # create main project dir\n",
    "        err = _make_proj_dirs(proj_fqdn)\n",
    "        \n",
    "        # create project level symlinks \n",
    "        lname = 'pyutils'\n",
    "        src = pyutils_fqp\n",
    "        dst = os.path.join(proj_fqdn, lname)\n",
    "        _mk_symlink(src,dst,lname)\n",
    "\n",
    "        lname = 'scriptutils'\n",
    "        src = script_fqp\n",
    "        dst = os.path.join(proj_fqdn, lname)\n",
    "        _mk_symlink(src,dst,lname)\n",
    "\n",
    "        #loop over number of events and create (run####) dirs\n",
    "        for e in range(nevents):\n",
    "\n",
    "            # make event dir\n",
    "            \n",
    "            rdir = 'run' + str(e+1).zfill(4)\n",
    "            edir = os.path.join(proj_fqdn, rdir)\n",
    "            err  = _make_proj_dirs(edir)\n",
    "            \n",
    "            ddir = os.path.join(edir, 'DATA')\n",
    "\n",
    "            # make sim links for each event dir (related to the computational node(s) filesytem\n",
    "            lname = 'bin'\n",
    "            src = spec_bin_fqp\n",
    "            print(f'src: {src}')\n",
    "            dst = os.path.join(edir, lname)\n",
    "            _mk_symlink(src,dst,lname)\n",
    "\n",
    "            lname = 'utils'\n",
    "            src = spec_utils_fqp\n",
    "            print(f'src: {src}')\n",
    "            dst = os.path.join(edir, lname)\n",
    "            _mk_symlink(src,dst,lname)\n",
    "\n",
    "            \n",
    "            # make subdirectorieds for each event\n",
    "            _recursive_proj_dirs(common_dir_struct,edir)\n",
    "                \n",
    "            # make sub-dirs needed only in the primary run0001 dir (used for inversion, model-updating, etc.)\n",
    "            if e == 0 and not fwd_only:\n",
    "                _recursive_proj_dirs(primary_dir_struct,edir)\n",
    "                \n",
    "        #end for e in range(nevents)\n",
    "                \n",
    "            \n",
    "        write_records(proj_fqdn,l_records,\n",
    "                      fname=proj_name + '_records',\n",
    "                      write_record_h=True,\n",
    "                      write_h=True,\n",
    "                      auto_name=True,\n",
    "                      auto_network=True)\n",
    "            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "test_proj_name = 'FirstTestProject'\n",
    "test_proj_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects')\n",
    "test_parfile_fqp =  os.path.join(data_out_dir, 'Par_file')\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "print(f'test_proj_fqp: {test_proj_fqp}')\n",
    "!ls {test_proj_fqp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.project import make_project\n",
    "#make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "make_project(test_proj_name,\n",
    "             test_proj_fqp,\n",
    "             test_parfile_fqp,\n",
    "             test_spec_fqp,\n",
    "             test_pyutils_fqp,\n",
    "             test_script_fqp,\n",
    "             l_grp_solutions_h,\n",
    "             src_grouped_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -ltrh data/output/tmp/TestProjects/FirstTestProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parfile_fqp = os.path.join(data_out_dir,'Par_file')\n",
    "out_parfile_fqp = os.path.join(data_out_dir,'New_Par_file')\n",
    "print(f'parfile_fqp: {parfile_fqp}')\n",
    "with open(parfile_fqp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "il = 0\n",
    "for line in lines:\n",
    "    if il == 12:\n",
    "        break\n",
    "    print(line)\n",
    "    il += 1\n",
    "    \n",
    "print()\n",
    "print('START search')\n",
    "print()\n",
    "\n",
    "match = 'SAVE_FORWARD'\n",
    "for il in range(len(lines)):\n",
    "    line = lines[il]\n",
    "    wlist = line.split()\n",
    "    if len(wlist) == 0:\n",
    "        il += 1\n",
    "        continue\n",
    "    if wlist[0] == match:\n",
    "        print(f'old_line: {line}')\n",
    "        print(f'Match: {match} at line: {il}')\n",
    "        val = line.split('=')[-1].strip()\n",
    "        print(f'Setting = {val}')\n",
    "        si = line.find(val)\n",
    "        print(f'found: {si}')\n",
    "        new_line = line[:si] + '.true.'\n",
    "        print(f'new_line: {new_line}')\n",
    "        lines[il] = new_line\n",
    "        break\n",
    "        '''\n",
    "        ieq = line.find('=')\n",
    "        print('ieq:',ieq)\n",
    "        iss = line.find('.',ieq)\n",
    "        print('iss:',iss)\n",
    "        ise = line.find('.',iss+1)\n",
    "        print('ise:',ise)\n",
    "        print(f'Setting = {line[iss:ise+1]}')\n",
    "        '''\n",
    "\n",
    "print('\\n\\n----------------------- changed -------------------------')\n",
    "il = 0\n",
    "for line in lines:\n",
    "    if il == 12:\n",
    "        break\n",
    "    print(line)\n",
    "    il += 1\n",
    "    \n",
    "    \n",
    "with open(out_parfile_fqp, 'w') as f:\n",
    "    f.writelines(lines)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyaspect.parfile import change_parameter\n",
    "from pyaspect.parfile import change_multiple_parameters\n",
    "\n",
    "parfile_fqp = os.path.join(data_out_dir,'Par_file')\n",
    "out_parfile_fqp = os.path.join(data_out_dir,'New_Par_file')\n",
    "\n",
    "with open(parfile_fqp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "il = 0\n",
    "for line in lines:\n",
    "    if il == 313:\n",
    "        break\n",
    "    print(line)\n",
    "    il += 1\n",
    "'''\n",
    "''';\n",
    "    \n",
    "#change_parameter(parfile_fqp,lines,'SAVE_FORWARD',True,out_parfile_fqp=out_parfile_fqp)\n",
    "par_keys = ['SIMULATION_TYPE','SAVE_FORWARD','NSTEP','DT','MODEL','MIN_ATTENUATION_PERIOD','LOCAL_PATH','SAVE_MESH_FILES']\n",
    "par_vals = [4,True,8192,0.002,'default',666666666,'./BOOGER/in_my_nose',True]\n",
    "s_keys_vals = set(zip(par_keys,par_vals))\n",
    "print(f's_keys_vals:\\n{s_keys_vals}')\n",
    "print(f'd_keys_vals:\\n{[(k,v) for k,v in dict(zip(par_keys,par_vals)).items()]}')\n",
    "\n",
    "change_multiple_parameters(parfile_fqp,s_keys_vals,out_parfile_fqp=out_parfile_fqp)\n",
    "    \n",
    "\n",
    "print('\\n\\n----------------------- changed -------------------------')\n",
    "\n",
    "with open(out_parfile_fqp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "il = 0\n",
    "for line in lines:\n",
    "    if il == 313:\n",
    "        break\n",
    "    print(line)\n",
    "    il += 1\n",
    "'''\n",
    "''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!diff {parfile_fqp} {out_parfile_fqp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "out_parfile_json = os.path.join(data_out_dir,'Par_file.json')\n",
    "with open(out_parfile_json, 'w') as f:\n",
    "    json.dump(lines,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parfile_fqp = os.path.join(data_out_dir,'Par_file')\n",
    "with open(parfile_fqp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "parfile_pickle = pickle.dumps(lines, 0)\n",
    "parfile_pickle_str = parfile_pickle.decode('ascii')\n",
    "#parfile_pickle_str = str(parfile_pickle)\n",
    "\n",
    "#print(f'Parfile Type:\\n{type(parfile_pickle)}')\n",
    "print(f'Parfile String:\\n{type(parfile_pickle_str)}')\n",
    "print(f'Parfile String:\\n{parfile_pickle_str.__repr__()}')\n",
    "\n",
    "new_par_bytes = parfile_pickle_str.encode('ascii')\n",
    "new_lines = pickle.loads(new_par_bytes)\n",
    "\n",
    "'''\n",
    "print('---------- The Winding Road -----------------')\n",
    "for line in new_lines:\n",
    "    print(line)\n",
    "''';\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parfile_str = \"\"\"(lp0\\nV#-----------------------------------------------------------\\\\u000a\\np1\\naV#\\\\u000a\\np2\\naV# Simulation input parameters\\\\u000a\\np3\\naV#\\\\u000a\\np4\\naV#-----------------------------------------------------------\\\\u000a\\np5\\naV\\\\u000a\\np6\\naV# forward or adjoint simulation\\\\u000a\\np7\\naV# 1 = forward, 2 = adjoint, 3 = both simultaneously\\\\u000a\\np8\\naVSIMULATION_TYPE                 = 1\\\\u000a\\np9\\naV# 0 = earthquake simulation,  1/2/3 = three steps in noise simulation\\\\u000a\\np10\\naVNOISE_TOMOGRAPHY                = 0\\\\u000a\\np11\\naVSAVE_FORWARD                    = .false.\\\\u000a\\np12\\nag6\\naV# solve a full FWI inverse problem from a single calling program with no I/Os, storing everything in memory,\\\\u000a\\np13\\naV# or run a classical forward or adjoint problem only and save the seismograms and/or sensitivity kernels to disk (with costlier I/Os)\\\\u000a\\np14\\naVINVERSE_FWI_FULL_PROBLEM        = .false.\\\\u000a\\np15\\nag6\\naV# UTM projection parameters\\\\u000a\\np16\\naV# Use a negative zone number for the Southern hemisphere:\\\\u000a\\np17\\naV# The Northern hemisphere corresponds to zones +1 to +60,\\\\u000a\\np18\\naV# The Southern hemisphere corresponds to zones -1 to -60.\\\\u000a\\np19\\naVUTM_PROJECTION_ZONE             = 11\\\\u000a\\np20\\naVSUPPRESS_UTM_PROJECTION         = .true.\\\\u000a\\np21\\nag6\\naV# number of MPI processors\\\\u000a\\np22\\naVNPROC                           = 4 \\\\u000a\\np23\\nag6\\naV# time step parameters\\\\u000a\\np24\\naVNSTEP                           = 4096\\\\u000a\\np25\\naVDT                              = 0.001\\\\u000a\\np26\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np27\\naV#\\\\u000a\\np28\\naV# LDDRK time scheme\\\\u000a\\np29\\naV#\\\\u000a\\np30\\naV#-----------------------------------------------------------\\\\u000a\\np31\\naVUSE_LDDRK                       = .false.\\\\u000a\\np32\\naVINCREASE_CFL_FOR_LDDRK          = .false.\\\\u000a\\np33\\naVRATIO_BY_WHICH_TO_INCREASE_IT   = 1.4\\\\u000a\\np34\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np35\\naV#\\\\u000a\\np36\\naV# Mesh\\\\u000a\\np37\\naV#\\\\u000a\\np38\\naV#-----------------------------------------------------------\\\\u000a\\np39\\nag6\\naV# Number of nodes for 2D and 3D shape functions for hexahedra.\\\\u000a\\np40\\naV# We use either 8-node mesh elements (bricks) or 27-node elements.\\\\u000a\\np41\\naV# If you use our internal mesher, the only option is 8-node bricks (27-node elements are not supported).\\\\u000a\\np42\\naVNGNOD                           = 8\\\\u000a\\np43\\nag6\\naV# models:\\\\u000a\\np44\\naV# available options are:\\\\u000a\\np45\\naV#   default (model parameters described by mesh properties)\\\\u000a\\np46\\naV# 1D models available are:\\\\u000a\\np47\\naV#   1d_prem,1d_socal,1d_cascadia\\\\u000a\\np48\\naV# 3D models available are:\\\\u000a\\np49\\naV#   aniso,external,gll,salton_trough,tomo,SEP,coupled,...\\\\u000a\\np50\\naVMODEL                           = gll\\\\u000a\\np51\\nag6\\naV# path for external tomographic models files\\\\u000a\\np52\\naVTOMOGRAPHY_PATH                 = ./DATA/tomo_files/\\\\u000a\\np53\\naV# if you are using a SEP model (oil-industry format)\\\\u000a\\np54\\naVSEP_MODEL_DIRECTORY             = ./DATA/my_SEP_model/\\\\u000a\\np55\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np56\\nag6\\naV# parameters describing the model\\\\u000a\\np57\\naVAPPROXIMATE_OCEAN_LOAD          = .false.\\\\u000a\\np58\\naVTOPOGRAPHY                      = .false.\\\\u000a\\np59\\naVATTENUATION                     = .false.\\\\u000a\\np60\\naVANISOTROPY                      = .false.\\\\u000a\\np61\\naVGRAVITY                         = .false.\\\\u000a\\np62\\nag6\\naV# in case of attenuation, reference frequency in Hz at which the velocity values in the velocity model are given (unused otherwise)\\\\u000a\\np63\\naVATTENUATION_f0_REFERENCE        = 18.d0\\\\u000a\\np64\\nag6\\naV# attenuation period range over which we try to mimic a constant Q factor\\\\u000a\\np65\\naVMIN_ATTENUATION_PERIOD          = 999999998.d0\\\\u000a\\np66\\naVMAX_ATTENUATION_PERIOD          = 999999999.d0\\\\u000a\\np67\\naV# ignore this range and ask the code to compute it automatically instead based on the estimated resolution of the mesh (use this unless you know what you are doing)\\\\u000a\\np68\\naVCOMPUTE_FREQ_BAND_AUTOMATIC     = .true.\\\\u000a\\np69\\nag6\\naV# Olsen\\'s constant for Q_mu = constant * V_s attenuation rule\\\\u000a\\np70\\naVUSE_OLSEN_ATTENUATION           = .false.\\\\u000a\\np71\\naVOLSEN_ATTENUATION_RATIO         = 0.05\\\\u000a\\np72\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np73\\naV#\\\\u000a\\np74\\naV# Absorbing boundary conditions\\\\u000a\\np75\\naV#\\\\u000a\\np76\\naV#-----------------------------------------------------------\\\\u000a\\np77\\nag6\\naV# C-PML boundary conditions for a regional simulation\\\\u000a\\np78\\naV# (if set to .false., and STACEY_ABSORBING_CONDITIONS is also set to .false., you get a free surface instead\\\\u000a\\np79\\naV# in the case of elastic or viscoelastic mesh elements, and a rigid surface in the case of acoustic (fluid) elements\\\\u000a\\np80\\naVPML_CONDITIONS                  = .false.\\\\u000a\\np81\\nag6\\naV# C-PML top surface\\\\u000a\\np82\\naVPML_INSTEAD_OF_FREE_SURFACE     = .false.\\\\u000a\\np83\\nag6\\naV# C-PML dominant frequency\\\\u000a\\np84\\naVf0_FOR_PML                      = 0.05555\\\\u000a\\np85\\nag6\\naV# parameters used to rotate C-PML boundary conditions by a given angle (not completed yet)\\\\u000a\\np86\\naV# ROTATE_PML_ACTIVATE           = .false.\\\\u000a\\np87\\naV# ROTATE_PML_ANGLE              = 0.\\\\u000a\\np88\\nag6\\naV# absorbing boundary conditions for a regional simulation\\\\u000a\\np89\\naV# (if set to .false., and PML_CONDITIONS is also set to .false., you get a free surface instead\\\\u000a\\np90\\naV# in the case of elastic or viscoelastic mesh elements, and a rigid surface in the case of acoustic (fluid) elements\\\\u000a\\np91\\naVSTACEY_ABSORBING_CONDITIONS     = .true.\\\\u000a\\np92\\nag6\\naV# absorbing top surface (defined in mesh as \\'free_surface_file\\')\\\\u000a\\np93\\naVSTACEY_INSTEAD_OF_FREE_SURFACE  = .false.\\\\u000a\\np94\\nag6\\naV# When STACEY_ABSORBING_CONDITIONS is set to .true. :\\\\u000a\\np95\\naV# absorbing conditions are defined in xmin, xmax, ymin, ymax and zmin\\\\u000a\\np96\\naV# this option BOTTOM_FREE_SURFACE can be set to .true. to\\\\u000a\\np97\\naV# make zmin free surface instead of absorbing condition\\\\u000a\\np98\\naVBOTTOM_FREE_SURFACE             = .false.\\\\u000a\\np99\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np100\\naV#\\\\u000a\\np101\\naV# undoing attenuation and/or PMLs for sensitivity kernel calculations\\\\u000a\\np102\\naV#\\\\u000a\\np103\\naV#-----------------------------------------------------------\\\\u000a\\np104\\nag6\\naV# to undo attenuation and/or PMLs for sensitivity kernel calculations or forward runs with SAVE_FORWARD\\\\u000a\\np105\\naV# use the flag below. It performs undoing of attenuation and/or of PMLs in an exact way for sensitivity kernel calculations\\\\u000a\\np106\\naV# but requires disk space for temporary storage, and uses a significant amount of memory used as buffers for temporary storage.\\\\u000a\\np107\\naV# When that option is on the second parameter indicates how often the code dumps restart files to disk (if in doubt, use something between 100 and 1000).\\\\u000a\\np108\\naVUNDO_ATTENUATION_AND_OR_PML     = .false.\\\\u000a\\np109\\naVNT_DUMP_ATTENUATION             = 500\\\\u000a\\np110\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np111\\naV#\\\\u000a\\np112\\naV# Visualization\\\\u000a\\np113\\naV#\\\\u000a\\np114\\naV#-----------------------------------------------------------\\\\u000a\\np115\\nag6\\naV# save AVS or OpenDX movies\\\\u000a\\np116\\naV# MOVIE_TYPE = 1 to show the top surface\\\\u000a\\np117\\naV# MOVIE_TYPE = 2 to show all the external faces of the mesh\\\\u000a\\np118\\naVCREATE_SHAKEMAP                 = .false.\\\\u000a\\np119\\naVMOVIE_SURFACE                   = .false.\\\\u000a\\np120\\naVMOVIE_TYPE                      = 2\\\\u000a\\np121\\naVMOVIE_VOLUME                    = .false.\\\\u000a\\np122\\naVSAVE_DISPLACEMENT               = .true.\\\\u000a\\np123\\naVUSE_HIGHRES_FOR_MOVIES          = .false.\\\\u000a\\np124\\naVNTSTEP_BETWEEN_FRAMES           = 500\\\\u000a\\np125\\naVHDUR_MOVIE                      = 0.0\\\\u000a\\np126\\nag6\\naV# save AVS or OpenDX mesh files to check the mesh\\\\u000a\\np127\\naVSAVE_MESH_FILES                 = .false.\\\\u000a\\np128\\nag6\\naV# path to store the local database file on each node\\\\u000a\\np129\\naVLOCAL_PATH                      = ./OUTPUT_FILES/DATABASES_MPI\\\\u000a\\np130\\naV#LOCAL_PATH                      = ./COMMON/DATABASES_MPI\\\\u000a\\np131\\nag6\\naV# interval at which we output time step info and max of norm of displacement\\\\u000a\\np132\\naVNTSTEP_BETWEEN_OUTPUT_INFO      = 1000\\\\u000a\\np133\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np134\\naV#\\\\u000a\\np135\\naV# Sources\\\\u000a\\np136\\naV#\\\\u000a\\np137\\naV#-----------------------------------------------------------\\\\u000a\\np138\\nag6\\naV# sources and receivers Z coordinates given directly (i.e. as their true position) instead of as their depth\\\\u000a\\np139\\naVUSE_SOURCES_RECEIVERS_Z         = .false.\\\\u000a\\np140\\nag6\\naV# use a (tilted) FORCESOLUTION force point source (or several) instead of a CMTSOLUTION moment-tensor source.\\\\u000a\\np141\\naV# This can be useful e.g. for oil industry foothills simulations or asteroid simulations\\\\u000a\\np142\\naV# in which the source is a vertical force, normal force, tilted force, impact etc.\\\\u000a\\np143\\naV# If this flag is turned on, the FORCESOLUTION file must be edited by giving:\\\\u000a\\np144\\naV# - the corresponding time-shift parameter,\\\\u000a\\np145\\naV# - the half duration parameter of the source,\\\\u000a\\np146\\naV# - the coordinates of the source,\\\\u000a\\np147\\naV# - the magnitude of the force source,\\\\u000a\\np148\\naV# - the components of a (non necessarily unitary) direction vector for the force source in the E/N/Z_UP basis.\\\\u000a\\np149\\naV# The direction vector is made unitary internally in the code and thus only its direction matters here;\\\\u000a\\np150\\naV# its norm is ignored and the norm of the force used is the factor force source times the source time function.\\\\u000a\\np151\\naVUSE_FORCE_POINT_SOURCE          = .true.\\\\u000a\\np152\\nag6\\naV# set to true to use a Ricker source time function instead of the source time functions set by default\\\\u000a\\np153\\naV# to represent a (tilted) FORCESOLUTION force point source or a CMTSOLUTION moment-tensor source.\\\\u000a\\np154\\naVUSE_RICKER_TIME_FUNCTION        = .false.\\\\u000a\\np155\\nag6\\naV# Use an external source time function.\\\\u000a\\np156\\naV# if .true. you must add a file with your source time function and the file name\\\\u000a\\np157\\naV# path relative to lauching directory at the end of CMTSOLUTION or FORCESOURCE file\\\\u000a\\np158\\naV# (with multiple sources, one file per source is required).\\\\u000a\\np159\\naV# This file must have a single column containing the amplitude of the source at that time step,\\\\u000a\\np160\\naV# and on its first line it must contain the time step used, which must be equal to DT as defined at the beginning of this Par_file (the code will check that).\\\\u000a\\np161\\naV# Be sure when this option is .false. to remove the name of stf file in CMTSOLUTION or FORCESOURCE\\\\u000a\\np162\\naVUSE_EXTERNAL_SOURCE_FILE        = .false.\\\\u000a\\np163\\nag6\\naV# print source time function\\\\u000a\\np164\\naVPRINT_SOURCE_TIME_FUNCTION      = .true.\\\\u000a\\np165\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np166\\naV#\\\\u000a\\np167\\naV# Seismograms\\\\u000a\\np168\\naV#\\\\u000a\\np169\\naV#-----------------------------------------------------------\\\\u000a\\np170\\nag6\\naV# interval in time steps for writing of seismograms\\\\u000a\\np171\\naVNTSTEP_BETWEEN_OUTPUT_SEISMOS   = 4096\\\\u000a\\np172\\nag6\\naV# decide if we save displacement, velocity, acceleration and/or pressure in forward runs (they can be set to true simultaneously)\\\\u000a\\np173\\naV# currently pressure seismograms are implemented in acoustic (i.e. fluid) elements only\\\\u000a\\np174\\naVSAVE_SEISMOGRAMS_DISPLACEMENT   = .true.\\\\u000a\\np175\\naVSAVE_SEISMOGRAMS_VELOCITY       = .false.\\\\u000a\\np176\\naVSAVE_SEISMOGRAMS_ACCELERATION   = .false.\\\\u000a\\np177\\naVSAVE_SEISMOGRAMS_PRESSURE       = .false.   # currently implemented in acoustic (i.e. fluid) elements only\\\\u000a\\np178\\nag6\\naV# save seismograms also when running the adjoint runs for an inverse problem\\\\u000a\\np179\\naV# (usually they are unused and not very meaningful, leave this off in almost all cases)\\\\u000a\\np180\\naVSAVE_SEISMOGRAMS_IN_ADJOINT_RUN = .false.\\\\u000a\\np181\\nag6\\naV# save seismograms in binary or ASCII format (binary is smaller but may not be portable between machines)\\\\u000a\\np182\\naVUSE_BINARY_FOR_SEISMOGRAMS      = .false.\\\\u000a\\np183\\nag6\\naV# output seismograms in Seismic Unix format (binary with 240-byte-headers)\\\\u000a\\np184\\naVSU_FORMAT                       = .false.\\\\u000a\\np185\\nag6\\naV# output seismograms in ASDF (requires asdf-library)\\\\u000a\\np186\\naVASDF_FORMAT                     = .false.\\\\u000a\\np187\\nag6\\naV# decide if master process writes all the seismograms or if all processes do it in parallel\\\\u000a\\np188\\naVWRITE_SEISMOGRAMS_BY_MASTER     = .false.\\\\u000a\\np189\\nag6\\naV# save all seismograms in one large combined file instead of one file per seismogram\\\\u000a\\np190\\naV# to avoid overloading shared non-local file systems such as LUSTRE or GPFS for instance\\\\u000a\\np191\\naVSAVE_ALL_SEISMOS_IN_ONE_FILE    = .false.\\\\u000a\\np192\\nag6\\naV# use a trick to increase accuracy of pressure seismograms in fluid (acoustic) elements:\\\\u000a\\np193\\naV# use the second derivative of the source for the source time function instead of the source itself,\\\\u000a\\np194\\naV# and then record -potential_acoustic() as pressure seismograms instead of -potential_dot_dot_acoustic();\\\\u000a\\np195\\naV# this is mathematically equivalent, but numerically significantly more accurate because in the explicit\\\\u000a\\np196\\naV# Newmark time scheme acceleration is accurate at zeroth order while displacement is accurate at second order,\\\\u000a\\np197\\naV# thus in fluid elements potential_dot_dot_acoustic() is accurate at zeroth order while potential_acoustic()\\\\u000a\\np198\\naV# is accurate at second order and thus contains significantly less numerical noise.\\\\u000a\\np199\\naVUSE_TRICK_FOR_BETTER_PRESSURE   = .false.\\\\u000a\\np200\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np201\\naV#\\\\u000a\\np202\\naV# Source encoding\\\\u000a\\np203\\naV#\\\\u000a\\np204\\naV#-----------------------------------------------------------\\\\u000a\\np205\\nag6\\naV# (for acoustic simulations only for now) determines source encoding factor +/-1 depending on sign of moment tensor\\\\u000a\\np206\\naV# (see e.g. Krebs et al., 2009. Fast full-wavefield seismic inversion using encoded sources, Geophysics, 74 (6), WCC177-WCC188.)\\\\u000a\\np207\\naVUSE_SOURCE_ENCODING             = .false.\\\\u000a\\np208\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np209\\naV#\\\\u000a\\np210\\naV# Energy calculation\\\\u000a\\np211\\naV#\\\\u000a\\np212\\naV#-----------------------------------------------------------\\\\u000a\\np213\\nag6\\naV# to plot energy curves, for instance to monitor how CPML absorbing layers behave;\\\\u000a\\np214\\naV# should be turned OFF in most cases because a bit expensive\\\\u000a\\np215\\naVOUTPUT_ENERGY                   = .false.\\\\u000a\\np216\\naV# every how many time steps we compute energy (which is a bit expensive to compute)\\\\u000a\\np217\\naVNTSTEP_BETWEEN_OUTPUT_ENERGY    = 10\\\\u000a\\np218\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np219\\naV#\\\\u000a\\np220\\naV# Adjoint kernel outputs\\\\u000a\\np221\\naV#\\\\u000a\\np222\\naV#-----------------------------------------------------------\\\\u000a\\np223\\nag6\\naV# interval in time steps for reading adjoint traces\\\\u000a\\np224\\naV# 0 = read the whole adjoint sources at the same time\\\\u000a\\np225\\naVNTSTEP_BETWEEN_READ_ADJSRC      = 0\\\\u000a\\np226\\nag6\\naV# read adjoint sources using ASDF (requires asdf-library)\\\\u000a\\np227\\naVREAD_ADJSRC_ASDF                = .false.\\\\u000a\\np228\\nag6\\naV# this parameter must be set to .true. to compute anisotropic kernels\\\\u000a\\np229\\naV# in crust and mantle (related to the 21 Cij in geographical coordinates)\\\\u000a\\np230\\naV# default is .false. to compute isotropic kernels (related to alpha and beta)\\\\u000a\\np231\\naVANISOTROPIC_KL                  = .false.\\\\u000a\\np232\\nag6\\naV# compute transverse isotropic kernels (alpha_v,alpha_h,beta_v,beta_h,eta,rho)\\\\u000a\\np233\\naV# rather than fully anisotropic kernels in case ANISOTROPIC_KL is set to .true.\\\\u000a\\np234\\naVSAVE_TRANSVERSE_KL              = .false.\\\\u000a\\np235\\nag6\\naV# this parameter must be set to .true. to compute anisotropic kernels for\\\\u000a\\np236\\naV# cost function using velocity observable rather than displacement\\\\u000a\\np237\\naVANISOTROPIC_VELOCITY_KL         = .false.\\\\u000a\\np238\\nag6\\naV# outputs approximate Hessian for preconditioning\\\\u000a\\np239\\naV#APPROXIMATE_HESS_KL             = .false. ##per Ebru do use for\\\\u000a\\np240\\naV#preconditioning\\\\u000a\\np241\\naVAPPROXIMATE_HESS_KL             = .true.\\\\u000a\\np242\\nag6\\naV# save Moho mesh and compute Moho boundary kernels\\\\u000a\\np243\\naVSAVE_MOHO_MESH                  = .false.\\\\u000a\\np244\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np245\\naV#\\\\u000a\\np246\\naV# Coupling with an injection technique (DSM, AxiSEM, or FK)\\\\u000a\\np247\\naV#\\\\u000a\\np248\\naV#-----------------------------------------------------------\\\\u000a\\np249\\nag6\\naVCOUPLE_WITH_INJECTION_TECHNIQUE = .false.\\\\u000a\\np250\\naVINJECTION_TECHNIQUE_TYPE        = 3   # 1 = DSM, 2 = AxiSEM, 3 = FK\\\\u000a\\np251\\naVMESH_A_CHUNK_OF_THE_EARTH       = .false.\\\\u000a\\np252\\naVTRACTION_PATH                   = ./DATA/AxiSEM_tractions/3/\\\\u000a\\np253\\naVFKMODEL_FILE                    = FKmodel\\\\u000a\\np254\\naVRECIPROCITY_AND_KH_INTEGRAL     = .false.   # does not work yet\\\\u000a\\np255\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np256\\nag6\\naV# Dimitri Komatitsch, July 2014, CNRS Marseille, France:\\\\u000a\\np257\\naV# added the ability to run several calculations (several earthquakes)\\\\u000a\\np258\\naV# in an embarrassingly-parallel fashion from within the same run;\\\\u000a\\np259\\naV# this can be useful when using a very large supercomputer to compute\\\\u000a\\np260\\naV# many earthquakes in a catalog, in which case it can be better from\\\\u000a\\np261\\naV# a batch job submission point of view to start fewer and much larger jobs,\\\\u000a\\np262\\naV# each of them computing several earthquakes in parallel.\\\\u000a\\np263\\naV# To turn that option on, set parameter NUMBER_OF_SIMULTANEOUS_RUNS to a value greater than 1.\\\\u000a\\np264\\naV# To implement that, we create NUMBER_OF_SIMULTANEOUS_RUNS MPI sub-communicators,\\\\u000a\\np265\\naV# each of them being labeled \"my_local_mpi_comm_world\", and we use them\\\\u000a\\np266\\naV# in all the routines in \"src/shared/parallel.f90\", except in MPI_ABORT() because in that case\\\\u000a\\np267\\naV# we need to kill the entire run.\\\\u000a\\np268\\naV# When that option is on, of course the number of processor cores used to start\\\\u000a\\np269\\naV# the code in the batch system must be a multiple of NUMBER_OF_SIMULTANEOUS_RUNS,\\\\u000a\\np270\\naV# all the individual runs must use the same number of processor cores,\\\\u000a\\np271\\naV# which as usual is NPROC in the Par_file,\\\\u000a\\np272\\naV# and thus the total number of processor cores to request from the batch system\\\\u000a\\np273\\naV# should be NUMBER_OF_SIMULTANEOUS_RUNS * NPROC.\\\\u000a\\np274\\naV# All the runs to perform must be placed in directories called run0001, run0002, run0003 and so on\\\\u000a\\np275\\naV# (with exactly four digits).\\\\u000a\\np276\\naV#\\\\u000a\\np277\\naV# Imagine you have 10 independent calculations to do, each of them on 100 cores; you have three options:\\\\u000a\\np278\\naV#\\\\u000a\\np279\\naV# 1/ submit 10 jobs to the batch system\\\\u000a\\np280\\naV#\\\\u000a\\np281\\naV# 2/ submit a single job on 1000 cores to the batch, and in that script create a sub-array of jobs to start 10 jobs,\\\\u000a\\np282\\naV# each running on 100 cores (see e.g. http://www.schedmd.com/slurmdocs/job_array.html )\\\\u000a\\np283\\naV#\\\\u000a\\np284\\naV# 3/ submit a single job on 1000 cores to the batch, start SPECFEM3D on 1000 cores, create 10 sub-communicators,\\\\u000a\\np285\\naV# cd into one of 10 subdirectories (called e.g. run0001, run0002,... run0010) depending on the sub-communicator\\\\u000a\\np286\\naV# your MPI rank belongs to, and run normally on 100 cores using that sub-communicator.\\\\u000a\\np287\\naV#\\\\u000a\\np288\\naV# The option below implements 3/.\\\\u000a\\np289\\naV#\\\\u000a\\np290\\naVNUMBER_OF_SIMULTANEOUS_RUNS     = 1\\\\u000a\\np291\\nag6\\naV# if we perform simultaneous runs in parallel, if only the source and receivers vary between these runs\\\\u000a\\np292\\naV# but not the mesh nor the model (velocity and density) then we can also read the mesh and model files\\\\u000a\\np293\\naV# from a single run in the beginning and broadcast them to all the others; for a large number of simultaneous\\\\u000a\\np294\\naV# runs for instance when solving inverse problems iteratively this can DRASTICALLY reduce I/Os to disk in the solver\\\\u000a\\np295\\naV# (by a factor equal to NUMBER_OF_SIMULTANEOUS_RUNS), and reducing I/Os is crucial in the case of huge runs.\\\\u000a\\np296\\naV# Thus, always set this option to .true. if the mesh and the model are the same for all simultaneous runs.\\\\u000a\\np297\\naV# In that case there is no need to duplicate the mesh and model file database (the content of the DATABASES_MPI\\\\u000a\\np298\\naV# directories) in each of the run0001, run0002,... directories, it is sufficient to have one in run0001\\\\u000a\\np299\\naV# and the code will broadcast it to the others)\\\\u000a\\np300\\naVBROADCAST_SAME_MESH_AND_MODEL   = .true.\\\\u000a\\np301\\nag6\\naV#-----------------------------------------------------------\\\\u000a\\np302\\nag6\\naV# set to true to use GPUs\\\\u000a\\np303\\naVGPU_MODE                        = .true.\\\\u000a\\np304\\nag6\\naV# ADIOS Options for I/Os\\\\u000a\\np305\\naVADIOS_ENABLED                   = .false.\\\\u000a\\np306\\naVADIOS_FOR_DATABASES             = .false.\\\\u000a\\np307\\naVADIOS_FOR_MESH                  = .false.\\\\u000a\\np308\\naVADIOS_FOR_FORWARD_ARRAYS        = .false.\\\\u000a\\np309\\naVADIOS_FOR_KERNELS               = .false.\\\\u000a\\np310\\nag6\\naV#**********************\\\\u000a\\np311\\naV#Serial mesh decomposer\\\\u000a\\np312\\naV#**********************\\\\u000a\\np313\\nag6\\naVLTS_MODE                        = .false.\\\\u000a\\np314\\nag6\\naVPARTITIONING_TYPE               = 1\\\\u000a\\np315\\na.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_parfile_str == parfile_pickle_str)\n",
    "#print(set(test_parfile_str.split()).difference(set(parfile_pickle_str.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = goobleydoobleydoo('42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sillyfunc(index=None):\n",
    "    if index == None:\n",
    "        print('none')\n",
    "    else:\n",
    "        print(f'index={index}')\n",
    "        \n",
    "sillyfunc(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = []\n",
    "for i in range(6):\n",
    "    df_rec.append({'sid':i%3,'eid':0})\n",
    "df = pd.DataFrame.from_records(df_rec)\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(df['sid'])\n",
    "print()\n",
    "s = set(df['sid'])\n",
    "ss = set(df['sid'])\n",
    "print(s)\n",
    "print(f's == ss: {s == ss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fqp = os.path.join(data_out_dir,'tmp')\n",
    "!ls {tmp_fqp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "for src_path in glob(data_out_dir):\n",
    "    print(src_path)\n",
    "    print(os.path.relpath(src_path, 'tmp'))\n",
    "    '''\n",
    "    os.symlink(\n",
    "        os.path.relpath(\n",
    "            src_path,\n",
    "            'dst/'\n",
    "        ),\n",
    "        os.path.join('dst', os.path.basename(src_path))\n",
    "    )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = os.path.relpath('tmp', start=os.curdir)\n",
    "p = os.path.relpath(tmp_fqp, start='data/')\n",
    "fqpname = os.path.join(p, 'tmp')\n",
    "\n",
    "print(p)\n",
    "print(fqpname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpath = 'data/output/tmp/TestProjects/FirstTestProject/run0001/DATA'\n",
    "os.symlink('STAIONS.sid0','data/output/tmp/TestProjects/FirstTestProject/run0001/DATA/LINKSTATIONS')\n",
    "!ls -ltrh {tpath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
