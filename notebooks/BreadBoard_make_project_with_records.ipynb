{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA_random_eqrthquake_locations_w_pyvista_vtk\n",
    "\n",
    "Testing the random picking of subsurface eqrthquake locations (moment-tensor locations) and 3D plotting of the picks and model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all packages\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sys import argv\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from pyaspect.model.gridmod3d import gridmod3d as gm\n",
    "from pyaspect.model.bbox import bbox as bb\n",
    "from pyaspect.model.gm3d_utils import compress_gm3d_to_file\n",
    "from pyaspect.model.gm3d_utils import decompress_gm3d_from_file\n",
    "from pyaspect.moment_tensor import MomentTensor\n",
    "from pyaspect.specfemio.headers import StationHeader\n",
    "from pyaspect.specfemio.headers import SolutionHeader\n",
    "from pyaspect.specfemio.headers import CMTSolutionHeader\n",
    "from pyaspect.specfemio.headers import ForceSolutionHeader\n",
    "from pyaspect.specfemio.write import write_cmtsolution\n",
    "from pyaspect.specfemio.write import write_forcesolution\n",
    "from pyaspect.specfemio.write import write_grouped_forcesolutions\n",
    "from pyaspect.specfemio.write import write_stations\n",
    "from pyaspect.specfemio.read import read_stations\n",
    "from pyaspect.specfemio.read import read_solution\n",
    "from pyaspect.specfemio.read import read_cmtsolution\n",
    "from pyaspect.specfemio.read import read_forcesolution\n",
    "from pyaspect.specfemio.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "Extract the ndarray of the subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_dir  = 'data/output/'\n",
    "data_out_dir = data_in_dir\n",
    "!ls {data_in_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 \n",
    "\n",
    "Decompress the ndarray of the sliced, subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename then used it to decompress model\n",
    "ifqn = f'{data_out_dir}/vsliced_subsmp_smth_nam_2017_vp_vs_rho_Q_model_dx100_dy100_dz100_maxdepth5850_sig250.npz'\n",
    "vslice_gm3d, other_pars = decompress_gm3d_from_file(ifqn)\n",
    "\n",
    "print()\n",
    "print('decompressed gridded model\\n:',vslice_gm3d) \n",
    "print()\n",
    "print('other parameters:\\n',other_pars)\n",
    "print()\n",
    "\n",
    "# WARNING: this will unpack all other_pars, if you overwrite a variable of the samename as val(key), then you \n",
    "#          may not notice, and this may cause large headaches.  I use it because I am aware of it.\n",
    "'''\n",
    "for key in other_pars:\n",
    "    locals()[key] = other_pars[key]  #this is more advanced python than I think is reasonable for most \n",
    "sig_meters = sig\n",
    "''';\n",
    "\n",
    "# another way to get these varibles is just use the accessor functions for the gridmod3d.  We need them later.\n",
    "xmin = other_pars['xmin']\n",
    "dx   = other_pars['dx']\n",
    "nx   = other_pars['nx']\n",
    "ymin = other_pars['ymin']\n",
    "dy   = other_pars['dy']\n",
    "ny   = other_pars['ny']\n",
    "zmin = other_pars['zmin']\n",
    "dz   = other_pars['dz']\n",
    "nz   = other_pars['nz']\n",
    "sig_meters = other_pars['sig']  # this variable is used later\n",
    "print('sig_meters:',sig_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "nam_dims = list(vslice_gm3d.get_npoints())\n",
    "nam_origin = list(vslice_gm3d.get_gorigin())\n",
    "nam_origin[2] *= -1\n",
    "nam_origin = tuple(nam_origin)\n",
    "nam_spacing = list(vslice_gm3d.get_deltas())\n",
    "nam_spacing[2] *=-1\n",
    "nam_spacing = tuple(nam_spacing)\n",
    "print('nam_dims:',nam_dims)\n",
    "print('nam_origin:',nam_origin)\n",
    "print('nam_spacing:',nam_spacing)\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.dimensions = np.array(nam_dims) + 1\n",
    "grid.origin = nam_origin  # The bottom left corner of the data set\n",
    "grid.spacing = nam_spacing  # These are the cell sizes along each axis\n",
    "nam_pvalues = vslice_gm3d.getNPArray()[0]\n",
    "print('pvalues.shape:',nam_pvalues.shape)\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_arrays[\"values\"] = nam_pvalues.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "# Now plot the grid!\n",
    "cmap = plt.cm.jet\n",
    "#grid.plot(show_edges=True,cmap=cmap)\n",
    "grid.plot(cmap=cmap,opacity=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = grid.slice_orthogonal()\n",
    "\n",
    "#slices.plot(show_edges=True,cmap=cmap)\n",
    "slices.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create random virtual source (to specfem stations, but using reciprocity -- sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 1000\n",
    "\n",
    "lrx = np.min(xc)\n",
    "lry = np.min(yc)\n",
    "lrz = -4300.0\n",
    "\n",
    "hrx = np.max(xc)\n",
    "hry = np.max(yc)\n",
    "hrz = -3000.0\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "srz = hrz - lrz\n",
    "\n",
    "r_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    rz = lrz + srz*np.random.rand()\n",
    "    r_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "r_xyz = np.array(r_xyz_list)\n",
    "    \n",
    "\n",
    "#r_xyz = np.vstack(np.meshgrid(rx,ry,rz)).reshape(3,-1).T\n",
    "print('r_xyz:\\n',r_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir {data_out_dir}/tmp\n",
    "!ls {data_out_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make cross (half or full) group for calculating spacial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'l_grp_stations' in locals() or 'l_grp_stations' in globals():\n",
    "    print('deleting')\n",
    "    del l_grp_stations\n",
    "    \n",
    "# make a list of station headers for each random location\n",
    "l_stations = []\n",
    "for i in range(len(r_xyz)):\n",
    "    \n",
    "    name = 't' + str(i).zfill(len(str(len(r_xyz))))\n",
    "    new_s = StationHeader(name=name,\n",
    "                          network='NL',\n",
    "                          lat_yc=r_xyz[i,1],\n",
    "                          lon_xc=r_xyz[i,0],\n",
    "                          elevation=0.0,\n",
    "                          depth=r_xyz[i,2],\n",
    "                          trid=i)\n",
    "    l_stations.append(new_s)\n",
    "#print('len(l_stats):',len(l_stations))\n",
    "                                           \n",
    "# make the group membors for each station above\n",
    "# function below returns a list[list[]] like structure\n",
    "l_grp_stations = make_grouped_half_cross_station_headers(stations=l_stations,delta=250.0)\n",
    "\n",
    "# this is a list[] structure with unique stations (small chance a group member has same coordinate)\n",
    "s_grp_stations = sorted(copy.deepcopy(flatten_grouped_headers_unique(l_grp_stations)))\n",
    "\n",
    "# if lengths are the same then all group members are unique\n",
    "print(f'len(l_grp): {len(flatten_grouped_headers(l_grp_stations))}')\n",
    "print(f'len(s_grp): {len(s_grp_stations)}')\n",
    "\n",
    "#flatten the l_grp_stations (NOT guarantied unique!)\n",
    "l_grp_stations = sorted(flatten_grouped_headers(l_grp_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_stations(data_out_dir + '/tmp',l_grp_stations,fname='STATIONS_GROUPED')\n",
    "\n",
    "# NOT grouped -> list[] instead of list[list[]]\n",
    "write_stations(data_out_dir + '/tmp',l_grp_stations)\n",
    "\n",
    "# For records only!\n",
    "#write_stations(data_out_dir + '/tmp',l_grp_stations,write_h=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh {data_out_dir + '/tmp'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail {data_out_dir}/tmp/STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqp = data_out_dir + '/tmp'\n",
    "rw_stations = sorted(read_stations(fqp))\n",
    "\n",
    "print('stations equal?:', rw_stations == l_grp_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(rw_stations)\n",
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make random virtual recievers locations (solutions/sources in specfem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "n_rand_p = 5\n",
    "rz = 150\n",
    "\n",
    "lrx = np.min(xc)\n",
    "lry = np.min(yc)\n",
    "\n",
    "hrx = np.max(xc)\n",
    "hry = np.max(yc)\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "\n",
    "s_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    s_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "s_xyz = np.array(s_xyz_list)\n",
    "    \n",
    "\n",
    "print('s_xyz:\\n',s_xyz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make force solution headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of station headers for each random location\n",
    "l_solutions = []\n",
    "for i in range(len(s_xyz)):\n",
    "    \n",
    "    name = 't' + str(i).zfill(len(str(len(r_xyz))))\n",
    "    new_s = StationHeader(name=name,\n",
    "                          network='NL',\n",
    "                          lat_yc=r_xyz[i,1],\n",
    "                          lon_xc=r_xyz[i,0],\n",
    "                          elevation=0.0,\n",
    "                          depth=r_xyz[i,2],\n",
    "                          trid=i)\n",
    "    new_s = ForceSolutionHeader(ename=f'Event-{str(i).zfill(4)}',\n",
    "                                lat_yc=s_xyz[i,1],\n",
    "                                lon_xc=s_xyz[i,0],\n",
    "                                depth=s_xyz[i,2],\n",
    "                                tshift=0.0,\n",
    "                                date=datetime.datetime.now(),\n",
    "                                f0=0.0,\n",
    "                                factor_fs=1,\n",
    "                                comp_src_EX=1,\n",
    "                                comp_src_NY=0,\n",
    "                                comp_src_Zup=0,\n",
    "                                proj_id=0,\n",
    "                                eid=i,\n",
    "                                sid=0)\n",
    "    l_solutions.append(new_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make solution group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_grp_solutions_h = make_grouped_triplet_force_solution_headers(solutions=l_solutions)\n",
    "grp_s_xyz = get_xyz_coords_from_solution_list(flatten_grouped_headers(l_grp_solutions_h))\n",
    "\n",
    "for s in flatten_grouped_headers(l_grp_solutions_h):\n",
    "    print(f'solution:\\n{s}')\n",
    "    print()\n",
    "print(f'coords:\\n{grp_s_xyz}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    p = f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}'\n",
    "    #print(p)\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_out_dir}/tmp\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_s in l_grp_solutions_h:\n",
    "    write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s)\n",
    "    #write_grouped_forcesolutions(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}',grp_s,write_h=False)\n",
    "    #print(f'{data_out_dir}/tmp/run{str(grp_s[0].eid+1).zfill(4)}')\n",
    "    #for s in grp_s:\n",
    "        #print(f's:\\n{s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -lrth {data_out_dir}/tmp/run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = goobledoobleydoo('42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh {data_out_dir + '/tmp/run*'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat {data_out_dir}/tmp/CMTSOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat {data_out_dir}/tmp/FORCESOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqp = f'{data_out_dir}/tmp'\n",
    "read_gen_cmt = read_solution(fqp,'CMTSOLUTION')\n",
    "read_gen_fs  = read_solution(fqp,'FORCESOLUTION')\n",
    "read_cmt = read_cmtsolution(fqp,fname='CMTSOLUTION')\n",
    "read_fs  = read_forcesolution(fqp,fname='FORCESOLUTION')\n",
    "    \n",
    "print('gen cmt:\\n',read_gen_cmt)\n",
    "print()\n",
    "print('gen fs:\\n',read_gen_fs)\n",
    "print()\n",
    "print('cmt:\\n',read_cmt)\n",
    "print()\n",
    "print('fs:\\n',read_fs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(rw_stations)\n",
    "exc_g_xyz = get_xyz_coords_from_station_list_except(rw_stations,key='gid',val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_exc_points = pv.wrap(exc_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_exc_points, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.75)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "class StationTrace(object):\n",
    "    \n",
    "    def __init__(self, header=None, data=None, start_t=0, dt=1):\n",
    "        \n",
    "        if not isinstance(header,StationHeader):\n",
    "            raise Exception('arg: \\'header\\' must be of type StationHeader')\n",
    "            \n",
    "        self.header  = copy.deepcopy(header)\n",
    "        self.data    = np.array(data,dtype=np.float32)\n",
    "        self.start_t = np.float32(start_t)\n",
    "        self.dt      = np.float32(dt)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, islice):\n",
    "        return self.data[islice]\n",
    "    \n",
    "    def __setitem__(self, islice, value):\n",
    "        self.data[islice] = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        out_str  = f'Station Header:\\n{self.header}\\n'\n",
    "        out_str += f'Data:\\n{self.data}'\n",
    "        return out_str\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out_str  = f'Station Header:\\n{self.header.__repr__()}\\n'\n",
    "        out_str += f'Data:\\n{self.data.__repr__()}'\n",
    "        return out_str\n",
    "    \n",
    "    def times(self):\n",
    "        return self.start_t + np.arange(len(self.data),dtype=np.float32)*self.dt\n",
    "        \n",
    "        \n",
    "    \n",
    "class Record(object):\n",
    "    \n",
    "    def __init__(self, solutions_h=None, stations_h=None, rid=0, iter_id=0):\n",
    "        \n",
    "        check_all = all(isinstance(s,SolutionHeader) for s in list(solutions_h))\n",
    "        if not check_all\n",
    "            raise Exception('arg: \\'solutions_h\\' must be of type SolutionHeader')\n",
    "            \n",
    "        check_all = all(isinstance(s,StationHeader) for s in list(stations_h))\n",
    "        if not check_all:\n",
    "            raise Exception('elements in arg: \\'stations_h\\' must be of type StationHeader')\n",
    "            \n",
    "        check_all = all(s.sid == solutions_h.sid for s in stations_h)\n",
    "        if not check_all:\n",
    "            raise Exception('sid of each element in arg: \\'stations_h\\' must match solutions_h.sid')\n",
    "            \n",
    "        #self.solution_header = copy.deepcopy(solutions_h)\n",
    "        #self.station_headers = copy.deepcopy(stations_h)\n",
    "        \n",
    "        self.df = pd.DataFrame.from_records(stations_h, index=['eid','sid','trid','gid'])\n",
    "        \n",
    "        self.rid     = rid\n",
    "        self.iter_id = iter_id\n",
    "        \n",
    "        \n",
    "        self.added_header_words = []\n",
    "        \n",
    "        \n",
    "        #self.traces = []\n",
    "        #for i in range(len(self.station_headers)):\n",
    "            #tr = StationTrace(header=self.station_headers[i], data=np.arange(2000,dtype=np.float32)+i, start_t=0, dt=0.25)\n",
    "            #self.traces.append(tr)\n",
    "        for i in range(len(self.station_headers)):\n",
    "            s = self.station_headers[i]\n",
    "            s['data'] = data=np.arange(2000,dtype=np.float32)+i\n",
    "            self._update_df\n",
    "        self.added_header_words.append('data')\n",
    "            \n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        out_str  = f'Solution Header:\\n{self.solution_header}\\n'\n",
    "        out_str += f'Station Headers:\\n {self.df}'\n",
    "        return out_str\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out_str  = f'Solution Header:\\n{self.solution_header.__repr__()}\\n'\n",
    "        out_str += f'Station Headers:\\n {self.df.__repr__()}'\n",
    "        return out_str\n",
    "    \n",
    "    \n",
    "    def _update_df(self):\n",
    "        df_index = copy.deepcopy(self.df.index)\n",
    "        del self.df\n",
    "        self.df = pd.DataFrame.from_records(self.station_headers, index=df_index)\n",
    "    \n",
    "    def add_header_word_to_stations(self, func=None):\n",
    "        \n",
    "        if not callable(func):\n",
    "            raise Exception('arg: \\'func\\' must be a function')\n",
    "            \n",
    "        key = func(self)\n",
    "        self.added_header_words.append(key)\n",
    "            \n",
    "        self._update_df()\n",
    "    \n",
    "    @property\n",
    "    def traces(self):\n",
    "        return self.df['data'].to_numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "#df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\n",
    "#df = pd.DataFrame.from_records(l_grp_stations)\n",
    "#df = pd.DataFrame.from_records(flatten_grouped_headers(l_grp_stations),index=['sid','trid','gid'])\n",
    "#headers = [{'A': 'apple', 'B': 0, 'C': 3, 'D': 3}, {'A': 'banana', 'B': 9, 'C': 3, 'D': 5}, {'A': 'orange', 'B': 4, 'C': 7, 'D': 6}]\n",
    "#df = pd.DataFrame(headers)\n",
    "#print('df:\\n',df[0:18])\n",
    "\n",
    "df_force_h = test_force_h\n",
    "df_force_h.sid = 0\n",
    "print('df_force_h.sid:',df_force_h.sid)\n",
    "print()\n",
    "t_record = Record(solutions_h=df_force_h,stations_h=flatten_grouped_headers(l_grp_stations))\n",
    "\n",
    "print(t_record)\n",
    "print()\n",
    "\n",
    "def offset_func(self,h_list):\n",
    "    for s in self.station_headers:\n",
    "        x_sqrd = (self.solution_header.lon_xc - s.lon_xc)**2\n",
    "        y_sqrd = (self.solution_header.lat_yc - s.lat_yc)**2\n",
    "        offset = np.sqrt(x_sqrd + y_sqrd)\n",
    "        s['offset'] = offset\n",
    "    return 'offset'\n",
    "        \n",
    "    \n",
    "t_record.add_header_word_to_stations(offset_func)\n",
    "\n",
    "print(t_record)\n",
    "print()\n",
    "print('added header words:\\n',t_record.added_header_words)\n",
    "print()\n",
    "print('Traces:\\n',t_record.traces)\n",
    "print()\n",
    "\n",
    "t_st = StationTrace(s_grp_stations[0],np.arange(2000), start_t=0, dt=0.25)\n",
    "\n",
    "print(t_st)\n",
    "print()\n",
    "\n",
    "t_st[1:3] = -1*np.ones((2))\n",
    "print('slice-data:',t_st[:5])\n",
    "print()\n",
    "\n",
    "print('times:',t_st.times())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.headers import RecordHeader\n",
    "\n",
    "class StationTrace(object):\n",
    "    \n",
    "    def __init__(self, header=None, data=None, start_t=0, dt=1):\n",
    "        \n",
    "        if not isinstance(header,StationHeader):\n",
    "            raise Exception('arg: \\'header\\' must be of type StationHeader')\n",
    "            \n",
    "        self.header  = copy.deepcopy(header)\n",
    "        self.data    = np.array(data,dtype=np.float32)\n",
    "        self.start_t = np.float32(start_t)\n",
    "        self.dt      = np.float32(dt)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, islice):\n",
    "        return self.data[islice]\n",
    "    \n",
    "    def __setitem__(self, islice, value):\n",
    "        self.data[islice] = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        out_str  = f'Station Header:\\n{self.header}\\n'\n",
    "        out_str += f'Data:\\n{self.data}'\n",
    "        return out_str\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out_str  = f'Station Header:\\n{self.header.__repr__()}\\n'\n",
    "        out_str += f'Data:\\n{self.data.__repr__()}'\n",
    "        return out_str\n",
    "    \n",
    "    def times(self):\n",
    "        return self.start_t + np.arange(len(self.data),dtype=np.float32)*self.dt\n",
    "        \n",
    "        \n",
    "    \n",
    "df_force_h0 = test_force_h.copy()\n",
    "df_force_h1 = test_force_h.copy()\n",
    "df_force_h2 = test_force_h.copy()\n",
    "\n",
    "print('h0 == h1:',df_force_h0 == df_force_h1)\n",
    "print()\n",
    "print('h0 == h2:',df_force_h0 == df_force_h2)\n",
    "print()\n",
    "\n",
    "df_force_h0.sid = 0\n",
    "df_force_h0.eid = 0\n",
    "df_force_h1.sid = 1\n",
    "df_force_h1.eid = 0\n",
    "df_force_h2.sid = 2\n",
    "df_force_h2.eid = 0\n",
    "print('df_force_h0.sid:',df_force_h0.sid)\n",
    "print()\n",
    "print('after h0 == h1:',df_force_h0 == df_force_h1)\n",
    "print()\n",
    "print('after h0 == h2:',df_force_h0 == df_force_h2)\n",
    "print()\n",
    "\n",
    "flat_grp_stations = sorted(flatten_grouped_headers(l_grp_stations))\n",
    "print(f'flat_grp_stations[0]:\\n{flat_grp_stations[0]}')\n",
    "print()\n",
    "#t_record = RecordHeader(solutions_h=[df_force_h0],stations_h=flat_grp_stations)\n",
    "t_record = RecordHeader(solutions_h=[df_force_h0,df_force_h1,df_force_h2],stations_h=flatten_grouped_headers(l_grp_stations))\n",
    "print(f'type(df_force_h0)={type(df_force_h0)}\\n')\n",
    "\n",
    "print(t_record)\n",
    "print()\n",
    "\n",
    "rec_solutions_h = t_record.get_solutions_header_list()[0]\n",
    "rec_stations_h  = t_record.get_stations_header_list()\n",
    "print(f'station[0]:\\n{rec_stations_h[0]}')\n",
    "print(f'isinstance(rec_stations_h[0],ForceSolutionHeader)={isinstance(rec_stations_h[0],StationHeader)}\\n')\n",
    "print(f'type(flat_grp_stations[0])==type(rec_stations_h[0]) = {type(flat_grp_stations[0])==type(rec_stations_h[0])}\\n')\n",
    "print()\n",
    "print(f'solution[0]:\\n{rec_solutions_h}')\n",
    "print(f'isinstance(rec_solutions_h,ForceSolutionHeader)={isinstance(rec_solutions_h,ForceSolutionHeader)}\\n')\n",
    "print(f'type(df_force_h0)==type(rec_solutions_h) = {type(df_force_h0)==type(rec_solutions_h)}\\n')\n",
    "print()\n",
    "\n",
    "h_vals = []\n",
    "#key = 'new_eid'\n",
    "#for s in rec_stations_h:\n",
    "    #h_vals.append(s.trid)\n",
    "    \n",
    "src = rec_solutions_h\n",
    "key = 'offset'\n",
    "for s in rec_stations_h:\n",
    "    x_sqrd = (src.lon_xc - s.lon_xc)**2\n",
    "    y_sqrd = (src.lat_yc - s.lat_yc)**2\n",
    "    offset = np.sqrt(x_sqrd + y_sqrd)\n",
    "    h_vals.append(offset)\n",
    "    \n",
    "t_record.add_station_header_word(key,h_vals)\n",
    "#print('added header word:\\n',t_record)\n",
    "print('added header word:\\n',t_record.stations_df[:21])\n",
    "print()\n",
    "        \n",
    "'''\n",
    "    \n",
    "t_record.add_header_word_to_stations(offset_func)\n",
    "\n",
    "print(t_record)\n",
    "print()\n",
    "print('added header words:\\n',t_record.added_header_words)\n",
    "print()\n",
    "print('Traces:\\n',t_record.traces)\n",
    "print()\n",
    "''';\n",
    "\n",
    "'''\n",
    "t_st = StationTrace(s_grp_stations[0],np.arange(2000), start_t=0, dt=0.25)\n",
    "\n",
    "print(t_st)\n",
    "print()\n",
    "\n",
    "t_st[1:3] = -1*np.ones((2))\n",
    "print('slice-data:',t_st[:5])\n",
    "print()\n",
    "\n",
    "print('times:',t_st.times())\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_out_dir + '/test_record_header.pickle','wb')\n",
    "pickle.dump(t_record,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lth {data_out_dir + '/test_record_header.pickle'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_out_dir + '/test_record_header.pickle','rb')\n",
    "dill_t_record = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(dill_t_record)\n",
    "print()\n",
    "\n",
    "print('slice:\\n',type(dill_t_record.stations_df.iloc[0]))\n",
    "print()\n",
    "print('to_dict():\\n',dill_t_record.stations_df.iloc[0].to_dict())\n",
    "print()\n",
    "print('stations equal?:',dill_t_record.stations_df.equals(t_record.stations_df))\n",
    "print()\n",
    "print('solutions equal?:',dill_t_record.solutions_df.equals(t_record.solutions_df))\n",
    "print()\n",
    "print('recs equal?:', dill_t_record == t_record)\n",
    "print()\n",
    "dill_t_record.rid = 1\n",
    "print('recs.rid equal?:', dill_t_record == t_record)\n",
    "print()\n",
    "dill_t_record.rid = t_record.rid\n",
    "dill_t_record.proj_id = 1\n",
    "print('recs.proj_id equal?:', dill_t_record == t_record)\n",
    "print()\n",
    "dill_t_record.proj_id = t_record.proj_id\n",
    "dill_t_record.iter_id = 1\n",
    "print('recs.iter_id equal?:', dill_t_record == t_record)\n",
    "print()\n",
    "dill_t_record.iter_id = t_record.iter_id\n",
    "print('recs.reset equal?:', dill_t_record == t_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "                 \n",
    "print()\n",
    "print('--------------------Test Record with CMTSolutions--------------------------------------------------')\n",
    "print()\n",
    "\n",
    "test_cmt_h0 = CMTSolutionHeader(date=datetime.datetime.now(),\n",
    "                               ename='test_event',\n",
    "                               tshift=0.0,\n",
    "                               hdur=0.0,\n",
    "                               lat_yc=2.71,\n",
    "                               lon_xc=3.14,\n",
    "                               depth=-5432,\n",
    "                               mt=MomentTensor(mw=3.5,strike=120,dip=40,rake=15),\n",
    "                               eid=7,\n",
    "                               sid=77)\n",
    "\n",
    "test_cmt_h1 = test_cmt_h0.copy()\n",
    "test_cmt_h2 = test_cmt_h0.copy()\n",
    "\n",
    "print('h0 == h1:',test_cmt_h0 == test_cmt_h1)\n",
    "print()\n",
    "print('h0 == h2:',test_cmt_h0 == test_cmt_h2)\n",
    "print()\n",
    "\n",
    "print('h0:',test_cmt_h0)\n",
    "print()\n",
    "print('h1:',test_cmt_h1)\n",
    "print()\n",
    "print('h2:',test_cmt_h2)\n",
    "print()\n",
    "\n",
    "\n",
    "flat_grp_stations = sorted(flatten_grouped_headers(l_grp_stations))\n",
    "print(f'flat_grp_stations[0]:\\n{flat_grp_stations[0]}')\n",
    "print()\n",
    "#t_record = RecordHeader(solutions_h=[test_cmt_h0],stations_h=flat_grp_stations)\n",
    "cmt_record = RecordHeader(solutions_h=[test_cmt_h0,test_cmt_h1,test_cmt_h2],stations_h=flatten_grouped_headers(l_grp_stations))\n",
    "print(f'type(test_cmt_h0)={type(test_cmt_h0)}\\n')\n",
    "\n",
    "print(cmt_record)\n",
    "print()\n",
    "\n",
    "cmt_rec_solutions_h = cmt_record.get_solutions_header_list()[0]\n",
    "cmt_rec_stations_h  = cmt_record.get_stations_header_list()\n",
    "print(f'station[0]:\\n{cmt_rec_stations_h[0]}')\n",
    "print(f'isinstance(cmt_rec_stations_h[0],ForceSolutionHeader)={isinstance(cmt_rec_stations_h[0],StationHeader)}\\n')\n",
    "print(f'type(flat_grp_stations[0])==type(cmt_rec_stations_h[0]) = {type(flat_grp_stations[0])==type(cmt_rec_stations_h[0])}\\n')\n",
    "print()\n",
    "print(f'solution[0]:\\n{cmt_rec_solutions_h}')\n",
    "print(f'isinstance(cmt_rec_solutions_h,ForceSolutionHeader)={isinstance(cmt_rec_solutions_h,ForceSolutionHeader)}\\n')\n",
    "print(f'type(test_cmt_h0)==type(cmt_rec_solutions_h) = {type(test_cmt_h0)==type(cmt_rec_solutions_h)}\\n')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_out_dir + '/test_cmt_record_header.pickle','wb')\n",
    "pickle.dump(cmt_record,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lth {data_out_dir + '/test_cmt_record_header.pickle'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open(data_out_dir + '/test_cmt_record_header.pickle','rb')\n",
    "dill_cmt_record = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(dill_cmt_record)\n",
    "print()\n",
    "\n",
    "'''\n",
    "dill_nrow = len(dill_cmt_record.stations_df)\n",
    "cmt_nrow = len(cmt_record.stations_df)\n",
    "print(f'dill_nrow,cmt_nrow = {dill_nrow},{cmt_nrow}')\n",
    "#for i in range(dill_nrow):\n",
    "    #if not dill_cmt_record.stations_df.iloc[i].equals(cmt_record.stations_df.iloc[i]): \n",
    "        #print(f'not equal at: {i}')\n",
    "for i in range(1):\n",
    "    print(f'dill:\\n{dill_cmt_record.stations_df.iloc[7]}')\n",
    "    print(f'orig:\\n{cmt_record.stations_df.iloc[7]}')\n",
    "''';\n",
    "\n",
    "dill_nrow = len(dill_cmt_record.solutions_df)\n",
    "cmt_nrow = len(cmt_record.solutions_df)\n",
    "print(f'dill_nrow,cmt_nrow = {dill_nrow},{cmt_nrow}')\n",
    "dill_s = dill_cmt_record.solutions_df.iloc[0]\n",
    "cmt_s  = cmt_record.solutions_df.iloc[0] \n",
    "unzip_dill = list(zip(*dill_s.iteritems()))\n",
    "unzip_orig = list(zip(*cmt_s.iteritems()))\n",
    "print(f'dill iteritems:\\n{unzip_dill}')\n",
    "print(f'orig iteritems:\\n{unzip_orig}')\n",
    "\n",
    "print()\n",
    "print('--------------------------------------------------------')\n",
    "print()\n",
    "for i in range(len(unzip_dill)):\n",
    "    if unzip_dill[i] != unzip_orig[i]:\n",
    "        print(f'unzip_dill[{i}]\\n{unzip_dill[i]}')\n",
    "        print(f'unzip_orig[{i}]\\n{unzip_orig[i]}')\n",
    "    else:\n",
    "        print('all clear Mr Magoo')\n",
    "print()\n",
    "print('--------------------------------------------------------')\n",
    "print()\n",
    "    \n",
    "print()\n",
    "print()\n",
    "print('dill_sol:\\n',dill_cmt_record.solutions_df.iloc[0])\n",
    "print()\n",
    "print('orig_sol:\\n',cmt_record.solutions_df.iloc[0])\n",
    "\n",
    "print('dill_slice:\\n',dill_cmt_record.stations_df.iloc[7])\n",
    "print()\n",
    "print('orig_slice:\\n',cmt_record.stations_df.iloc[7])\n",
    "print()\n",
    "print('to_dict():\\n',dill_cmt_record.stations_df.iloc[0].to_dict())\n",
    "print()\n",
    "print('stations equal?:',dill_cmt_record.stations_df.equals(cmt_record.stations_df))\n",
    "#print('stations equal?:',dill_cmt_record.stations_df.equals(dill_cmt_record.stations_df))\n",
    "print()\n",
    "print('solutions equal?:',dill_cmt_record.solutions_df.equals(cmt_record.solutions_df))\n",
    "print()\n",
    "print('recs equal?:', dill_cmt_record == cmt_record)\n",
    "print('recs.proj_id equal?:', dill_cmt_record.proj_id == cmt_record.proj_id)\n",
    "print('recs.rid equal?:', dill_cmt_record.rid == cmt_record.rid)\n",
    "print('recs.iter_id equal?:', dill_cmt_record.iter_id == cmt_record.iter_id)\n",
    "print()\n",
    "dill_cmt_record.rid = 1\n",
    "print('recs.rid equal?:', dill_cmt_record == cmt_record)\n",
    "print()\n",
    "dill_cmt_record.rid = cmt_record.rid\n",
    "dill_cmt_record.proj_id = 1\n",
    "print('recs.proj_id equal?:', dill_cmt_record == cmt_record)\n",
    "print()\n",
    "dill_cmt_record.proj_id = cmt_record.proj_id\n",
    "dill_cmt_record.iter_id = 1\n",
    "print('recs.iter_id equal?:', dill_cmt_record == cmt_record)\n",
    "print()\n",
    "dill_cmt_record.iter_id = cmt_record.iter_id\n",
    "print('recs.reset equal?:', dill_cmt_record == cmt_record)\n",
    "'''\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silly_func(x):\n",
    "    print('Hahwoe!')\n",
    "    \n",
    "silly_func = 'Hahwoe!'\n",
    "    \n",
    "print(callable(silly_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mys = slice(0,10,2)\n",
    "print('dir:',dir(mys))\n",
    "print()\n",
    "print('dir(index):',dir(mys.indices))\n",
    "print()\n",
    "print('start:',mys.start)\n",
    "print('stop :',mys.stop)\n",
    "print('step :',mys.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "x[slice(2,10,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = t_record.stations_df\n",
    "#row = mydf[0]\n",
    "h_vals = list(mydf.iloc[1].values)\n",
    "print('row values :\\n',h_vals)\n",
    "h_keys = list(mydf.iloc[1].index.values)\n",
    "print('row indicis:\\n',h_keys)\n",
    "print()\n",
    "ih_keys = list(mydf.index.names)\n",
    "print('inames:\\n',ih_keys)\n",
    "print()\n",
    "ih_vals = list(mydf.index.values[1])\n",
    "print('ivalue:\\n',ih_vals)\n",
    "print()\n",
    "print('columns:',mydf.columns.values)\n",
    "print()\n",
    "#for index, row in mydf.iterrows():\n",
    "    #print(row)\n",
    "    \n",
    "h_dict = dict(zip(ih_keys + h_keys, ih_vals + h_vals))\n",
    "print('new dict:\\n',h_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(StationHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_dict = {'goob':0, 'smoog':1, 'cloeggy':2}\n",
    "print(dir(turk_dict))\n",
    "\n",
    "for key in turk_dict:\n",
    "    print(key)\n",
    "\n",
    "del turk_dict['goob']\n",
    "print(turk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(**turk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
