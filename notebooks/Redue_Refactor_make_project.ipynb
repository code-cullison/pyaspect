{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redue/refactor Project.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all packages\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from sys import argv\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "from pyaspect.project import *\n",
    "from pyaspect.model.gridmod3d import gridmod3d as gm\n",
    "from pyaspect.model.bbox import bbox as bb\n",
    "from pyaspect.model.gm3d_utils import *\n",
    "from pyaspect.moment_tensor import MomentTensor\n",
    "from pyaspect.specfemio.headers import *\n",
    "#from pyaspect.specfemio.write import *\n",
    "from pyaspect.specfemio.read import *\n",
    "from pyaspect.specfemio.utils import *\n",
    "\n",
    "\n",
    "import pyaspect.events.gevents as gevents\n",
    "import pyaspect.events.gstations as gstations\n",
    "from pyaspect.events.munge.knmi import correct_station_depths as csd_f\n",
    "import pyaspect.events.mtensors as mtensors\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy import UTCDateTime\n",
    "import shapefile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "Extract the ndarray of the subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_dir  = 'data/output/'\n",
    "data_out_dir = data_in_dir\n",
    "!ls {data_in_dir}\n",
    "!ls data/groningen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 \n",
    "\n",
    "Decompress the ndarray of the sliced, subsampled, smoothed NAM model and instantiate a new GriddedModel3D object for QC'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename then used it to decompress model\n",
    "ifqn = f'{data_out_dir}/vsliced_subsmp_smth_nam_2017_vp_vs_rho_Q_model_dx100_dy100_dz100_maxdepth5850_sig250.npz'\n",
    "vslice_gm3d, other_pars = decompress_gm3d_from_file(ifqn)\n",
    "\n",
    "print()\n",
    "print('decompressed gridded model\\n:',vslice_gm3d) \n",
    "print()\n",
    "print('other parameters:\\n',other_pars)\n",
    "print()\n",
    "\n",
    "# WARNING: this will unpack all other_pars, if you overwrite a variable of the samename as val(key), then you \n",
    "#          may not notice, and this may cause large headaches.  I use it because I am aware of it.\n",
    "'''\n",
    "for key in other_pars:\n",
    "    locals()[key] = other_pars[key]  #this is more advanced python than I think is reasonable for most \n",
    "sig_meters = sig\n",
    "''';\n",
    "\n",
    "# another way to get these varibles is just use the accessor functions for the gridmod3d.  We need them later.\n",
    "xmin = other_pars['xmin']\n",
    "dx   = other_pars['dx']\n",
    "nx   = other_pars['nx']\n",
    "ymin = other_pars['ymin']\n",
    "dy   = other_pars['dy']\n",
    "ny   = other_pars['ny']\n",
    "zmin = other_pars['zmin']\n",
    "dz   = other_pars['dz']\n",
    "nz   = other_pars['nz']\n",
    "sig_meters = other_pars['sig']  # this variable is used later\n",
    "print('sig_meters:',sig_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "nam_dims = list(vslice_gm3d.get_npoints())\n",
    "nam_origin = [0,0,-vslice_gm3d.get_gorigin()[2]]\n",
    "#nam_origin = list(vslice_gm3d.get_gorigin())\n",
    "#nam_origin[2] *= -1\n",
    "nam_origin = tuple(nam_origin)\n",
    "nam_spacing = list(vslice_gm3d.get_deltas())\n",
    "nam_spacing[2] *=-1\n",
    "nam_spacing = tuple(nam_spacing)\n",
    "print('nam_dims:',nam_dims)\n",
    "print('nam_origin:',nam_origin)\n",
    "print('nam_spacing:',nam_spacing)\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.dimensions = np.array(nam_dims) + 1\n",
    "grid.origin = nam_origin  # The bottom left corner of the data set\n",
    "grid.spacing = nam_spacing  # These are the cell sizes along each axis\n",
    "nam_pvalues = vslice_gm3d.getNPArray()[0]\n",
    "print('pvalues.shape:',nam_pvalues.shape)\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_arrays[\"values\"] = nam_pvalues.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "# Now plot the grid!\n",
    "cmap = plt.cm.jet\n",
    "#grid.plot(show_edges=True,cmap=cmap)\n",
    "grid.plot(cmap=cmap,opacity=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = grid.slice_orthogonal()\n",
    "\n",
    "#slices.plot(show_edges=True,cmap=cmap)\n",
    "slices.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create random virtual source (to specfem stations, but using reciprocity -- sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = vslice_gm3d.getGlobalCoordsPointsXYZ()\n",
    "coords = vslice_gm3d.getLocalCoordsPointsXYZ()\n",
    "coords[:,2] = -coords[:,2]\n",
    "\n",
    "xc = np.unique(coords.T[0,:])\n",
    "yc = np.unique(coords.T[1,:])\n",
    "zc = np.unique(coords.T[2,:])\n",
    "\n",
    "\n",
    "#n_rand_p = 1000\n",
    "\n",
    "n_rand_p = 3\n",
    "np.random.seed(n_rand_p) #nothing special about using n_rand_p just want reproducible random\n",
    "\n",
    "#stay away from the edges of the model for derivatives \n",
    "# and to avoid boundary effects\n",
    "xy_pad = 500 \n",
    "\n",
    "lrx = np.min(xc) + xy_pad\n",
    "lry = np.min(yc) + xy_pad\n",
    "lrz = -3400.0\n",
    "\n",
    "hrx = np.max(xc) - xy_pad\n",
    "hry = np.max(yc) - xy_pad\n",
    "hrz = -2600.0\n",
    "\n",
    "srx = hrx - lrx\n",
    "sry = hry - lry\n",
    "srz = hrz - lrz\n",
    "\n",
    "r_xyz_list = []\n",
    "for i in range(n_rand_p):\n",
    "    rx = lrx + srx*np.random.rand()\n",
    "    ry = lry + sry*np.random.rand()\n",
    "    rz = lrz + srz*np.random.rand()\n",
    "    r_xyz_list.append([rx,ry,rz])\n",
    "    \n",
    "r_xyz = np.array(r_xyz_list)\n",
    "    \n",
    "\n",
    "#r_xyz = np.vstack(np.meshgrid(rx,ry,rz)).reshape(3,-1).T\n",
    "print('r_xyz:\\n',r_xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rpoints = pv.wrap(r_xyz)\n",
    "p = pv.Plotter()\n",
    "slices = grid.slice_orthogonal()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(slices,cmap=cmap,opacity=1)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=1.0)\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Moment Tensors and CMTSolutionHeaders for each tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is the path to the project dir on the cluster\n",
    "my_proj_dir = '/scratch/seismology/tcullison/test_mesh/FWD_Batch_Src_Test'\n",
    "\n",
    "magnitude = np.pi\n",
    "strike = [30,45,90] # just making three to test\n",
    "dip = [30,30,60]\n",
    "rake = [330,190,20]\n",
    "\n",
    "l_mt = []\n",
    "for i in range(len(strike)):\n",
    "    l_mt.append(MomentTensor(mw=magnitude,strike=strike[i],dip=dip[i],rake=rake[i]))\n",
    "\n",
    "assert len(l_mt) == len(r_xyz)\n",
    "\n",
    "for mt in l_mt:\n",
    "    print(mt)\n",
    "    \n",
    "l_cmt_src = []\n",
    "for i in range(len(r_xyz)):\n",
    "    cmt_h = CMTSolutionHeader(date=datetime.datetime.now(),\n",
    "                              ename=f'Event-{str(i).zfill(4)}',\n",
    "                              tshift=0.0,\n",
    "                              hdur=0.0,\n",
    "                              lat_yc=r_xyz[i,1],\n",
    "                              lon_xc=r_xyz[i,0],\n",
    "                              depth=-r_xyz[i,2],\n",
    "                              mt=l_mt[i],\n",
    "                              eid=i,\n",
    "                              sid=0)\n",
    "    l_cmt_src.append(cmt_h)\n",
    "    \n",
    "print()\n",
    "for cmt in l_cmt_src:\n",
    "    print(cmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Corresponding \"Virtual\" Recievers (including cross membors for derivatives) for the CMT's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_delta = 50.0 # distance between cross stations for derivatives\n",
    "assert m_delta < xy_pad #see cells above this is padding\n",
    "#l_grp_vrecs = make_grouped_half_cross_reciprocal_station_headers_from_cmt_list(l_cmt_src,m_delta)\n",
    "l_grp_vrecs = make_grouped_cross_reciprocal_station_headers_from_cmt_list(l_cmt_src,m_delta)\n",
    "\n",
    "ig = 0\n",
    "for grp in l_grp_vrecs:\n",
    "    print(f'***** Group: {ig} *****\\n')\n",
    "    ir = 0\n",
    "    for gvrec in grp:\n",
    "        print(f'*** vrec: {ir} ***\\n{gvrec}')\n",
    "        ir += 1\n",
    "    ig += 1\n",
    "\n",
    "print(len(flatten_grouped_headers(l_grp_vrecs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Virtual Receiver Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_xyz = get_xyz_coords_from_station_list(flatten_grouped_headers(l_grp_vrecs))\n",
    "all_g_xyz[:,2] *= -1 #pyview z-up positive and oposize sign of standard geophysics \n",
    "pv_all_points = pv.wrap(all_g_xyz)\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.5)\n",
    "p.add_mesh(slices,cmap=cmap,opacity=1.0)\n",
    "p.add_mesh(pv_all_points, render_points_as_spheres=True, point_size=5,opacity=1.0)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get receiver/station coordinates created from a different notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickle the Bounding box (from a different notebook)\n",
    "\n",
    "#ifqn  = data_out_dir + 'bbox_nvl' + str(int(nvl)) + '_nvb' + str(int(nvb))\n",
    "#ifqn += '_xsft' + str(xshift) + '_ysft' + str(yshift) + '.pickle'\n",
    "ifqn = data_out_dir + 'bbox_nvl152_nvb197_xsft4400_ysft19100.pickle'\n",
    "f = open(ifqn, 'rb')\n",
    "sgf_bbox = pickle.load(f)\n",
    "f.close()\n",
    "print()\n",
    "print('Unpickled Bounding:\\n',sgf_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickle the events if needed (again from a different notebook)\n",
    "ifqn = data_out_dir + 'bbox_groning_events.pickle'\n",
    "f = open(ifqn, 'rb')\n",
    "bbox_events = pickle.load(f)\n",
    "f.close()\n",
    "print()\n",
    "print('Unpickled Events:\\n',bbox_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read moment tensors\n",
    "mt_in_file  = 'data/groningen/events/event_moments.csv' \n",
    "!ls {mt_in_file}\n",
    "bbox_gf_mts = mtensors(mt_in_file)\n",
    "\n",
    "# get event catalog of the events (ObsPy catalog)\n",
    "bbox_event_cat = copy.deepcopy(bbox_events.getIncCatalog())\n",
    "\n",
    "# This is a bit hokey, but it works. Here we update the\n",
    "# event time from the moment tensor CSV file with thouse\n",
    "# from the event catalog\n",
    "bbox_gf_mts.update_utcdatetime(bbox_event_cat)\n",
    "\n",
    "'''\n",
    "#for imt in range(len(bbox_gf_mts)):\n",
    "#    print(\"Moment-Tensor %d:/n\" %(imt),bbox_gf_mts[imt])\n",
    "'''\n",
    "\n",
    "# Create a dictionary that maps moment tensors to events\n",
    "bbox_emap,bbox_mt_cat,bbox_mts = bbox_gf_mts.get_intersect_map_events_mts(bbox_event_cat)\n",
    "bbox_e2mt_keys = bbox_emap.keys()\n",
    "\n",
    "# Print a comparison of events to moment tensors\n",
    "for key in bbox_e2mt_keys:\n",
    "    print('UTC: event[%d][Date] = %s' %(key,bbox_mt_cat[key].origins[0].time))\n",
    "    print('UTC:    MT[%d][Date] = %s' %(key,bbox_emap[key]['Date']))\n",
    "    print('Mag: event[%d][Date] = %s' %(key,bbox_mt_cat[key].magnitudes[0].mag))\n",
    "    print('Mag:    MT[%d][Date] = %s' %(key,bbox_emap[key]['ML']))\n",
    "    print()\n",
    "\n",
    "#replace moment-tensors with only those that intersect with the events in the BoundingBox\n",
    "bbox_gf_mts.replace_moment_tensors_from_map(bbox_emap)\n",
    "    \n",
    "# add mt_catalog to bbox_events\n",
    "bbox_events.mergeMomentTensorsCatalog(bbox_mt_cat)\n",
    "merged_bbox_event_cat = bbox_events.getIncCatalog()\n",
    "print('bbox_event_cat:\\n', bbox_event_cat)\n",
    "print()\n",
    "print('merged_bbox_event_cat:\\n', merged_bbox_event_cat)\n",
    "print()\n",
    "print('bbox_mt_cat:\\n', bbox_mt_cat)\n",
    "print()\n",
    "print('bbox_mt_df:\\n', bbox_gf_mts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifqn = data_out_dir + 'bbox_groning_stations.pickle'\n",
    "\n",
    "print('Unpickling Station Traces')\n",
    "f = open(ifqn, 'rb')\n",
    "bbox_straces = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print('Stations:\\n',type(bbox_straces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read shapefiles\n",
    "shape_in_files  = 'data/groningen/shapefile/Groningen_field' \n",
    "\n",
    "gf_shape = sf.Reader(shape_in_files)\n",
    "print('Groningen Field shape:',gf_shape)\n",
    "\n",
    "#get coordinates for the Shape-File\n",
    "s = gf_shape.shape(0)\n",
    "shape_xy = np.asarray(s.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is kind of hokey, but it works for now.\n",
    "# Some of the stations depths do not follow the \n",
    "# 50, 100, 150, 200 meter depths -- possibly because\n",
    "# the boreholes are slanted. To correct for this,\n",
    "# a hard coded \"patch/update\" is applied. See the\n",
    "# code for details and update values.\n",
    "#from gnam.events.munge.knmi import correct_station_depths as csd_f\n",
    "bbox_straces.correct_stations(csd_f)\n",
    "\n",
    "bbox_bb_diam = 1500  #size of the beachball for plotting. I had to play with this parameter\n",
    "bbox_cmt_bballs = bbox_gf_mts.get_cmt_beachballs(diam=bbox_bb_diam,fc='black')\n",
    "\n",
    "bbox_mt_coords = bbox_events.getIncCoords()\n",
    "\n",
    "#get event and borhole keys used for indexing\n",
    "ekeys = bbox_straces.getEventKeys()\n",
    "bkeys = bbox_straces.getBoreholeKeys()\n",
    "\n",
    "#Plot seuence of events with stations \n",
    "#for ie in ekeys:\n",
    "for i in range(1):\n",
    "    ie = ekeys[i]\n",
    "    # coordinates for stations that are in the bounding box\n",
    "    xy3 = bbox_straces.getIncStationCoords(ie,bkeys[0]) #station code G##3\n",
    "    xy4 = bbox_straces.getIncStationCoords(ie,bkeys[1]) #station code G##4\n",
    "    \n",
    "    # coordinates for stations that are G-stations but outside the bounding box\n",
    "    ex_xy3 = bbox_straces.getExcStationCoords(ie,bkeys[0]) #station code G##3\n",
    "    ex_xy4 = bbox_straces.getExcStationCoords(ie,bkeys[1]) #station code G##4\n",
    "    \n",
    "    # coordinates for stations that are inside the bounding box but there is no data\n",
    "    er_xy3 = bbox_straces.getErrStationCoords(ie,bkeys[0]) #station code G##3\n",
    "    er_xy4 = bbox_straces.getErrStationCoords(ie,bkeys[1]) #station code G##4\n",
    "\n",
    "    fig, ax = plt.subplots(1,figsize=(8,8))\n",
    "    fig.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    #Groningen Field Shape-File\n",
    "    ax.scatter(shape_xy[:,0],shape_xy[:,1],s=1,c='black',zorder=0)\n",
    "    \n",
    "    #Bounding Box\n",
    "    ax.plot(sgf_bbox.getCLoop()[:,0],sgf_bbox.getCLoop()[:,1],c='green',zorder=1)\n",
    "    \n",
    "    #Events (reuse event coordinates from cell further above)\n",
    "    ax.scatter(bbox_mt_coords[ie,0],bbox_mt_coords[ie,1],s=90,c='red',marker='*',zorder=5)\n",
    "    beach = bbox_cmt_bballs[ie]  #this creates a plot collection for the beachball points\n",
    "    beach.set_zorder(3)\n",
    "    ax.add_collection(beach)\n",
    "    \n",
    "    #Included stations\n",
    "    ax.scatter(xy3[:,0],xy3[:,1],s=50,c='blue',marker='v',zorder=3)\n",
    "    ax.scatter(xy4[:,0],xy4[:,1],s=100,c='gray',marker='o',zorder=2)\n",
    "    \n",
    "    #Excluded stations\n",
    "    ax.scatter(ex_xy3[:,0],ex_xy3[:,1],s=80,c='lightgray',marker='1',zorder=4)\n",
    "    ax.scatter(ex_xy4[:,0],ex_xy4[:,1],s=100,c='lightgray',marker='2',zorder=3)\n",
    "    \n",
    "    #Stations without data\n",
    "    ax.scatter(er_xy3[:,0],er_xy3[:,1],s=50,c='yellow',marker='v',zorder=4)\n",
    "    ax.scatter(er_xy4[:,0],er_xy4[:,1],s=100,c='gray',marker='o',zorder=3)\n",
    "    \n",
    "    origin_time = bbox_events[ie].origins[0].time\n",
    "    mag = bbox_events[ie].magnitudes[0].mag\n",
    "    title_str = 'Event-%d, Origin Time: %s, Magnitude: %1.2f' %(ie,str(origin_time),mag)\n",
    "    ax.set_title(title_str)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(1,figsize=(8,8))\n",
    "fig1.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "#Groningen Field Shape-File\n",
    "ax1.scatter(shape_xy[:,0],shape_xy[:,1],s=1,c='black',zorder=0)\n",
    "\n",
    "#Bounding Box\n",
    "ax1.plot(sgf_bbox.getCLoop()[:,0],sgf_bbox.getCLoop()[:,1],c='green',zorder=1)\n",
    "\n",
    "#Events (reuse event coordinates from cell further above)\n",
    "ax1.scatter(bbox_mt_coords[ie,0],bbox_mt_coords[ie,1],s=90,c='red',marker='*',zorder=5)\n",
    "    \n",
    "all_xy4 = np.concatenate((xy4,er_xy4),axis=0)\n",
    "#all_xy4 = xy4\n",
    "\n",
    "ax1.scatter(all_xy4[:,0],all_xy4[:,1],s=100,c='gray',marker='o',zorder=2)\n",
    "ax1.scatter(er_xy4[:,0],er_xy4[:,1],s=100,c='yellow',marker='x',zorder=2)\n",
    "\n",
    "title_str = 'Event-%d, Origin Time: %s, Magnitude: %1.2f' %(ie,str(origin_time),mag)\n",
    "ax1.set_title(title_str)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make random virtual sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = vslice_gm3d.getLocalCoordsPointsXY()\n",
    "\n",
    "x_orig = vslice_gm3d.get_gorigin()[0]\n",
    "y_orig = vslice_gm3d.get_gorigin()[1]\n",
    "\n",
    "clip_xy = all_xy4[9:13]\n",
    "print(clip_xy)\n",
    "\n",
    "s_xyz = np.zeros((len(clip_xy),3))\n",
    "s_xyz[:,0] = clip_xy[:,0] - x_orig\n",
    "s_xyz[:,1] = clip_xy[:,1] - y_orig\n",
    "s_xyz[:,2] = -200\n",
    "\n",
    "print(s_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot virtual sources (red) with virtual receivers (white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_spoints = pv.wrap(s_xyz)\n",
    "p = pv.Plotter()\n",
    "#p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=8,opacity=1,color='red')\n",
    "#p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.add_mesh(all_g_xyz, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make StationHeaders (real recievers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_real_recs = []\n",
    "for i in range(len(s_xyz)):\n",
    "    \n",
    "    tr_bname = 'tr'\n",
    "    new_r = StationHeader(name=tr_bname,\n",
    "                          network='NL', #FIXME\n",
    "                          lon_xc=s_xyz[i,0],\n",
    "                          lat_yc=s_xyz[i,1],\n",
    "                          depth=-s_xyz[i,2], #specfem z-down is positive\n",
    "                          elevation=0.0,\n",
    "                          trid=i)\n",
    "    l_real_recs.append(new_r)\n",
    "    \n",
    "for rec in l_real_recs:\n",
    "    print(rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make ForceSolutionHeaders for the above virtual sources (including force-triplets for calculation derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_grp_vsrcs = make_grouped_reciprocal_force_solution_triplet_headers_from_rec_list(l_real_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make replicates of each virtual receiver list: one for each force-triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_grp_vrecs_by_vsrcs = make_replicated_reciprocal_station_headers_from_src_triplet_list(l_grp_vsrcs,\n",
    "                                                                                          l_grp_vrecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot virtual sources (red) and virtual receivers (white) FROM headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp_s_xyz = get_unique_xyz_coords_from_solution_list(flatten_grouped_headers(l_grp_vsrcs))\n",
    "grp_s_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "flat_recs = flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs))\n",
    "grp_r_xyz = get_unique_xyz_coords_from_station_list(flat_recs)\n",
    "grp_r_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "print(len(grp_s_xyz))\n",
    "print(len(grp_r_xyz))\n",
    "\n",
    "pv_spoints = pv.wrap(grp_s_xyz)\n",
    "pv_rpoints = pv.wrap(grp_r_xyz)\n",
    "\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=8,opacity=1,color='red')\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=5,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make replicates of each \"real\" receiver list: for each CMT source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_grp_recs_by_srcs = make_replicated_station_headers_from_src_list(l_cmt_src,l_real_recs)\n",
    "\n",
    "\n",
    "for i in range(len(l_cmt_src)):\n",
    "    print(f'***** SRC Records for Source: {i} *****\\n')\n",
    "    for j in range(len(l_real_recs)):\n",
    "        print(f'*** REC Header for Receiver: {j} ***\\n{l_grp_recs_by_srcs[i][j]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot \"real\" sources (red) and virtual receivers (white) FROM headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_s_xyz = get_unique_xyz_coords_from_solution_list(l_cmt_src)\n",
    "grp_s_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "flat_recs = flatten_grouped_headers(l_grp_recs_by_srcs) #real!\n",
    "grp_r_xyz = get_unique_xyz_coords_from_station_list(flat_recs)\n",
    "grp_r_xyz[:,2] *= -1 #pyvista z-up is positive\n",
    "\n",
    "print(len(grp_s_xyz))\n",
    "print(len(grp_r_xyz))\n",
    "\n",
    "pv_spoints = pv.wrap(grp_s_xyz)\n",
    "pv_rpoints = pv.wrap(grp_r_xyz)\n",
    "\n",
    "p = pv.Plotter()\n",
    "p.add_mesh(slices,cmap=cmap,opacity=0.50)\n",
    "p.add_mesh(grid,cmap=cmap,opacity=0.3)\n",
    "p.add_mesh(pv_spoints, render_points_as_spheres=True, point_size=12,opacity=1,color='red')\n",
    "p.add_mesh(pv_rpoints, render_points_as_spheres=True, point_size=8,opacity=0.5)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reciprical RecordHeader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.utils import station_auto_data_fname_id\n",
    "from pyaspect.specfemio.write import _write_header\n",
    "\n",
    "print(len(flatten_grouped_headers(l_grp_vsrcs.copy())))\n",
    "print(len(flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs.copy()))))\n",
    "print('nrec_per_src*nsrc:',21*12)\n",
    "\n",
    "l_flat_vsrcs = flatten_grouped_headers(l_grp_vsrcs)\n",
    "l_flat_vrecs = flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs))\n",
    "\n",
    "vrecord_h = RecordHeader(name='Reciprocal-Record',solutions_h=l_flat_vsrcs,stations_h=l_flat_vrecs)\n",
    "\n",
    "vrec_fqp = os.path.join(data_out_dir,'simple_record_h')\n",
    "_write_header(vrec_fqp,vrecord_h)\n",
    "\n",
    "!ls -l {vrec_fqp}\n",
    "\n",
    "print(vrecord_h)\n",
    "print('\\n***************************************************\\n')\n",
    "'''\n",
    "#print(vrecord_h.get_event_nsolutions(1))\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "slice_rec_h = vrecord_h.copy()\n",
    "slice_rec_h.solutions_df.reset_index(inplace=True)\n",
    "#slice_rec_h.stations_df.reset_index(inplace=True)\n",
    "slice_rec_h.stations_df\n",
    "rec_df = slice_rec_h.stations_df\n",
    "#print(f'rec_df = {rec_df}')\n",
    "for index, src in slice_rec_h.solutions_df.iterrows():\n",
    "    print(f'**** src.sid = {src.sid} ****************\\n')\n",
    "    print(f'**** src.eid = {src.eid} ****************\\n')\n",
    "    #print(rec_df[rec_df['proj_id'] == src.proj_id])\n",
    "    #print(f'index = {index}')\n",
    "    #new_rec_df = rec_df[(rec_df['proj_id'] == src.proj_id) & (rec_df['eid'] == src.eid) & (rec_df['sid'] == src.sid)]\n",
    "    for index,rec in rec_df.loc[idx[src.proj_id,src.eid,src.sid],:].reset_index().iterrows():\n",
    "        print(rec)\n",
    "#slice_rec_h.stations_df.loc[idx[0,0,:,:],'data_fqdn'] = '/somewhere/over/the/rainbow'\n",
    "#print(slice_rec_h.stations_df.loc[idx[0,0,:,:],'data_fqdn'])\n",
    "''';\n",
    "\n",
    "print()\n",
    "#svr = vrecord_h[0,0,0,0]\n",
    "svr = vrecord_h[::5,::5,::5,::7]\n",
    "print('svr:',svr)\n",
    "print('\\n***************************************************\\n')\n",
    "svr.solutions_df['proj_id'] = 1\n",
    "svr.stations_df['proj_id'] = 1\n",
    "old_idx = svr.stations_df.index\n",
    "svr.stations_df.reset_index(inplace=True)\n",
    "for index,row in svr.stations_df.iterrows():\n",
    "    #svr.stations_df.loc[index,'data_fqdn'] = f'../SYN/s{row.sid}.t{str(row.trid).zfill(6)}'\n",
    "    svr.stations_df.loc[index,'data_fqdn'] = f'../SYN/{station_auto_data_fname_id(row)}'\n",
    "\n",
    "print()\n",
    "svr.stations_df.set_index(old_idx,inplace=True)\n",
    "print('proj svr:',svr)\n",
    "\n",
    "#print('index:',svr.index)\n",
    "#print('shape:',svr.index.shape)\n",
    "\n",
    "'''\n",
    "svr_idx_names = svr.index.names\n",
    "print('names:\\n',svr_idx_names)\n",
    "print()\n",
    "print('orig:\\n',svr)\n",
    "svr.reset_index(inplace=True)\n",
    "print()\n",
    "print('reset:\\n',svr)\n",
    "svr['proj_id'] = 1\n",
    "print()\n",
    "print('new proj_id:\\n',svr)\n",
    "svr.set_index(svr_idx_names)\n",
    "print()\n",
    "print('new idx:\\n',svr)\n",
    "''';\n",
    "\n",
    "#assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo Make Project Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.write import write_solution\n",
    "from pyaspect.specfemio.write import write_stations\n",
    "from pyaspect.specfemio.write import _write_header\n",
    "from pyaspect.specfemio.utils import station_auto_data_fname_id\n",
    "from pyaspect.specfemio.headers import SolutionHeader\n",
    "from pyaspect.specfemio.headers import StationHeader\n",
    "\n",
    "def df_to_header_list(df,HeaderCls):\n",
    "    return  [HeaderCls.from_series(row) for index, row in df.iterrows()] \n",
    "\n",
    "def write_record(rdir_fqp,\n",
    "                 record_h,\n",
    "                 fname='event_record',\n",
    "                 write_record_h=True,\n",
    "                 write_h=False,\n",
    "                 auto_name=False,\n",
    "                 auto_network=False):\n",
    "    \n",
    "    \n",
    "    data_fqp = os.path.join(rdir_fqp,'DATA')\n",
    "    syn_fqp  = os.path.join(rdir_fqp,'SYN')\n",
    "    rel_syn_fqp = os.path.relpath(syn_fqp,syn_fqp)\n",
    "    rel_syn_data_fqp = os.path.relpath(syn_fqp,data_fqp)\n",
    "    \n",
    "    record_h.reset_midx()\n",
    "    data_h = record_h.copy()\n",
    "    \n",
    "    \n",
    "    src_df = record_h.solutions_df\n",
    "    SrcHeader = record_h.solution_cls\n",
    "    \n",
    "    rec_df = record_h.stations_df\n",
    "    data_rec_df = data_h.stations_df\n",
    "    RecHeader = record_h.station_cls\n",
    "    \n",
    "    mk_sym_link = True # first <CMT|FORCE>SOLUTION and STATIONS files get symlink\n",
    "    for sidx, src in src_df.iterrows():\n",
    "        \n",
    "        #solution = src_htype.from_series(src)\n",
    "        solution = SrcHeader.from_series(src)\n",
    "        write_solution(data_fqp,\n",
    "                       solution,\n",
    "                       postfix=f'e{src.eid}s{src.sid}',\n",
    "                       write_h=write_h,\n",
    "                       mk_sym_link=mk_sym_link)\n",
    "        \n",
    "        for ridx, rec in rec_df.loc[rec_df['sid'] == src.sid].iterrows():\n",
    "            rec_df.loc[ridx,'data_fqdn'] = os.path.join(rel_syn_fqp,station_auto_data_fname_id(rec))\n",
    "            data_rec_df.loc[ridx,'data_fqdn'] = os.path.join(rel_syn_data_fqp,station_auto_data_fname_id(rec))\n",
    "            \n",
    "        \n",
    "        #get list of dictionaries\n",
    "        l_stations = df_to_header_list(data_rec_df,RecHeader)\n",
    "        write_stations(data_fqp,\n",
    "                       l_stations,\n",
    "                       fname=f'STATIONS.e{src.eid}s{src.sid}',\n",
    "                       write_h=write_h,\n",
    "                       auto_name=auto_name,\n",
    "                       auto_network=auto_network,\n",
    "                       mk_sym_link=mk_sym_link)\n",
    "        \n",
    "        \n",
    "        mk_sym_link = False #only write for first src\n",
    "        \n",
    "    # write record header in run####/SYN\n",
    "    record_h.set_default_midx()\n",
    "    syn_record_fqp = _get_header_path(syn_fqp,fname)\n",
    "    _write_header(syn_record_fqp,record_h)\n",
    "        \n",
    "        \n",
    "    #write record header in run####/DATA\n",
    "    data_h.set_default_midx()\n",
    "    if write_record_h:\n",
    "        data_record_fqp = _get_header_path(data_fqp,fname)\n",
    "        _write_header(data_record_fqp,data_h)\n",
    "        \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.utils import make_record_headers\n",
    "from pyaspect.specfemio.utils import _mk_symlink\n",
    "from pyaspect.specfemio.utils import _copy_recursive_dir\n",
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "from pyaspect.specfemio.utils import _get_header_path\n",
    "from pyaspect.specfemio.utils import station_auto_data_fname_id\n",
    "from pyaspect.parfile import change_multiple_parameters_in_lines\n",
    "from pyaspect.parfile import readlines\n",
    "from pyaspect.parfile import writelines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MAX_SPEC_SRC = int(9999) # see SPECFEM3D_Cartesian manual\n",
    "\n",
    "# list of directories need for every event\n",
    "common_dir_struct = {'DATA': {},\n",
    "                     'OUTPUT_FILES' : {'DATABASES_MPI':{}},\n",
    "                     'SYN': {},\n",
    "                     'FILT_SYN': {} }\n",
    "\n",
    "# extra common dirs for fwi\n",
    "common_fwi_dir_struct = {'SEM': {},\n",
    "                         'OBS': {},\n",
    "                         'FILT_OBS': {} }\n",
    "\n",
    "# list of directories only needed for the primary run0001 dir\n",
    "primary_dir_struct= {'INPUT_GRADIENT': {},\n",
    "                    'INPUT_KERNELS': {},\n",
    "                    'INPUT_MODEL': {},\n",
    "                    'OUTPUT_MODEL': {},\n",
    "                    'OUTPUT_SUM': {},\n",
    "                    'SMOOTH': {},\n",
    "                    'COMBINE': {},\n",
    "                    'topo': {} }\n",
    "\n",
    "\n",
    "def _make_dirs(fqdn,access_rights=0o755):\n",
    "    if os.path.isdir(fqdn):\n",
    "        raise OSError(f'The directory {fqdn} has already been created')\n",
    "    try:\n",
    "        os.makedirs(fqdn, access_rights)\n",
    "    except OSError:\n",
    "        print(f'Creation of the directory {fqdn} failed')\n",
    "        return OSError\n",
    "    \n",
    "    \n",
    "def _recursive_proj_dirs(dl,pdir,access_rights=0o755):\n",
    "\n",
    "    if len(dl.keys()) == 0:\n",
    "        return\n",
    "    else:\n",
    "        for dl_key in dl.keys():\n",
    "            new_dir = os.path.join(pdir, dl_key)\n",
    "            _make_dirs(new_dir,access_rights=0o755)\n",
    "            _recursive_proj_dirs(dl[dl_key],new_dir)\n",
    "\n",
    "\n",
    "def _make_proj_dir(proj_root_fqp,\n",
    "                   proj_base_name,\n",
    "                   pyutils_fqp=None,\n",
    "                   script_fqp=None):\n",
    "\n",
    "        projdir_fqp = os.path.join(proj_root_fqp, proj_base_name)\n",
    "        _make_dirs(projdir_fqp)\n",
    "        \n",
    "        # create project level symlinks \n",
    "        if pyutils_fqp != None:\n",
    "            lname = 'pyutils'\n",
    "            src = pyutils_fqp\n",
    "            dst = os.path.join(projdir_fqp, lname)\n",
    "            _mk_symlink(src,dst)\n",
    "\n",
    "        if script_fqp != None:\n",
    "            lname = 'scriptutils'\n",
    "            src = script_fqp\n",
    "            dst = os.path.join(projdir_fqp, lname)\n",
    "            _mk_symlink(src,dst)\n",
    "        \n",
    "        return projdir_fqp\n",
    "            \n",
    "def _make_run_dir(irdir,\n",
    "                  projdir_fqp,\n",
    "                  spec_bin_fqp,\n",
    "                  spec_utils_fqp,\n",
    "                  par_lines,\n",
    "                  dir_struct,\n",
    "                  record_h):\n",
    "\n",
    "    rdir_name = 'run' + str(irdir+1).zfill(4)\n",
    "    rundir_fqp = os.path.join(projdir_fqp, rdir_name)\n",
    "    _make_dirs(rundir_fqp)\n",
    "\n",
    "    # make sim links for each event dir \n",
    "    # (related to the computational node(s) filesytem\n",
    "    lname = 'bin'\n",
    "    src = spec_bin_fqp\n",
    "    dst = os.path.join(rundir_fqp, lname)\n",
    "    _mk_symlink(src,dst)\n",
    "\n",
    "    lname = 'utils'\n",
    "    src = spec_utils_fqp\n",
    "    dst = os.path.join(rundir_fqp, lname)\n",
    "    _mk_symlink(src,dst)\n",
    "\n",
    "    # make subdirectorieds for each event\n",
    "    _recursive_proj_dirs(common_dir_struct,rundir_fqp)\n",
    "\n",
    "    #write Par_files in DATA dirs\n",
    "    ddir_fqp = os.path.join(rundir_fqp, 'DATA')\n",
    "    out_par_fqp  = os.path.join(ddir_fqp, 'Par_file')\n",
    "    writelines(out_par_fqp,par_lines)\n",
    "    \n",
    "    #write Headers and Record\n",
    "    write_record(rundir_fqp,\n",
    "                 record_h,\n",
    "                 fname='record',\n",
    "                 write_record_h=True,\n",
    "                 write_h=False,\n",
    "                 auto_name=True,\n",
    "                 auto_network=True)\n",
    "\n",
    "\n",
    "def setup_mesh_dir(proj_fqp, mesh_fqp, copy_mesh=False):\n",
    "\n",
    "    mesh_dst_fqp = os.path.join(proj_fqp, 'MESH-default')\n",
    "    if copy_mesh:\n",
    "        _copy_recursive_dir(mesh_fqp,mesh_dst_fqp)\n",
    "    else:\n",
    "        _mk_symlink(mesh_fqp,mesh_dst_fqp)\n",
    "    \n",
    "\n",
    "def make_fwd_project_dir(proj_base_name,\n",
    "                         proj_root_fqp,\n",
    "                         parfile_fqp,\n",
    "                         mesh_fqp,\n",
    "                         spec_fqp,\n",
    "                         pyutils_fqp,\n",
    "                         script_fqp,\n",
    "                         proj_record_h,\n",
    "                         sub_proj_name=None,\n",
    "                         batch_srcs=False,\n",
    "                         copy_mesh=False,\n",
    "                         max_event_rdirs=MAX_SPEC_SRC,\n",
    "                         verbose=False):\n",
    "    \n",
    "    \n",
    "    if not isinstance(proj_record_h,RecordHeader):\n",
    "        raise TypeError('arg: \\'record_h\\' must be a RecordHeader type')\n",
    "\n",
    "    if not isinstance(proj_base_name,str):\n",
    "        raise TypeError('proj_base_name must be a str type')\n",
    "\n",
    "    if not isinstance(proj_root_fqp,str):\n",
    "        raise TypeError('proj_root_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(parfile_fqp,str):\n",
    "        raise TypeError('parfile_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(mesh_fqp,str):\n",
    "        raise TypeError('mesh_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(spec_fqp,str):\n",
    "        raise TypeError('spec_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(pyutils_fqp,str):\n",
    "        raise TypeError('pyutils_fqp must be a str type')\n",
    "\n",
    "    if not isinstance(script_fqp,str):\n",
    "        raise TypeError('script_fqp must be a str type')\n",
    "        \n",
    "    \n",
    "    ########################################################################\n",
    "    #\n",
    "    # setup project structure parameters\n",
    "    #\n",
    "    ########################################################################\n",
    "    \n",
    "    nevents = proj_record_h.nevents\n",
    "    nsrc = proj_record_h.nsrc\n",
    "    \n",
    "    #setup nbatch\n",
    "    nbatch = 1\n",
    "    if batch_srcs:\n",
    "        nbatch = nsrc\n",
    "            \n",
    "       \n",
    "    # calculate number of rundirs (events) per subproject/project\n",
    "    l_nrundirs = [nevents]\n",
    "    if not batch_srcs:\n",
    "        l_nrundirs[0] = nevents*nsrc\n",
    "        \n",
    "    # calculate if project or subprojects and addjust rundirs\n",
    "    if max_event_rdirs < l_nrundirs[0]:\n",
    "        old_nrundirs = l_nrundirs[0]\n",
    "        ndiv = l_nrundirs[0]//max_event_rdirs\n",
    "        l_nrundirs[0] = max_event_rdirs\n",
    "        for i in range(1,ndiv):\n",
    "            l_nrundirs.append(max_event_rdirs)\n",
    "        rem_rundirs = old_nrundirs%max_event_rdirs \n",
    "        if rem_rundirs != 0:\n",
    "            l_nrundirs.append(rem_rundirs)\n",
    "            \n",
    "    # actual number of subproject dirs\n",
    "    nprojdirs = len(l_nrundirs)\n",
    "    \n",
    "    # info\n",
    "    if verbose:\n",
    "        print(f'nevents: {nevents}')\n",
    "        print(f'nsrc: {nsrc}')\n",
    "        print(f'nbatch: {nbatch}')\n",
    "        print(f'l_nrundirs: {l_nrundirs}')\n",
    "        print(f'total_rundirs: {sum(l_nrundirs)}')\n",
    "        print(f'nprojdirs: {nprojdirs}')\n",
    "        \n",
    "        \n",
    "    # setup paths to specfem binarys and utils/tools\n",
    "    spec_bin_fqp   = os.path.join(spec_fqp, 'bin')\n",
    "    spec_utils_fqp   = os.path.join(spec_fqp, 'utils')\n",
    "    \n",
    "    \n",
    "    # Read input Par_file stub \n",
    "    par_lines = readlines(parfile_fqp)\n",
    "    \n",
    "    # Setup output Par_files based on user input Par_file stub\n",
    "    par_keys = ['SIMULATION_TYPE',\n",
    "                'SAVE_FORWARD',\n",
    "                'USE_FORCE_POINT_SOURCE',\n",
    "                'MODEL',\n",
    "                'SAVE_MESH_FILES',\n",
    "                'USE_BINARY_FOR_SEISMOGRAMS']\n",
    "    \n",
    "    #set solution type\n",
    "    use_force_src = ForceSolutionHeader == proj_record_h.solution_cls\n",
    "    keys_vals_dict = dict(zip(par_keys,[1,False,use_force_src,'gll',False,True]))\n",
    "    par_lines = change_multiple_parameters_in_lines(par_lines,keys_vals_dict)\n",
    "    \n",
    "    \n",
    "    ########################################################################\n",
    "    #\n",
    "    # If more than one proj_dir is needed then make a main proj_dir\n",
    "    # for the sub_proj_dirs.  \n",
    "    #\n",
    "    ########################################################################\n",
    "    \n",
    "    #Setup sub_dir pars here\n",
    "    if sub_proj_name == None:\n",
    "        sub_proj_name = 'Sub_' + proj_base_name\n",
    "        \n",
    "    sub_proj_root_fqp = proj_root_fqp\n",
    "    sub_pdir_name = proj_base_name\n",
    "    \n",
    "    \n",
    "    # make the main_dir if needed\n",
    "    if 1 < nprojdirs:\n",
    "        sub_proj_root_fqp = _make_proj_dir(proj_root_fqp,\n",
    "                                           proj_base_name)\n",
    "        \n",
    "        # make or copy MESH-default to the main project dir\n",
    "        setup_mesh_dir(sub_proj_root_fqp, mesh_fqp, copy_mesh=copy_mesh)\n",
    "        print(f'yes sub projects Main Dir:{sub_proj_root_fqp}')\n",
    "        \n",
    "    if verbose: print(f'sub_proj_root_fqp: {sub_proj_root_fqp}')\n",
    "        \n",
    "    \n",
    "    accum_src = 0\n",
    "    for ipdir in range(nprojdirs):\n",
    "        \n",
    "        # Make subdir or main project dir depending on number of proj_dirs\n",
    "        sub_projdir_fqp = sub_proj_root_fqp\n",
    "        \n",
    "        if nprojdirs != 1:\n",
    "            sub_pdir_name = sub_proj_name + '_' + str(ipdir+1).zfill(4)\n",
    "            \n",
    "        sub_projdir_fqp = _make_proj_dir(sub_projdir_fqp,\n",
    "                                         sub_pdir_name,\n",
    "                                         pyutils_fqp=pyutils_fqp,\n",
    "                                         script_fqp=script_fqp,)\n",
    "        \n",
    "        \n",
    "        if nprojdirs == 1:\n",
    "            #no sub projects so copy or make symlink (user par)\n",
    "            setup_mesh_dir(sub_projdir_fqp, mesh_fqp, copy_mesh=copy_mesh)\n",
    "            print(f'no sub projects Main Dir:{sub_projdir_fqp}')\n",
    "        else: \n",
    "            #multiple subproject: only make symlink for subproj dirs\n",
    "            sym_mesh_fqp = os.path.join(sub_proj_root_fqp,'MESH-default')\n",
    "            rel_mesh_fqp = os.path.relpath(sub_proj_root_fqp,sym_mesh_fqp)\n",
    "            rel_mesh_fqp = os.path.join(rel_mesh_fqp,'MESH-default')\n",
    "            print(f'sub_projdir_fqp:{sub_projdir_fqp}')\n",
    "            print(f'rel_mesh_fqp:{rel_mesh_fqp}')\n",
    "            setup_mesh_dir(sub_projdir_fqp, rel_mesh_fqp, copy_mesh=False)\n",
    "        \n",
    "        \n",
    "        ####################################################################\n",
    "        #\n",
    "        # Make all the run dirs per proj/sub_proj dirs\n",
    "        #\n",
    "        ####################################################################\n",
    "        for irdir in range(l_nrundirs[ipdir]):\n",
    "            \n",
    "            #FIXME: \"ie\" is not Correct! need list[ipdir][irdir] returns ie\n",
    "            ie   = accum_src//nsrc\n",
    "            isrc = accum_src%nsrc\n",
    "            rdir_record_h = proj_record_h[ie,isrc:isrc+nbatch,:,:]\n",
    "            \n",
    "            rdir_record_h.solutions_df['proj_id'] = ipdir\n",
    "            rdir_record_h.stations_df['proj_id']  = ipdir\n",
    "            \n",
    "            _make_run_dir(irdir,\n",
    "                          sub_projdir_fqp,\n",
    "                          spec_bin_fqp,\n",
    "                          spec_utils_fqp,\n",
    "                          par_lines,\n",
    "                          common_dir_struct,\n",
    "                          rdir_record_h)\n",
    "            \n",
    "            accum_src += nbatch\n",
    "            \n",
    "            ####################################################################\n",
    "            #   \n",
    "            # Setup and set the SPECFEM station data paths and file prefix names\n",
    "            #\n",
    "            ####################################################################\n",
    "\n",
    "            #setup paths for changing data_fqdn's in stations\n",
    "            rdir_name = 'run' + str(irdir+1).zfill(4)\n",
    "            rundir_fqp = os.path.join(sub_projdir_fqp, rdir_name)\n",
    "            syn_fqp = os.path.join(rundir_fqp, 'SYN')\n",
    "            rel_syn_fqp = os.path.relpath(syn_fqp,sub_proj_root_fqp)\n",
    "            \n",
    "\n",
    "            # reset the pandas.Multiindex to allow eid and sid filtering\n",
    "            proj_record_h.reset_midx()\n",
    "            rec_df = proj_record_h.stations_df\n",
    "\n",
    "            # loop over all stations and set data_fqdn\n",
    "            ibool = (rec_df['eid'] == ie) & ((rec_df['sid'] >= isrc) & (rec_df['sid'] < isrc+nbatch))\n",
    "            for ridx, rec in rec_df[ibool].iterrows():\n",
    "                new_data_fqdn = os.path.join(rel_syn_fqp,station_auto_data_fname_id(rec))\n",
    "                rec_df.loc[ridx,\"data_fqdn\"] = new_data_fqdn\n",
    "                \n",
    "\n",
    "            # set index back to origanl\n",
    "            proj_record_h.set_default_midx()\n",
    "            \n",
    "    ###############################################\n",
    "    #\n",
    "    # write record header for the main project\n",
    "    #\n",
    "    ###############################################\n",
    "    proj_record_fqp = _get_header_path(sub_proj_root_fqp,'project_record')\n",
    "    if nprojdirs == 1:\n",
    "        single_proj_fqp = os.path.join(sub_proj_root_fqp,proj_base_name)\n",
    "        proj_record_fqp = _get_header_path(single_proj_fqp,'project_record')\n",
    "    _write_header(proj_record_fqp,proj_record_h)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_proj_name = 'TestNewMKProject'\n",
    "test_proj_root_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects/NewMKProj')\n",
    "test_parfile_fqp =  os.path.join(data_out_dir, 'Par_file')\n",
    "test_mesh_fqp = '/scratch/seismology/tcullison/test_mesh/MESH-default_batch_force_src'\n",
    "#test_mesh_fqp = '/Users/seismac/Documents/Work/Bench/ForkGnam/pyaspect/notebooks/data/output/tmp/TestProjects/MESH-default_norot_xsft4400_ysft19100_quart_100m_min_vs_sig250m'\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/python/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "\n",
    "#print(len(flatten_grouped_headers(l_grp_vsrcs.copy())))\n",
    "#print(len(flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs.copy()))))\n",
    "#print('nrec_per_src*nsrc:',21*12)\n",
    "\n",
    "l_flat_vsrcs = flatten_grouped_headers(l_grp_vsrcs)\n",
    "l_flat_vrecs = flatten_grouped_headers(flatten_grouped_headers(l_grp_vrecs_by_vsrcs))\n",
    "\n",
    "vrecord_h = RecordHeader(name='Reciprocal-Record',solutions_h=l_flat_vsrcs,stations_h=l_flat_vrecs)\n",
    "\n",
    "#print(vrecord_h)\n",
    "src_df = vrecord_h.solutions_df\n",
    "rec_df = vrecord_h.stations_df\n",
    "\n",
    "s_nsid = list(src_df.index.get_level_values('sid').unique())\n",
    "r_nsid = list(rec_df.index.get_level_values('sid').unique())\n",
    "\n",
    "#print(f'Are Same: {s_nsid == r_nsid}')\n",
    "\n",
    "d = {'one':1, 'two':2, 'three': 3}\n",
    "k = list(d.keys())\n",
    "\n",
    "isin = 'three' in k\n",
    "#print(f'is in: {isin}')\n",
    "\n",
    "test_proj_record_h = vrecord_h.copy()\n",
    "\n",
    "make_fwd_project_dir(test_proj_name,\n",
    "                     test_proj_root_fqp,\n",
    "                     test_parfile_fqp,\n",
    "                     test_mesh_fqp,\n",
    "                     test_spec_fqp,\n",
    "                     test_pyutils_fqp,\n",
    "                     test_script_fqp,\n",
    "                     test_proj_record_h,\n",
    "                     batch_srcs=False,\n",
    "                     verbose=True,\n",
    "                     copy_mesh=False,\n",
    "                     max_event_rdirs=5)\n",
    "                     #max_event_rdirs=MAX_SPEC_SRC)\n",
    "        \n",
    "\n",
    "print()\n",
    "print('ls:')\n",
    "!ls {test_proj_root_fqp}\n",
    "print('ls:')\n",
    "!ls {test_proj_root_fqp}/*/*\n",
    "'''\n",
    "print('rm:')\n",
    "!rm -rf {test_proj_root_fqp}/*\n",
    "print('ls:')\n",
    "!ls {test_proj_root_fqp}\n",
    "''';\n",
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reciprocal project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False\n",
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "test_proj_name = 'ReciprocalTestProject'\n",
    "test_proj_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects')\n",
    "test_parfile_fqp =  os.path.join(data_out_dir, 'Par_file')\n",
    "test_mesh_fqp = '/scratch/seismology/tcullison/test_mesh/MESH-default_batch_force_src'\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/python/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "\n",
    "from pyaspect.project import make_project\n",
    "#make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "make_project(test_proj_name,\n",
    "             test_proj_fqp,\n",
    "             test_parfile_fqp,\n",
    "             test_mesh_fqp,\n",
    "             test_spec_fqp,\n",
    "             test_pyutils_fqp,\n",
    "             test_script_fqp,\n",
    "             l_grp_vsrcs,\n",
    "             l_grp_vrecs_by_vsrcs,\n",
    "             copy_mesh=False)\n",
    "\n",
    "!ls -ltrh {os.path.join(test_proj_fqp,test_proj_name)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make \"real\" project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspect.specfemio.utils import _join_path_fname\n",
    "test_proj_name = 'ForwardTestProject'\n",
    "test_proj_fqp =  os.path.join(data_out_dir, 'tmp/TestProjects')\n",
    "test_parfile_fqp =  os.path.join(data_out_dir, 'Par_file')\n",
    "test_mesh_fqp = '/scratch/seismology/tcullison/test_mesh/MESH-default_batch_force_src'\n",
    "test_spec_fqp = '/quanta1/home/tcullison/DevGPU_specfem3d'\n",
    "test_pyutils_fqp = '/quanta1/home/tcullison/myscripts/python/specfem/pyutils'\n",
    "test_script_fqp = '/quanta1/home/tcullison/myscripts/specfem'\n",
    "\n",
    "from pyaspect.project import make_project\n",
    "#make_records(l_src=l_grp_solutions_h,l_rec=src_grouped_stations)\n",
    "make_project(test_proj_name,\n",
    "             test_proj_fqp,\n",
    "             test_parfile_fqp,\n",
    "             test_mesh_fqp,\n",
    "             test_spec_fqp,\n",
    "             test_pyutils_fqp,\n",
    "             test_script_fqp,\n",
    "             l_cmt_src,\n",
    "             l_grp_recs_by_srcs,\n",
    "             copy_mesh=False)\n",
    "\n",
    "!ls -ltrh {os.path.join(test_proj_fqp,test_proj_name)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
